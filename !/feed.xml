<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>http://www.thomaslevine.com/</id>
  <title>Thomas Levine</title>
  <updated>2014-03-21T00:00:00Z</updated>
  <link rel="alternate" href="http://www.thomaslevine.com/"/>
  <link rel="self" href="http://www.thomaslevine.com/!/feed.xml"/>
  <author>
    <name>Thomas Levine</name>
    <uri>http://www.thomaslevine.com</uri>
  </author>
  <entry>
    <id>tag:www.thomaslevine.com,2014-03-21:/!/statistics-with-doodles-2014-03/index.html</id>
    <title type="html">Statistics with Doodles</title>
    <published>2014-03-21T00:00:00Z</published>
    <updated>2014-03-21T00:00:00Z</updated>
    <link rel="alternate" href="http://www.thomaslevine.com/!/statistics-with-doodles-2014-03/index.html"/>
    <content type="html">&lt;p&gt;I doodled some statistics
&lt;a href="http://www.meetup.com/NYC-Machine-Learning/events/170546362/"&gt;last night&lt;/a&gt;,
with some alarming breaks for non-fires!&lt;/p&gt;

&lt;h2 id="materials"&gt;Materials&lt;/h2&gt;
&lt;p&gt;Here are the
&lt;a href="https://raw.githubusercontent.com/tlevine/statistics-doodles/master/doodles.pdf"&gt;slides&lt;/a&gt;,
and &lt;a href="https://raw.githubusercontent.com/tlevine/statistics-doodles/master/formulae.pdf"&gt;here&lt;/a&gt;’s
a more conventional write-up of the concepts I discussed.
The source codes for these two files are
&lt;a href="https://github.com/tlevine/statistics-doodles/blob/master/doodles.r"&gt;here&lt;/a&gt; and
&lt;a href="https://github.com/tlevine/statistics-doodles/blob/master/formulae.tex"&gt;here&lt;/a&gt;,
respectively.&lt;/p&gt;

&lt;h2 id="cutoff"&gt;Cutoff&lt;/h2&gt;
&lt;p&gt;The abstract indicated that I would discuss the following four statistics.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Covariance&lt;/li&gt;
  &lt;li&gt;Variance&lt;/li&gt;
  &lt;li&gt;(Pearson) correlation&lt;/li&gt;
  &lt;li&gt;Least-squares regression&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I skipped the last of these because of the second fire alarm.&lt;/p&gt;

&lt;h2 id="related-things"&gt;Related things&lt;/h2&gt;
&lt;p&gt;There’s a
&lt;a href="/!/statistics-with-doodles-sudoroom"&gt;video of a similar talk&lt;/a&gt;
that I gave, and a
&lt;a href="/!/higher-power-distance-measures/"&gt;drawing of a related concept&lt;/a&gt;
that some people brought up after the talk.&lt;/p&gt;

&lt;h2 id="thing-to-remember"&gt;Thing to remember&lt;/h2&gt;
&lt;p&gt;You can pretty much always draw math.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <id>tag:www.thomaslevine.com,2014-02-24:/!/cool-things-happen-when-you-have-lots-of-spreadsheets/index.html</id>
    <title type="html">Cool things happen when you look at lots of spreadsheets at once</title>
    <published>2014-02-24T00:00:00Z</published>
    <updated>2014-02-24T00:00:00Z</updated>
    <link rel="alternate" href="http://www.thomaslevine.com/!/cool-things-happen-when-you-have-lots-of-spreadsheets/index.html"/>
    <content type="html">&lt;p&gt;Here are some materials for
&lt;a href=""&gt;my talk at Open Austin&lt;/a&gt;,
but they’re written in normal language, so they’ll probably serve as a decent summary
of my work thus far to people who are reading this on the internet.&lt;/p&gt;

&lt;h2 id="before-we-start-why-i-can-be-here"&gt;Before we start: Why I can be here.&lt;/h2&gt;

&lt;p&gt;https://github.com/tlevine/undervalued-sublets&lt;/p&gt;

&lt;h2 id="outline-of-the-talk"&gt;Outline of the talk&lt;/h2&gt;

&lt;p&gt;Introduction&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;What are “open data”? Show the video.&lt;/li&gt;
  &lt;li&gt;Data about open data, data-driven open data&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Two approaches&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;We have all of these data, so something interesting must be in it.&lt;/li&gt;
  &lt;li&gt;We are interested in something. Let’s collect data that will tell us about that thing.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;How I collect these data&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Downloading the data&lt;/li&gt;
  &lt;li&gt;Looking inside the various datasets&lt;/li&gt;
  &lt;li&gt;Representing all the spreadsheets as one spreadsheet&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Cool things that happen&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Better ways of searching&lt;/li&gt;
  &lt;li&gt;Dealing with bad (meta)data.&lt;/li&gt;
  &lt;li&gt;Quantifying data quality&lt;/li&gt;
  &lt;li&gt;Causal inferences: Do open data do anything?&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Let’s talk about some things I’ve been &lt;a href="/open-data"&gt;learning about open data&lt;/a&gt;.
For the longest time, I had no idea what people meant when they were talking
about “open data”, and I think that’s part of why I started looking at the
stuff. So now I have a bit of an idea of what the open data thing is.&lt;/p&gt;

&lt;h3 id="open-data"&gt;“Open data”&lt;/h3&gt;
&lt;p&gt;&lt;a href="/!/open-data-in-plain-english"&gt;This video&lt;/a&gt; summarizes pretty well most of
the good things people say about open data, at least for government data.
But I think this sort of thinking makes open data seem way to complicated.&lt;/p&gt;

&lt;h3 id="sharing"&gt;Sharing&lt;/h3&gt;
&lt;p&gt;Sharing is usually good, and I think that’s the heart of what everyone likes
about “open data”, and we have a word for it because people aren’t used to
this idea. But when the thing that we’re sharing is complicated and
sometimes-private numbers, useful sharing gets hard. This happens when you’re
trying to share with the whole world, but it also happens when you’re trying
to share within a small company. &lt;/p&gt;

&lt;p&gt;We want to share, but we’re not very good at it, and this open data stuff is
part of our attempt to get better at sharing.&lt;/p&gt;

&lt;h3 id="recursive-data-driving"&gt;Recursive data-driving&lt;/h3&gt;
&lt;p&gt;One benefit of open data might be the ability for people to use lots of different
datasets in order to make data-driven decisions. The people who are releasing open
data surely get this, so they’re obviously using data to make decisions about their
open data initiatives, right?&lt;/p&gt;

&lt;p&gt;Actually, they’re not, so I started doing that. Also, I’m doing it quite publicly,
so you could say this is open data about open data.&lt;/p&gt;

&lt;h2 id="my-process-and-findings"&gt;My process and findings&lt;/h2&gt;
&lt;p&gt;I was taught in school that you come up with your question and then collect data
that perfectly answer that question. This way works, but you can often learn more
faster and with less work if you’re a bit sloppier.&lt;/p&gt;

&lt;p&gt;I like to think of two approaches of deciding what to study.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;We have all of these data, so something interesting must be in it.&lt;/li&gt;
  &lt;li&gt;We are interested in something. Let’s collect data that will tell us about that thing.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I think the former is more obvious: Initially, I found it quite odd that
nobody had looked at the data about the data. So I did.&lt;/p&gt;

&lt;p&gt;Let’s talk a bit about the latter. Let’s say we want to study someone’s sleep patterns.
In order to do this, we wind to find out when the person is sleeping. We could do this
by having the person record on paper the times at which she goes to sleep and wakes up,
but that would be a lot of work. Other ideas&lt;/p&gt;

&lt;p&gt;&lt;a href="http://yihui.name/en/2009/10/50000-revisions-committed-to-r/"&gt;Version control commits&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src="../data-about-open-data-talk-december-2-2013/r-commits.gif" alt="R commits" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://hackpad.com/Measuring-Socioeconomic-Indicators-in-Arabic-Tweets-IZ5ByP2LvIt"&gt;Tweets&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src="../data-about-open-data-talk-december-2-2013/tweet-times.jpg" alt="Bar plot of Tweet times" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;If the person is me, we can use shell history activity.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/sh
# This file is history.sh
for epochtime in $(grep '^#[0-9]\{10\}$' ~/.history/sh-2013-1[12]*|cut -d\# -f2); do
  date --date=@$epochtime +%H
done | sort | uniq -c | awk '{print $2, "%"$1"s"}' &amp;gt; /tmp/formatted

while read line; do
  # Remove the first space
  nospace=$(echo $line | sed 's/ //')
  printf "$line\n" | tr \  -|sed s/----------------------------------------/=/g|sed -e s/-//g -e 's/=/ =/'
done &amp;lt; /tmp/formatted
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here’s the resulting histogram.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./history.sh
00 =========
01 =======
02 =====
03 ====
04 =======
05 ============
06 ======
07 ====
08 =
09 =
10 =====
11 ===
12 ==
13 ======
14 =====
15 ======
16 ==============
17 ==============================
18 =======================================
19 ============================
20 ===============
21 ==========
22 ==================
23 ==================
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that these times are in UTC because that’s how I roll.&lt;/p&gt;

&lt;p&gt;In this approach of deciding what to study, the idea is that we can
answer our curiosities by building on some existing data collection.
&lt;a href="/!/brainstorming"&gt;These&lt;/a&gt; brief thoughts on brainstorming might be
of interest.&lt;/p&gt;

&lt;h2 id="collecting-the-data-about-data"&gt;Collecting the data about data&lt;/h2&gt;
&lt;p&gt;First I download a bunch of spreadsheets and spreadsheet metadata.
Then I assemble all this stuff into a spreadsheet about spreadsheets.
In this super-spreadsheet, each record corresponds to a full
sub-spreadsheet; you could say that I am aggregating each spreadsheet
to produce a few statistics that get put into this spreadsheet.&lt;/p&gt;

&lt;h3 id="downloading"&gt;Downloading&lt;/h3&gt;
&lt;p&gt;Data catalogs make it kind of easy to get a bunch of spreadsheets all together.
The basic approach is this.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Get all of the dataset identifiers.&lt;/li&gt;
  &lt;li&gt;Download the metadata document about each dataset.&lt;/li&gt;
  &lt;li&gt;Download data files about each dataset.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I’ve implemented this for the following data catalog softwares.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Socrata&lt;/li&gt;
  &lt;li&gt;CKAN&lt;/li&gt;
  &lt;li&gt;Junar (kind of)&lt;/li&gt;
  &lt;li&gt;OpenDataSoft&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This allows me to get all of the data from most of the open data catalogs I know about.&lt;/p&gt;

&lt;p&gt;Let’s walk through how that works for the different softwares.&lt;/p&gt;

&lt;h4 id="socrata"&gt;Socrata&lt;/h4&gt;
&lt;p&gt;In Socrata, I hit the &lt;code&gt;/api/views&lt;/code&gt; endpoint to get all of the datasets.
(They’re spread across different pages, but they’re all returned.)&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;http://data.austintexas.gov//api/views?page=3&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;All of the metadata are returned in the search results, so this also accomplishes
the second step of downloading the metadata documents.&lt;/p&gt;

&lt;p&gt;That said, you can also download the metadata documents separately;
here’s one of them.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;https://data.austintexas.gov/api/views/5tye-7ray&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Most datasets in Socrata Open Data Portal correspond to spreadsheets, and
you can download those by appending &lt;code&gt;/rows.csv?accessType=DOWNLOAD&lt;/code&gt;.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;https://data.austintexas.gov/api/views/5tye-7ray/rows.csv?accessType=DOWNLOAD&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;It took me a while to figure all of this out, so a lot of what I was doing over
the summer was writing documentation.&lt;/p&gt;

&lt;h4 id="ckan"&gt;CKAN&lt;/h4&gt;
&lt;p&gt;Someone wrote a good CKAN client, so I use that to download the CKAN stuff.
This is how I get a list of all the dataset identifiers.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/usr/bin/env python
import ckanapi
portal = ckanapi.RemoteCKAN('http://data.gov.uk')
datasets = portal.action.package_list()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That only provides the identifiers, so I continue with the following code
to get the metadata documents.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for dataset in datasets:
    dataset_information = portal.action.package_show(id = dataset)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href="http://data.gov.uk/api/2/rest/package/index-of-multiple-deprivation"&gt;Here&lt;/a&gt;’s
an example of one such metadata file.&lt;/p&gt;

&lt;p&gt;Most datasets on CKAN catalogs link to other websites for the main “data”
files, and the links are stored in the matadata files.&lt;/p&gt;

&lt;h4 id="junar"&gt;Junar&lt;/h4&gt;
&lt;p&gt;In Junar, it’s hard to get a list of all of the datasets. You can do a
search like so.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;http://paloalto.cloudapi.junar.com/datastreams/search?query=grapefruit&amp;amp;auth-key=da782fcac90afb0a310f72a4f63baff6d26fc0b1&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Well at least that used to work. It seems that that API key doesn’t work anymore.&lt;/p&gt;

&lt;p&gt;I’m pretty sure that the rest of the process works just fine once you have
a dataset identifier, but I don’t remember how that all works at the moment.&lt;/p&gt;

&lt;h4 id="opendatasoft"&gt;OpenDataSoft&lt;/h4&gt;
&lt;p&gt;In OpenDataSoft, you can run an empty search to get the metadata about all
of the datasets in a single file.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;http://parisdata.opendatasoft.com/api/datasets/1.0/search?rows=1000000&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Like with the other softwares, you can also get the metadata about a
specific dataset; here’s a URL for that.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;http://parisdata.opendatasoft.com/api/datasets/1.0/arbresremarquablesparis2011&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Each dataset corresponds to a spreadsheet, and you can download that by
adding &lt;code&gt;/download?format=csv&lt;/code&gt; to the above URL.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;http://parisdata.opendatasoft.com/explore/dataset/arbresremarquablesparis2011/download?format=csv&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id="looking-inside-datasets"&gt;Looking inside datasets&lt;/h3&gt;
&lt;p&gt;A statistic is a single number that describes a bunch of numbers.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;4
2
8    --mean--&amp;gt; 5.8
12
3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I create statistics about each dataset.&lt;/p&gt;

&lt;p&gt;&lt;img src="/!/dataset-as-datapoint/dataset-features.jpg" alt="" class="wide" /&gt;&lt;/p&gt;

&lt;h3 id="putting-them-in-a-spreadsheet"&gt;Putting them in a spreadsheet&lt;/h3&gt;
&lt;p&gt;Combining the metadata and the new dataset statistics, I create
a spreadsheet of datasets, in which each record corresponds to a dataset.&lt;/p&gt;

&lt;p&gt;&lt;a href="/!/dataset-as-datapoint"&gt;&lt;img src="/!/dataset-as-datapoint/spreadsheet-spreadsheet.png" alt="A spreadsheet of spreadsheets" class="wide" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id="cool-things"&gt;Cool things&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Better ways of searching&lt;/li&gt;
  &lt;li&gt;Dealing with bad (meta)data.&lt;/li&gt;
  &lt;li&gt;Quantifying data quality&lt;/li&gt;
  &lt;li&gt;Causal inferences: Do open data do anything?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;(They’re really all the same thing, actually.)&lt;/p&gt;

&lt;h3 id="searching"&gt;Searching&lt;/h3&gt;
&lt;p&gt;Many people know about datasets that are relevant to their work,
municipality, &amp;amp;c., but nobody seems to know about the availability of
data on broader topics, and nobody seems to have a good way of
finding out what is available. And nobody has a great idea of who
is using which data. Here are two aspects of the difficulty.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Naive search method&lt;/li&gt;
  &lt;li&gt;Siloed open data portals&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;(Read more &lt;a href="/!/openprism"&gt;here&lt;/a&gt;.)&lt;/p&gt;

&lt;h4 id="search-method"&gt;Search method&lt;/h4&gt;
&lt;p&gt;We search for prose by typing prose into a search bar; why don’t
we search for spreadsheets by typing spreadsheets into a search bar?
Aside frorm finding datasets that contain particular keywords, here
are some other ways we could search.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Datasets that were produced by the same program as this dataset&lt;/li&gt;
  &lt;li&gt;Datasets that I can join to this dataset&lt;/li&gt;
  &lt;li&gt;Datasets that pertain to this particular geographic region&lt;/li&gt;
  &lt;li&gt;Datasets in long format (rather than wide format)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here are some things that might get you thinking about the possibilities.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href="Datasets with the same schema"&gt;http://appgen.me/audit/report&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="https://pypi.python.org/pypi/special_snowflake"&gt;&lt;code&gt;special_snowflake&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Group by an arbitrary key, aggregate, join. The most common column
  in New York City’s data catalog was zip code. The zip code dataset!&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id="siloed-data-catalogs"&gt;Siloed data catalogs.&lt;/h4&gt;
&lt;p&gt;Different people put out their datasets on their own websites.
These sites work as a way of getting the data on the internet, but
they aren’t really designed for accessing data around a specific analytical
inquiry. For example, if I want to know where in Austin to build a house,
I don’t necessarily just data from the City of Austin; I might want data from
other cities, from the county, or from the state.&lt;/p&gt;

&lt;p&gt;&lt;img src="../data-about-open-data-talk-december-2-2013/unsilo.jpg" alt="Diagram about siloed open data portals and some layer to un-silo them" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;I made a &lt;a href="http://openprism.thomaslevine.com"&gt;rather simple site&lt;/a&gt; to demonstrate this idea.&lt;/p&gt;

&lt;h3 id="dealing-with-bad-metadata"&gt;Dealing with bad (meta)data&lt;/h3&gt;
&lt;p&gt;People complain about how data are bad and metadata are bad. Rather than
fixing it on a case-by-case basis, I think we should just come up with ways
of dealing with it.&lt;/p&gt;

&lt;p&gt;&lt;a href="/!/missouri-data-licensing/"&gt;Missouri&lt;/a&gt; provides a good illustration of this.
The titles of datasets are related to the contents of datasets.&lt;/p&gt;

&lt;p&gt;You can see my alternative search approaches as ways of guessing metadata.&lt;/p&gt;

&lt;h3 id="quantifying-data-quality"&gt;Quantifying data quality&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Open Knowledge Foundation &lt;a href="http://census.okfn.org/"&gt;Open Data Census&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Tim Berners-Lee &lt;a href="http://inkdroid.org/journal/2010/06/04/the-5-stars-of-open-linked-data/"&gt;Five Stars&lt;/a&gt; of open linked data.
  &lt;!-- http://opendata.stackexchange.com/a/529 --&gt;&lt;/li&gt;
  &lt;li&gt;Open Government Working Group &lt;a href="http://www.opengovdata.org/home/8principles"&gt;8 Principles of Open Government Data&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Sunlight Foundation &lt;a href="http://sunlightfoundation.com/opendataguidelines/"&gt;Open Data Policy Guidelines&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Open Data Institute &lt;a href="https://certificates.theodi.org/"&gt;Certificates&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id="licensing"&gt;Licensing&lt;/h4&gt;
&lt;p&gt;I &lt;a href="http://thomaslevine.com/!/open-data-licensing/"&gt;looked at&lt;/a&gt;
the licenses that different datasets have.&lt;/p&gt;

&lt;p&gt;&lt;img src="/!/open-data-licensing/p2.png" alt="Licenses across all portals" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;Most data catalogs either have a license on everything or a license on nothing.)&lt;/p&gt;

&lt;p&gt;&lt;img src="/!/open-data-licensing/p1.png" alt="Bar graph of proportion of datasets" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="http://opendatacommons.org/faq/"&gt;Licensing is important because it reduces uncertainty.&lt;/a&gt;&lt;/p&gt;

&lt;h4 id="updating"&gt;Updating&lt;/h4&gt;
&lt;p&gt;Open government data are supposed to be kept up-to-date.
&lt;a href="http://thomaslevine.com/!/data-updatedness/#even-simpler"&gt;Pretty much nobody&lt;/a&gt; does this.&lt;/p&gt;

&lt;h4 id="getting-the-data"&gt;Getting the data&lt;/h4&gt;
&lt;p&gt;Socrata has some date fields in the metadata, so I could look at the update behavior.&lt;/p&gt;

&lt;p&gt;First, hardly any datasets ever get updated.&lt;/p&gt;

&lt;p&gt;&lt;img src="/!/data-updatedness/figure/any_update.png" alt="Hardly any datasets get updated" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;Second, the ones that have been updated were mostly updated two years ago.
There might have been some bulk Socrata migration at the beginning of September 2011.&lt;/p&gt;

&lt;p&gt;&lt;img src="/!/data-updatedness/figure/publish_v_update.png" alt="Bulk migration?" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;Here are the datasets that got published before 2013 and got updated during 2013.&lt;/p&gt;

&lt;p&gt;&lt;img src="/!/data-updatedness/figure/publish_v_update_2013.png" alt="Old data still kept up-to-date" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;It’s only 13 datasets.&lt;/p&gt;

&lt;p&gt;&lt;img src="/!/data-updatedness/figure/updates_2013_url.png" alt="Those 13 datasets, by portal" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;Data are probably being updated thorugh other means,&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Adding new data as a separate dataset (a 2011 dataset and a 2012 dataset)&lt;/li&gt;
  &lt;li&gt;Deleting the old dataset and adding a new one&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;but these aren’t as good because they don’t preserve URIs.&lt;/p&gt;

&lt;h3 id="causal-inferences-how-things-work"&gt;Causal inferences: How things work&lt;/h3&gt;
&lt;p&gt;It would be great to tie the release of spreadsheets to outcomes that people really
care about, like levels of corruption, life expectancies, and employment rates.
But that’s hard, so I’m starting simpler; we can start by seeing what different
software products do.&lt;/p&gt;

&lt;p&gt;What do I mean by different products? Any of the below things would count.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Different features within a product&lt;/li&gt;
  &lt;li&gt;Different vendors (Socrata, Open Knowledge Foundation, Junar, OpenDataSoft, &amp;amp;c.)&lt;/li&gt;
  &lt;li&gt;Different plugins and &lt;a href="http://ckan.org/category/extensions/"&gt;extensions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Different &lt;a href="/!/socrata-products"&gt;products and services&lt;/a&gt; sold by one company&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This can be framed as practical questions like “Which data catalog software should I use?”&lt;/p&gt;

&lt;h4 id="charting-tools"&gt;Charting tools&lt;/h4&gt;
&lt;p&gt;These open data catalog softwares all have charting tools built in.
Basically, people don’t use these charting tools all that much.&lt;/p&gt;

&lt;p&gt;The Socrata metadata indicate the users that updated the data.
Most of the users in the dataset (7790 to be exact) had made exactly one view.&lt;/p&gt;

&lt;p&gt;&lt;img src="/!/socrata-users/figure/n.views.png" alt="" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;Actually, there are probably even more with no views, but I don’t have the
data on them. Also, about four-fifths of Socrata’s data is private, (Several
people who work for Socrata have told me this.) so I’m probably missing even
more. Oh well.&lt;/p&gt;

&lt;p&gt;Similarly, the users who have owned and authored the most tables tend to work
for either Socrata or clients of Socrata.&lt;/p&gt;

&lt;p&gt;Neither of these discoveries should be a surprise; you can call it the
&lt;a href="http://en.wikipedia.org/wiki/Pareto_principle"&gt;Pareto principle&lt;/a&gt; if you want.&lt;/p&gt;

&lt;p&gt;Socrata is trying to “consumerize” the data experience, so 
I tried to find users who were not employed by Socrata or its
clients. I eventually &lt;a href="/!/socrata-users/#also-no-tables"&gt;found some&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As I said above, my main conclusion is that people don’t use these charting
tools all that much. More specifically,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The people who create the most charts are people who maintain data portals&lt;/li&gt;
  &lt;li&gt;Aside from those who maintain data portals, the people who create the most
 charts are usually making different charts of the same data.&lt;/li&gt;
  &lt;li&gt;I found a small number of people who seem to be using the charts for broader
 things. I haven’t really talked to any of them, but the little I do know of
 their stories is interesting.&lt;/li&gt;
  &lt;li&gt;All of the data catalog softwares have charting tools like this.
 Socrata’s is the most built-out, so I doubt that we’ll see more usage
 in other sites’ charting tools.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If you are making a data catalog for your organization, I recommend that you
not worry about including data visualization things in the catalog.&lt;/p&gt;

&lt;h4 id="links"&gt;Links&lt;/h4&gt;
&lt;p&gt;Links can go &lt;a href="/404.html"&gt;dead&lt;/a&gt;. I looked at dataset liveliness for Socrata
and CKAN.&lt;/p&gt;

&lt;p&gt;&lt;img src="http://thomaslevine.com/!/zombie-links/figure/p_prop_links.png" alt="" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;In Socrata, datasets tend to be stored in the Socrata application and strongly
associated with the entity on the data portal. All of these internally stored
datasets stay alive (presumably). There are a few externally stored datasets,
and some of those are dead.&lt;/p&gt;

&lt;p&gt;In CKAN, datasets tend to reference external files, and a lot of them are dead.
That said, it seems to be better at keeping external links alive than Socrata is.&lt;/p&gt;

&lt;p&gt;This difference totally makes sense if you look at the processes for
&lt;a href="/!/data-catalog-dead-links/#software-suggests-behavior"&gt;uploading data&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We can also see what sorts of problems are arising.&lt;/p&gt;

&lt;p&gt;&lt;img src="/!/zombie-links/figure/p_no_redirects.png" alt="" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src="/!/zombie-links/figure/storage.png" alt="" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;Read more &lt;a href="http://thomaslevine.com/!/zombie-links/#new-results"&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;h2 id="final-thoughts"&gt;Final thoughts&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Ways of inquiring
    &lt;ul&gt;
      &lt;li&gt;Question to data&lt;/li&gt;
      &lt;li&gt;Data to question&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Open data is about sharing.&lt;/li&gt;
  &lt;li&gt;You can collect data about data.&lt;/li&gt;
  &lt;li&gt;You don’t really need to distinguish between metadata and data.&lt;/li&gt;
&lt;/ol&gt;
</content>
  </entry>
  <entry>
    <id>tag:www.thomaslevine.com,2014-02-22:/!/zombie-links/index.html</id>
    <title type="html">Zombie links on data catalogs</title>
    <published>2014-02-22T00:00:00Z</published>
    <updated>2014-02-22T00:00:00Z</updated>
    <link rel="alternate" href="http://www.thomaslevine.com/!/zombie-links/index.html"/>
    <content type="html">&lt;p&gt;After I wrote about
&lt;a href="/!/dead-links-on-data-catalogs"&gt;dead links on data catalogs&lt;/a&gt;,
some people commented that the links were less dead than I’d thought.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://twitter.com/statshero/status/424147773852622848"&gt;&lt;img src="trentino.png" alt="Anecdotally I don't think that @DatiTrentinoit has so many broken links. Check validator? @thomaslevine http://thomaslevine.com/!/data-catalog-dead-links/" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://twitter.com/waldojaquith/status/424026174508261376"&gt;&lt;img src="openva.png" alt="@thomaslevine You've got ~45% of data․openva․com datasets missing. I just audited 75% of them, and found just 2 missing. Any idea what's up?" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Some explanations were proposed. Samuele and Jindřich both suggested that
the CKAN FileStore doesn’t support HEAD requests.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://twitter.com/_rshk/status/424208140016418816"&gt;&lt;img src="samuele.png" alt="@DatiTrentinoit @statshero @thomaslevine Ckan returns 404 on HEAD requests, that's the problem.." /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://twitter.com/jindrichmynarz/status/428194318063370241"&gt;&lt;img src="jindrich.png" alt="" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;And &lt;a href="http://waldo.jaquith.org/"&gt;Waldo&lt;/a&gt;
suggested that I might be checking for status code 200 but
receiving status code 303 (redirect) from OpenVA.&lt;/p&gt;

&lt;p&gt;So what was going on?&lt;/p&gt;

&lt;h2 id="not-reasons"&gt;Not reasons&lt;/h2&gt;
&lt;p&gt;I considered the two possibilities that were mentioned above, and
I don’t think either of them was the issue.&lt;/p&gt;

&lt;h3 id="head"&gt;HEAD&lt;/h3&gt;
&lt;p&gt;CKAN does just fine on HEAD requests.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;url = 'http://dati.trentino.it/storage/f/2013-06-16T114537/_EBmYVk.csv'
import requests

get = requests.get(url)
head = requests.head(url)

print(get.status_code)
# 200
print(head.status_code)
# 200
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So I don’t think the issue was that CKAN returns 404 on HEAD requests.&lt;/p&gt;

&lt;h3 id="status-code"&gt;Status code&lt;/h3&gt;
&lt;p&gt;I counted a link as alive if and only if it returned a status code of 200.
Could the issue be that I needed to check for other status codes? Specifically,
could it be that the URLs for files hosted on some CKAN FileStores are redirects
rather than final URLs?&lt;/p&gt;

&lt;p&gt;URLs from the CKAN FileStore should not be an issue because I didn’t check URLs
when the data were marked as being stored internally in the catalog.&lt;/p&gt;

&lt;p&gt;That said, it’s possible that people uploaded the data to the CKAN FileStore and
then told CKAN that it was an external link; it turns out that this was the case
for OpenVA&lt;/p&gt;

&lt;p&gt;Running this query&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SELECT DISTINCT url FROM links WHERE catalog = 'data.openva.com' AND NOT is_link;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;yields three URLs with that characteristic.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;https://data.openva.com.s3.amazonaws.com/2013-06-30T21:27:16.600Z/2013.json
https://data.openva.com.s3.amazonaws.com/2013-12-16T03:06:56.875Z/sentencing.csv
https://data.openva.com.s3.amazonaws.com/2013-05-11T04:57:18.031Z/agency-websites.csv
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;They have issues with SSL certificates,&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;requests.head('https://data.openva.com.s3.amazonaws.com/2013-06-30T21:27:16.600Z/2013.json')
# SSLError: hostname 'data.openva.com.s3.amazonaws.com' doesn't match either of '*.s3.amazonaws.com', 's3.amazonaws.com'
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and they work just fine with unencrypted HTTP.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;requests.head('http://data.openva.com.s3.amazonaws.com/2013-06-30T21:27:16.600Z/2013.json').status_code
# 200
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Moreover, there aren’t very many redirect status codes across all the links.&lt;/p&gt;

&lt;p&gt;&lt;img src="figure/p_no_redirects.png" alt="" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;Redirects are normally of status code 301 or 303, or at least in the 300-399
range. I followed redirects when I downloaded them, so this plot contains no
redirect status codes.&lt;/p&gt;

&lt;h2 id="reasons"&gt;Reasons&lt;/h2&gt;
&lt;p&gt;If the HEAD request thing and the redirect status code weren’t the issues,
what was?&lt;/p&gt;

&lt;p&gt;To figure this out, I tweaked my downloader and ran it on just the links that
had timed out or otherwise not responded; I didn’t run it on links that had
responded with error status codes like 404. I also pulled out the hostname of
the links. (For example, &lt;code&gt;thomaslevine.com&lt;/code&gt; of the URL of the page you’re
reading.) Then I looked at the errors I got back.&lt;/p&gt;

&lt;p&gt;I also looked around in the SQLite3 database in which I’d been storing
everything for this link analysis.&lt;/p&gt;

&lt;p&gt;It’s not like there was just one issue, of course.
Here are the main factors I see as leading to the strange results.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;I didn’t fully parse links to datasets.&lt;/li&gt;
  &lt;li&gt;I had a low timeout on my HTTP requests (2 seconds).&lt;/li&gt;
  &lt;li&gt;I had duplicate data in my database.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id="incompletely-parsed-urls"&gt;Incompletely parsed URLs&lt;/h3&gt;
&lt;p&gt;I looked at the different sorts of errors that I got when I requested links
to different hostnames.&lt;/p&gt;

&lt;p&gt;&lt;img src="figure/p_hostname_error.png" alt="" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;That the left-most bar, for the hostname &lt;code&gt;http:&lt;/code&gt;, is quite large and has
a lot of invalid URLs. Things that my hostname-parser detected as &lt;code&gt;http:&lt;/code&gt; are
usually invalid URLs.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[1] "http://"
[2] "http://www..ic.nhs.uk/catalogue/PUB09271/per-soc-ser-adu-soc-car-sur-eng-2011-12-fin-tables-charts.xls"
[3] "http://Gemeente-Den-Haag-open-data-Bodemenergie-restwarmtelocaties-bestaande-warmte-koude-opslag.zip"
[4] "http://financial-transactions-data-west-midlands-strategic-health-authority-October-2012"
[5] "http://Con la legge 18 giugno 2009, n. 69 è stato previsto, all’art. 21, che le pubbliche amministrazioni pubblichino sui rispettivi siti internet le retribuzioni annuali, i curricula vitae, gli indirizzi di posta elettronica e i numeri telefonici ad uso professionale dei dirigenti, nonché le statistiche sui tassi di assenza e di maggiore presenza del personale distinti per uffici di livello dirigenziale.  La Presidenza è quindi impegnata nella necessaria raccolta dei dati trattati dai diversi Uffici del Segretariato generale e dei Ministri senza portafoglio, anche al fine di assicurarne l’omogeneità indispensabile per poter presentare elaborazioni attendibili e significative di una realtà complessa ed articolata come quella della Presidenza del Consiglio. "
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Many of the links were specified as relative URLs, and I didn’t try to parse them.
I could have known to look for this, but having absolute URLs would still have
been easier.&lt;/p&gt;

&lt;p&gt;Also, some of the URLs were intranet file paths (like &lt;code&gt;S:Foo\Bar\Baz.xls&lt;/code&gt;).
Not all datasets are public yet, so this is better than nothing.
That said, I wonder whether people realize that these intranet paths are not
accessible on the normal internet.&lt;/p&gt;

&lt;h3 id="low-timeout"&gt;Low Timeout&lt;/h3&gt;
&lt;p&gt;Aside from the invalid URLs, most of the bad links gave a timeout.&lt;/p&gt;

&lt;p&gt;&lt;img src="figure/p_errors_total.png" alt="" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;As we saw above, different websites (hostnames) tend to give different errors.
The following plot should make it more clear.&lt;/p&gt;

&lt;p&gt;&lt;img src="figure/p_hostname_facet.png" alt="" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;Recall that people thought that CKAN doesn’t respond to HEAD requests. It might
just be that CKAN is slow to respond to requests. Here is a plot with datasets
color-coded based on whether they appear to be served from a CKAN FileStore.&lt;/p&gt;

&lt;p&gt;&lt;img src="figure/storage.png" alt="plot of chunk storage" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;(The &lt;code&gt;/storage&lt;/code&gt; endpoint is typically used for the CKAN FileStore.)&lt;/p&gt;

&lt;p&gt;It looks like the CKAN FileStore can take a while to respond, and that might be
why people thought that HEAD requests fail.&lt;/p&gt;

&lt;h2 id="duplicate-data"&gt;Duplicate data&lt;/h2&gt;
&lt;p&gt;It turned out that I had duplicate records in my table of link information. As I was working
with it, I forgot the correct schema and remembered a different one; I thought I had a unique
index on something for which I didn’t.&lt;/p&gt;

&lt;p&gt;To illustrate this, see below a plot of the number of CKAN datasets for which my link liveliness
data has duplicate records. The top bar graph is for datasets that were stored internally, and
the bottom bar graph is for datasets that were stored externally.&lt;/p&gt;

&lt;p&gt;&lt;img src="figure/p_duplicates_ckan.png" alt="plot of chunk p_duplicates_ckan" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;We see that there were quite a many duplicates, particularly for datasets that were stored
externally. This happened because I ran my downloader script multiple times, thinking that
duplicates were being skipped rather than resolved properly.&lt;/p&gt;

&lt;p&gt;Because of how I set up the database, it’s hard to see this in the Socrata data.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
## stat_bin: binwidth defaulted to range/30. Use 'binwidth = x' to adjust
## this. stat_bin: binwidth defaulted to range/30. Use 'binwidth = x' to
## adjust this.
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src="figure/p_duplicates_socrata.png" alt="plot of chunk p_duplicates_socrata" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;This plot shows that I have hundreds of duplicates of the same dataset.
Most of these duplicates probably came from original Socrata site (as explained
&lt;a href="/!/socrata-deduplicate"&gt;here&lt;/a&gt;).
Like with the CKAN datasets, my database schema issue probably only doubled or
tripled the duplication for the Socrata datasets.&lt;/p&gt;

&lt;h2 id="new-results"&gt;New results&lt;/h2&gt;
&lt;p&gt;I used a longer timeout on the previously-erring links and deduplicated my duplicates.
Here are new results that correspond with the plots in the
&lt;a href="/!/data-catalog-dead-links"&gt;previous article&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In these plots, each bar represents all of the datasets for one data catalog. The bar
is colored differently at different segments to represent different categories of dataset.&lt;/p&gt;

&lt;p&gt;The first plot shows that most datasets within a given catalog are alive, either because
they are internally stored or because they are stored externally and have live links.&lt;/p&gt;

&lt;p&gt;&lt;img src="figure/p_link_types.png" alt="plot of chunk p_link_types" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;Let’s ignore the datasets that are stored internally. The bars will represent only the
datasets that are external links, and the missing bars correspond to catalogs with no
external links.&lt;/p&gt;

&lt;p&gt;&lt;img src="figure/p_link_types_onlylinks.png" alt="plot of chunk p_link_types_onlylinks" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;One of my main conclusions in the prior article was that Socrata and CKAN encourage
different paradigms for adding data to the catalog. Socrata sites are more likely to
have internally stored data, and CKAN sites are more likely to have externally stored
data. On the other hand, externally stored datasets are more likely to be alive in
CKAN sites than in Socrata sites. I made this plot to explain that.&lt;/p&gt;

&lt;p&gt;&lt;img src="/!/data-catalog-dead-links/figure/prop_links.png" alt="Old version of the plot" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;The plot looks a bit different with the fixed data (below),&lt;/p&gt;

&lt;p&gt;&lt;img src="figure/p_prop_links.png" alt="plot of chunk p_prop_links" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;but conclusion still seems reasonable.&lt;/p&gt;

&lt;h2 id="unexplained"&gt;Unexplained&lt;/h2&gt;
&lt;p&gt;Let’s look closely at the two catalogs for which my prior results didn’t seem right.&lt;/p&gt;

&lt;p&gt;&lt;img src="figure/p_link_types_specifics.png" alt="plot of chunk p_link_types_specifics" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;OpenVA is looking more alive than it did before, and the links that I marked as dead
are links with SSL problems. But I don’t know why Trentino’s links are marked as dead.
I checked a few of these links, and they download just fine.&lt;/p&gt;

&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;My prior article gave some strange figures regarding the liveliness of datasets.
In particular, there were high rates of dead links for some catalogs.
Most of the strange figures in my prior article are explained by me having duplicates
in my database. Also, some dead links are explained by factors that we might not think
of as dead links, such as ambigous URLs or bad SSL configurations.&lt;/p&gt;

&lt;p&gt;My main conclusion in the previous article is that CKAN and Socrata encourage storing
data in different places (internally and externally). After I accounted for the
duplication, this conclusion continues to seem valid.&lt;/p&gt;

&lt;p&gt;Finally, we saw some interesting issues with the construction of URLs for datasets.
There was a bit more variation in URLs than I had anticipated.&lt;/p&gt;

&lt;h1 id="appendix-how-i-figured-this-all-out"&gt;Appendix: How I figured this all out&lt;/h1&gt;
&lt;p&gt;Here are a bunch of things I tried doing in order to figure out what was going on.
It’s not very well organized or explained, but you might find it interesting.&lt;/p&gt;

&lt;h2 id="status-codes"&gt;Status codes&lt;/h2&gt;
&lt;p&gt;I called a URL alive if an ordinary HEAD request to it returned a
&lt;a href="http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html"&gt;status code&lt;/a&gt; of 200.
This simplifies things a little bit.
I started out by considering whether this was an appropriate test.&lt;/p&gt;

&lt;p&gt;Here are all of the status codes that I received, from all of the different
links from all of the catalogs.&lt;/p&gt;

&lt;p&gt;&lt;img src="figure/status_codes.png" alt="plot of chunk status_codes" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;Let’s look at this for just the specific catalogs that those Tweets were about.
Here’s OpenVA.&lt;/p&gt;

&lt;p&gt;&lt;img src="figure/status_codes_va.png" alt="plot of chunk status_codes_va" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;And here’s Trentino.&lt;/p&gt;

&lt;p&gt;&lt;img src="figure/status_codes_trentino.png" alt="plot of chunk status_codes_trentino" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;Aside from 200, all of those status codes are errors, so this method of checking
seems fine. On the other hand, it seems like there were a lot of non-responses….
More about non-responses later; for now, we’ll just say that the status code
check is fine.&lt;/p&gt;

&lt;h2 id="duplicates"&gt;Duplicates&lt;/h2&gt;
&lt;p&gt;When plotting this, I realized that some of the data didn’t line up. I set up
a database schema that was more normalized so that I wouldn’t check a link twice
if two datasets linked to it. I thus had a datasets table and a links table.&lt;/p&gt;

&lt;p&gt;Then I wound up changing my mind and using the links table to store a single record
per dataset. Here arose a problem; there was no unique index on this datasets table,
but I thought there was, so I added multiple records for each link.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style="text-align: left"&gt;catalog&lt;/th&gt;
      &lt;th style="text-align: left"&gt;identifier&lt;/th&gt;
      &lt;th style="text-align: right"&gt;count(*)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style="text-align: left"&gt;dartportal.leeds.ac.uk&lt;/td&gt;
      &lt;td style="text-align: left"&gt;dart_excavation_data&lt;/td&gt;
      &lt;td style="text-align: right"&gt;5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style="text-align: left"&gt;dartportal.leeds.ac.uk&lt;/td&gt;
      &lt;td style="text-align: left"&gt;dart_geophysics_cmdminiexplorer&lt;/td&gt;
      &lt;td style="text-align: right"&gt;5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style="text-align: left"&gt;dartportal.leeds.ac.uk&lt;/td&gt;
      &lt;td style="text-align: left"&gt;dart_geophysics_flashres64&lt;/td&gt;
      &lt;td style="text-align: right"&gt;5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style="text-align: left"&gt;dartportal.leeds.ac.uk&lt;/td&gt;
      &lt;td style="text-align: left"&gt;dart_geophysics_geoscanrm15&lt;/td&gt;
      &lt;td style="text-align: right"&gt;5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style="text-align: left"&gt;dartportal.leeds.ac.uk&lt;/td&gt;
      &lt;td style="text-align: left"&gt;dart_laboratorydata&lt;/td&gt;
      &lt;td style="text-align: right"&gt;5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style="text-align: left"&gt;dartportal.leeds.ac.uk&lt;/td&gt;
      &lt;td style="text-align: left"&gt;dart_monitoring_apparent_permitivity_cstdr100&lt;/td&gt;
      &lt;td style="text-align: right"&gt;5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style="text-align: left"&gt;dartportal.leeds.ac.uk&lt;/td&gt;
      &lt;td style="text-align: left"&gt;dart_monitoring_bulk_electrical_conductivity_cstdr100&lt;/td&gt;
      &lt;td style="text-align: right"&gt;5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style="text-align: left"&gt;dartportal.leeds.ac.uk&lt;/td&gt;
      &lt;td style="text-align: left"&gt;dart_monitoring_soilconductivity_imko_pico_t3p&lt;/td&gt;
      &lt;td style="text-align: right"&gt;5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style="text-align: left"&gt;dartportal.leeds.ac.uk&lt;/td&gt;
      &lt;td style="text-align: left"&gt;dart_monitoring_temperature_cs107l&lt;/td&gt;
      &lt;td style="text-align: right"&gt;5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style="text-align: left"&gt;dartportal.leeds.ac.uk&lt;/td&gt;
      &lt;td style="text-align: left"&gt;dart_monitoring_weather_data&lt;/td&gt;
      &lt;td style="text-align: right"&gt;5&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;I had thought that there was a unique index on
&lt;code&gt;links.software, links.catalog, links.identifier&lt;/code&gt;,
but there wasn’t!&lt;/p&gt;

&lt;h2 id="misinterpreting-nulls"&gt;Misinterpreting NULLs&lt;/h2&gt;
&lt;p&gt;Another issue: I had interpreted NULL as meaning that the dataset is not a link,
but it really represents that link’s liveliness has yet  to be checked;
here’s the relevant line of code.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;url_list = [row['url'] for row in dt.execute('SELECT DISTINCT url FROM links WHERE status_code IS NULL')]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I don’t remember exactly how this affected the results thought; it might not have
been a big deal.&lt;/p&gt;

&lt;h2 id="unresponsive-datasets"&gt;Unresponsive datasets&lt;/h2&gt;
&lt;p&gt;I recorded when HTTP requests for datasets had timed out or otherwise
not responded. (In the database, these are indicated as status code -42.)
I checked a few of these manually. Here are two of them.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dati.trentino.it/storage/f/2013-06-16T111814/_ggeiWE.csv
https://www-genesis.destatis.de/genesis/online/link/tabelleDownload/46421-0001.html
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;They both work, but they take a while. Also, I found that they took longer
to download to my desktop computer on my desk than they took to download to
my server in a datacenter.
In case the problem was my internet connection (tethered from a phone),
I used that server in a datacenter to run the checker again on
all datasets that had timed out. Results didn’t remarkably change.&lt;/p&gt;

&lt;h2 id="slow-datasets"&gt;Slow datasets&lt;/h2&gt;
&lt;p&gt;Those links took a while to download. Maybe my timeout threshold is being hit?&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;url = 'http://dati.trentino.it/storage/f/2013-06-16T114537/_EBmYVk.csv'
import requests

timeout = requests.head(url, timeout = 2)
# timeout error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So it is. More detail follows.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;get = requests.get(url)
head = requests.head(url)

print(get)
# &amp;lt;Response [200]&amp;gt;

print(head)
# &amp;lt;Response [200]&amp;gt;

print(head.elapsed)
# datetime.timedelta(0, 3, 353558)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The link is alive, but my timeout of 2 seconds was too short.
In this case, the request with a timeout failed, so the initial
response took too long. Also, it took a total of 3.35 seconds to
download. The elapsed time isn’t the same thing as the time until
which the request will time out, but they’re related.&lt;/p&gt;

&lt;h3 id="bad-urls"&gt;Bad URLs&lt;/h3&gt;
&lt;p&gt;A bunch of datasets have a field for an external link but provided
an empty URL.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[1] catalog  count(*)
&amp;lt;0 rows&amp;gt; (or 0-length row.names)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Aside from being interesting in itself, this pointed out to me that
there were probably lots of types of errors that I hadn’t really
thought about. I realized I could look at the errors for each of the links
I checked. I thought I had saved the error in the database, but
my code for doing that turned out to be broken.&lt;/p&gt;

&lt;p&gt;So I fixed that! Collecting better error information, I ran the checker
on all of the links that had failed before.&lt;/p&gt;

&lt;h3 id="hostname"&gt;Hostname&lt;/h3&gt;
&lt;p&gt;I figured that errors might be related to the server that a dataset comes
from, and I figured that the hostname of the URL would be a decent proxy
for server. (In the URL “http://thomaslevine.com/open-data”, the hostname
is “thomaslevine.com”.) So I wrote a sloppy function to detect these hostnames.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  datasets$hostname &amp;lt;- sub('(?:(?:http|ftp|https)://)?([^/]*)/.*$', '\\1', datasets$url)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here are the top few hostnames and the number of datasets with each hostname.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;              \t 0ccfs001.sussex.nhs.uk\\csu
               1                           1
10.96.9.105:8080               176.32.230.19
               1                           1
 192.171.153.213               194.151.67.33
               1                           1
   195.217.160.2              195.55.247.252
               1                           1
 2010.census.gov              207.251.86.229
               1                           1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Having come up with this variable, I now could look at error types by hostname.&lt;/p&gt;

&lt;h3 id="base-error-rate"&gt;Base error rate&lt;/h3&gt;
&lt;p&gt;&lt;img src="figure/p_errors_total.png" alt="" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src="figure/p_hostname_total.png" alt="" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src="figure/p_hostname_error.png" alt="" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src="figure/p_hostname_facet.png" alt="" class="wide" /&gt;&lt;/p&gt;

&lt;h3 id="invalid-urls"&gt;Invalid URLs&lt;/h3&gt;
&lt;p&gt;The “http:” datasets weren’t valid URLs.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Error type&lt;/th&gt;
      &lt;th&gt;URL&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;ConnectionError&lt;/td&gt;
      &lt;td&gt;http:// Localisation des accès des offices de tourisme&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ConnectionError&lt;/td&gt;
      &lt;td&gt;http://nullFPM.shp.zip&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ConnectionError&lt;/td&gt;
      &lt;td&gt;http:// 2012_PNOA.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ConnectionError&lt;/td&gt;
      &lt;td&gt;http://fotovoltaico.provincia.tn.it\solar.xml&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;LocationParseError&lt;/td&gt;
      &lt;td&gt;http://2011 NFL draft: Andrew Luck is cementing his status as the No. 1 overall prospect on the board.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id="connection-errors"&gt;Connection errors&lt;/h3&gt;
&lt;p&gt;Connection errors seem to correspond to some datasets with strange URLs and others for
which the site just can’t be contacted.&lt;/p&gt;

&lt;p&gt;&lt;img src="figure/connectionerror.png" alt="plot of chunk connectionerror" class="wide" /&gt;&lt;/p&gt;

&lt;h3 id="invalid-schemas"&gt;Invalid schemas&lt;/h3&gt;
&lt;p&gt;Invalid schemas are for datasets sent over protocals other than HTTP, like FTP.&lt;/p&gt;

&lt;p&gt;&lt;img src="figure/invalidschema.png" alt="plot of chunk invalidschema" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;Most of these schemas indicate that the files are stored on local systems
rather than being accessible from the internet. But a large minority of these
(FTP, specifically) is fully reasonable to put on the internet; I didn’t
check them properly.&lt;/p&gt;

&lt;h3 id="missing-schemas"&gt;Missing schemas&lt;/h3&gt;
&lt;p&gt;Missing schemas tend to be for datasets where the hostname was not specified.
Examples:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[1] "/storage/f/2012-08-13T002240/COSCParks.kmz"
[2] "/storage/f/2012-08-22T025312/prod_test.csv"
[3] "/storage/f/2012-08-10T100153/sc_addies.csv"
[4] "/en/storage/f/2013-02-11T170442/Copy-of-GB_Certified_130211_for-map.csv"
[5] "/storage/f/2012-08-10T071459/annual-new-h2o-meters-2000-2010.csv"
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src="figure/missingschema.png" alt="plot of chunk missingschema" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;This is a valid relative URL. I could have gotten the hostname from the site
from which I got the link, but I did not do this.&lt;/p&gt;

&lt;h3 id="ssl-errors"&gt;SSL Errors&lt;/h3&gt;
&lt;p&gt;A bunch of sites did not have SSL certificates that I recognized.&lt;/p&gt;

&lt;p&gt;&lt;img src="figure/p_ssl_error.png" alt="plot of chunk p_ssl_error" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;I could ignore the certificates and download the dataset, but the SSL warning
is slightly unnerving.&lt;/p&gt;

&lt;p&gt;SSL errors explain only a small part of the links I marked as dead. Of the
datasets that I’d marked as dead before, 457 had SSL errors and 7630 did not.&lt;/p&gt;

&lt;p&gt;They are interesting, but they don’t explain my strange results.&lt;/p&gt;

&lt;h3 id="timeouts"&gt;Timeouts&lt;/h3&gt;
&lt;p&gt;Once we get rid of the strange URLs, most of these links have no errors or have
timeouts. (Remember, these are the links that I marked as dead in my previous
analysis, and I tried downloading again for the present analysis.)&lt;/p&gt;

&lt;p&gt;&lt;img src="figure/p_hostname_facet.png" alt="" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;The “www-genesis.destatis.de” datasets seem mostly okay, though there are some timeouts.&lt;/p&gt;

&lt;p&gt;&lt;img src="figure/plot_destatis.png" alt="plot of chunk plot_destatis" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src="figure/p_elapsed.png" alt="plot of chunk p_elapsed" class="wide" /&gt;&lt;/p&gt;
</content>
  </entry>
  <entry>
    <id>tag:www.thomaslevine.com,2014-02-21:/!/decompilers-in-css/index.html</id>
    <title type="html">Decompilers in CSS</title>
    <published>2014-02-21T00:00:00Z</published>
    <updated>2014-02-21T00:00:00Z</updated>
    <link rel="alternate" href="http://www.thomaslevine.com/!/decompilers-in-css/index.html"/>
    <content type="html">&lt;p&gt;I wrote
&lt;a href="https://github.com/tlevine/csv.css"&gt;CSV&lt;/a&gt;,
&lt;a href="http://openprism.thomaslevine.com/"&gt;markdown&lt;/a&gt;, and
&lt;a href="https://github.com/tlevine/html2tex"&gt;LaTeX&lt;/a&gt;
decompilers in CSS.&lt;/p&gt;

&lt;p&gt;And now the tables on thomaslevine.com are styled as CSVs!&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;column1&lt;/th&gt;
      &lt;th&gt;column2&lt;/th&gt;
      &lt;th&gt;column3&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
</content>
  </entry>
  <entry>
    <id>tag:www.thomaslevine.com,2014-02-03:/!/open-data-500-data-package/index.html</id>
    <title type="html">Open Data 500 Data Package</title>
    <published>2014-02-03T00:00:00Z</published>
    <updated>2014-02-03T00:00:00Z</updated>
    <link rel="alternate" href="http://www.thomaslevine.com/!/open-data-500-data-package/index.html"/>
    <content type="html">&lt;p&gt;I recently took a stab at documenting the
&lt;a href="http://www.opendata500.com"&gt;Open Data 500&lt;/a&gt; data format,
but my &lt;a href="/!/open-data-500-data-dictionary"&gt;work thus far&lt;/a&gt;
leaves much to be desired. I looked at it a bit more over the weekend,
and I think I have a pretty good idea about it now.
Collecting all of my findings about the data format,
I packaged the data in a way that should be a bit easier to work with.&lt;/p&gt;

&lt;h2 id="more-stuff-i-figured-out-about-the-open-data-500-data"&gt;More stuff I figured out about the Open Data 500 data&lt;/h2&gt;
&lt;p&gt;It was pretty simple to convert the data formats once I figured them out, but
that step took me a while. The main issue was reconciling the conflicting data
that are reported on the Open Data 500 site.&lt;/p&gt;

&lt;h3 id="the-conflicting-data"&gt;The conflicting data&lt;/h3&gt;
&lt;p&gt;The Open Data 500 Team released four data files in its “Downloads” section,
and it implicitly released two HTML data files elsewhere on its site.
Some of these seem to present the same data in different formats, but when
I looked closely, I found that they actually contained different data, and
I wasn’t really sure which data to use. Do I combine the different files?
If a company is in one file but not another, is it part of the study?
Is one of the files more up-to-date?&lt;/p&gt;

&lt;p&gt;I never really figured out what to do about all this.&lt;/p&gt;

&lt;h3 id="all-questionnaire-responses-will-be-included"&gt;All questionnaire responses will be included&lt;/h3&gt;
&lt;p&gt;Until I did last week’s research into the Open Data 500 methodology,
I had the impression that the Open Data 500 involved some assessment of
companies beyond what they say in the questionnaire. Now, I think that
the study is just an ordinary questionnaire survey and that they want as
many responses as they can get. If this is the case, the Open Data 500 Team
would probably want to report any company that completed the questionnaire.&lt;/p&gt;

&lt;p&gt;In assembling my Open Data 500 dataset, I thus think it’s safe to include
any company that was included in any of the official Open Data 500 dataset
releases.&lt;/p&gt;

&lt;h3 id="only-the-html-version-gets-updated"&gt;Only the HTML version gets updated&lt;/h3&gt;
&lt;p&gt;The CSV and JSON versions of the larger Open Data 500 list include
51 companies each, and the HTML version of the larger Open Data 500 list
includes 100 companies. I wasn’t really sure what to make of this, but now
I have a guess.&lt;/p&gt;

&lt;p&gt;I scraped the data from the various HTML pages on the site, and I noticed
that there is indeed quite complete data for all of these 100 companies.
I thus think that these companies responded to the questionnaire.&lt;/p&gt;

&lt;p&gt;Also, I noticed that the “download” section isn’t being updated.
I downloaded the JSON and CSV files in this section when they
were first released in December, and they files on the website
haven’t changed since.&lt;/p&gt;

&lt;p&gt;I haven’t been downloading the HTML files regularly, but it looks like the
site is being updated.
I found &lt;a href="https://github.com/GovLab/OpenData500"&gt;this code repository&lt;/a&gt;,
which seems to be the code for the site. In that repository as well,
the “download” files have never been updated. But recent development on
the site has been happening, so I find it conceivable that the data behind
the HTML files is being updated. The data are being stored in Mongo rather
than as files in the repository, so I can’t say for sure that the data are
being updated, but it seems likely.&lt;/p&gt;

&lt;p&gt;Anyway, my broader conclusion is that the HTML pages &lt;strong&gt;are&lt;/strong&gt; being updated and
the “download” section &lt;strong&gt;is not&lt;/strong&gt; being updated.&lt;/p&gt;

&lt;h2 id="packaging-the-data"&gt;Packaging the data&lt;/h2&gt;
&lt;p&gt;I produced two different CSV file to describe the same data, and these file
should make a lot more sense than the official CSV files. One of the files,
&lt;code&gt;companies.csv&lt;/code&gt;, has a row for every company and is normalized. The other,
&lt;code&gt;datasets.csv&lt;/code&gt;, has a row for every dataset, with different rows duplicating
the data about the companies; it is denormalized and contains no data about
the companies that lack datasets.&lt;/p&gt;

&lt;h3 id="upstream-sources"&gt;Upstream sources&lt;/h3&gt;
&lt;p&gt;The data come from the “full list” section of the Open Data 500 website.
I took most of the data for any particular company from its page on the website;
for example, most of the data for Appallicious came from
&lt;a href="http://www.opendata500.com/Appallicious/"&gt;&lt;code&gt;http://www.opendata500.com/Appallicious/&lt;/code&gt;&lt;/a&gt;.
I also took data from the &lt;a href="http://www.opendata500.com/candidates"&gt;full list page&lt;/a&gt;.
If the same field was present on both pages (for example, the company name),
I used the value on the company’s particular page rather than the value on the
main “full list” page.&lt;/p&gt;

&lt;h3 id="schema-of-the-companies-file"&gt;Schema of the companies file&lt;/h3&gt;
&lt;p&gt;This version of the dataset is a single CSV file,
where each row is a company and most columns
are answers to questionnaire questions. (I think! Remember that I’m guessing at
all of this.) Here are the columns that I think to come from the first page of
the questionnaire.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Column name&lt;/th&gt;
      &lt;th&gt;Question from the questionnaire&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code&gt;company.name&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;Name of your company&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;`company.url&lt;/td&gt;
      &lt;td&gt;Company URL&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code&gt;location&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;“In which city is this company located?”, and “State” (two questions)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code&gt;year.founded&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;Founding Year&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code&gt;fte&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;Number of FTE’s&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code&gt;type.of.company&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;Type of Company&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code&gt;category&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;What category best describes your company?&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code&gt;function&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;Which best describes the function of your company?&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code&gt;source.of.revenue&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;Which of the following are significant sources of revenue for your company?&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code&gt;description&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;Please give us a short public statement describing your company’s mission and work. You can take this material from your website or other publications if you choose to.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code&gt;description.short&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;As a summary, please provide a one sentence description of your company.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code&gt;social.impact&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;Besides revenue generation, how do you measure the impact your company has for society and the public good?&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code&gt;financial.info&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;Please include any financial or operational information that will help us understand your company. We are interested in specific information like past and projected annual revenues, total outside investment dollars to date, and significant investors or partners.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The &lt;code&gt;dataset&lt;/code&gt; field contains a CSV file of the datasets listed on subsequent
pages of the questionnaire. This file has a row for each dataset, and it has
columns for the URL (&lt;code&gt;dataset.url&lt;/code&gt;) and name (&lt;code&gt;dataset.name&lt;/code&gt;) of the dataset.&lt;/p&gt;

&lt;p&gt;The three remaining fields are not from the questionnaire&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Column name&lt;/th&gt;
      &lt;th&gt;Meaning&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code&gt;company.href&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;Location of the company page within the Open Data 500 website&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code&gt;data.collection&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;Which method was used for data collection? (“questionnaire” or “undocumented”)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;And I suspect that the &lt;code&gt;sectors(s)&lt;/code&gt; column comes from the questionnaire,
but I haven’t figured out what question it’s from.&lt;/p&gt;

&lt;h3 id="schema-of-the-datasets-file"&gt;Schema of the datasets file&lt;/h3&gt;
&lt;p&gt;In the datasets file, each row is a dataset used by a company.
The datasets file contains all of the columns that are in the companies
file except for the &lt;code&gt;datasets&lt;/code&gt; column, and it adds two more columns:
&lt;code&gt;dataset.url&lt;/code&gt; for the link to the dataset, and &lt;code&gt;dataset.name&lt;/code&gt; for the
name of the dataset. These are the two columns inside the nested CSV
files in the companies file.&lt;/p&gt;

&lt;p&gt;Because each row is about a dataset, this file contains no data about
companies with zero datasets.&lt;/p&gt;

&lt;h3 id="questionnaire-versus-undocumented-data-collection"&gt;Questionnaire versus undocumented data collection&lt;/h3&gt;
&lt;p&gt;I separate the data collection methods for the Open Data 500 into two methods.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Convenience-sampled questionnaire responses
 (“&lt;a href="#comprehensive-call-sampling-strategy"&gt;comprehensive call&lt;/a&gt;”)&lt;/li&gt;
  &lt;li&gt;Undocumented data collection&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Data from both sorts of data collection are included in the companies dataset,
and the &lt;code&gt;data.collection&lt;/code&gt; field indicates which method was used.
You’ll also notice that much of the data are missing for rows that were
collected with the undocumented method.&lt;/p&gt;

&lt;p&gt;The datasets file, on the other hand, contains no data about companies that
were collected through the undocumented method; in the undocumented data
collection method, no data were produced about datasets used by the companies.&lt;/p&gt;

&lt;h2 id="using-the-data"&gt;Using the data&lt;/h2&gt;
&lt;p&gt;You can download the files here.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href="https://raw.github.com/tlevine/open-data-500/master/companies.csv"&gt;&lt;code&gt;companies.csv&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;a href="https://raw.github.com/tlevine/open-data-500/master/datasets.csv"&gt;&lt;code&gt;datasets.csv&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I’ve also packaged the it as a
&lt;a href="http://data.okfn.org/standards/data-package"&gt;Data Package&lt;/a&gt;,
so you can check out the git repository 
(&lt;code&gt;git://github.com/tlevine/open-data-500.git&lt;/code&gt;)
and use any of
&lt;a href="http://data.okfn.org/tools"&gt;these fancy tools&lt;/a&gt;
to load the &lt;code&gt;datapackage.json&lt;/code&gt; file.&lt;/p&gt;

&lt;p&gt;Do tell me if you have any trouble with the files.
And I’d love to hear about
anything that you find with the data!&lt;/p&gt;
</content>
  </entry>
</feed>

