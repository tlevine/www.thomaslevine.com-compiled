<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>http://www.thomaslevine.com/</id>
  <title>Thomas Levine</title>
  <updated>2013-09-23T00:00:00Z</updated>
  <link rel="alternate" href="http://www.thomaslevine.com/"/>
  <link rel="self" href="http://www.thomaslevine.com/!/feed.xml"/>
  <author>
    <name>Thomas Levine</name>
    <uri>http://www.thomaslevine.com</uri>
  </author>
  <entry>
    <id>tag:www.thomaslevine.com,2013-09-23:/!/socrata-deduplicate/index.html</id>
    <title type="html">What's in a count?</title>
    <published>2013-09-23T00:00:00Z</published>
    <updated>2013-09-23T00:00:00Z</updated>
    <link rel="alternate" href="http://www.thomaslevine.com/!/socrata-deduplicate/index.html"/>
    <content type="html">&lt;p&gt;Lots of governments use Socrata data portals to release their open data.
This results in a super-dataset of datasets that tells us something about
the ecosystem surrounding open data.&lt;/p&gt;

&lt;p&gt;For a &lt;a href="/!/socrata-summary"&gt;couple months&lt;/a&gt;, I’ve been studying the data
about these data and have continuously been dealing with duplicated data.
I finally fixed that, so now I have more accurate statistics on portals.&lt;/p&gt;

&lt;p&gt;Quick preview:&lt;/p&gt;

&lt;p&gt;&lt;img src="histogram.png" alt="Histogram of dataset counts by portal" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;(I’ll explain later.)&lt;/p&gt;

&lt;h2 id="types-of-duplication"&gt;Types of duplication&lt;/h2&gt;
&lt;p&gt;I’m concerned with
&lt;a href="/!/socrata-genealogies/#types-of-duplicate-datasets"&gt;two forms of duplication&lt;/a&gt;.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href="/!/socrata-genealogies/#term-view"&gt;Views&lt;/a&gt; derived from original
 &lt;a href="/!/socrata-genealogies/#term-dataset"&gt;datasets&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Federated datasets, which are just links to datasets on another portal&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id="dealing-with-derived-views"&gt;Dealing with derived views&lt;/h2&gt;
&lt;p&gt;It’s easy enough to deal with the former type of duplication.
One way is to use the &lt;code&gt;/data.json&lt;/code&gt; (or &lt;code&gt;/api/dcat&lt;/code&gt;) endpoint.
Unfortunately, this endpoint only provides the first 1000 entries.
I have been told that this has been fixed, but I had trouble getting
it working this morning.&lt;/p&gt;

&lt;p&gt;That’s not a big deal because there’s another easy way. I produced a
&lt;a href="https://github.com/tlevine/socrata-analysis/blob/master/socrata.csv"&gt;CSV file of Socrata data&lt;/a&gt;
a couple months ago. Each row in the file corresponds to a view on Socrata,
and the views that are original datasets have a displayType of &lt;code&gt;table&lt;/code&gt;, I think.
(There’s a notable exception: Sometimes, the original table is private and
a derived view is made available to the public.)&lt;/p&gt;

&lt;h2 id="dealing-with-federation"&gt;Dealing with federation&lt;/h2&gt;
&lt;p&gt;Dealing with federation is less straightforward. The &lt;code&gt;/data.json&lt;/code&gt; endpoint
doesn’t clearly indicate whether a dataset is federated, and neither does
my CSV file. If I had been more careful when producing the CSV file, it could
have such a field, but I don’t want to make another one because that took a
long time. So I took the federation links from somewhere else.&lt;/p&gt;

&lt;p&gt;Scroll down on the homepage of a portal (like &lt;a href="https://explore.data.gov/"&gt;data.gov&lt;/a&gt;),
and you might see something like this.&lt;/p&gt;

&lt;p&gt;&lt;img src="federated-domains.png" alt="Federated Domains" /&gt;&lt;/p&gt;

&lt;p&gt;This shows up if the portal is federating any other portals. It’s part of the
search menu, so it lets you filter your search by portal.&lt;/p&gt;

&lt;p&gt;I &lt;a href="https://github.com/tlevine/socrata-defederate"&gt;downloaded all of the homepages and pulled out the federation links&lt;/a&gt;.
Here they are, in a rather plain diagram. (Hover over a node to see the portal name.)&lt;/p&gt;

&lt;style&gt;
  line { stroke: white; }
  circle { fill: rgb(254, 87, 161); }
&lt;/style&gt;

&lt;div id="graph-diagram"&gt;&lt;/div&gt;

&lt;p&gt;Yay! Now we can deduplicate based on federation. I looked through the CSV file
that I produced a couple months ago and removed records that were federated.
That is, if two datasets had the same 4x4 identifier, I kept the one belonging
to the portal that was being federated.&lt;/p&gt;

&lt;h2 id="better-statistics"&gt;Better statistics&lt;/h2&gt;
&lt;p&gt;I plotted two graphs again to get a feel for how this changes things.&lt;/p&gt;

&lt;h3 id="dataset-counts-by-portal"&gt;Dataset counts by portal&lt;/h3&gt;
&lt;p&gt;First, here’s the graph of dataset counts from the new data.&lt;/p&gt;

&lt;p&gt;&lt;img src="histogram.png" alt="Histogram of dataset counts by portal" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;My &lt;a href="/!/socrata-summary/#sizes"&gt;first version&lt;/a&gt;, also below, includes both types of deplication.&lt;/p&gt;

&lt;p&gt;&lt;img src="/!/socrata-summary/figure/big_portals_datasets.png" alt="Old histogram" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;The &lt;a href="/!/socrata-formats/#how-many-datasets"&gt;version that uses &lt;code&gt;data.json&lt;/code&gt;&lt;/a&gt;, also below,
doesn’t have the derived-view duplication, but it does have the federation duplication.&lt;/p&gt;

&lt;p&gt;&lt;img src="http://localhost:3000/!/socrata-formats/figure/portal-counts.png" alt="data.json version" /&gt;&lt;/p&gt;

&lt;h3 id="dataset-creation-dates"&gt;Dataset creation dates&lt;/h3&gt;
&lt;p&gt;I also plotted dataset creation dates over time.&lt;/p&gt;

&lt;p&gt;&lt;img src="dates.png" alt="Dataset creation over time" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;I hadn’t made a plot quite like this before, but the issue of duplication
has turned up in other plots about dataset creation dates, like in the
&lt;a href="/!/socrata-summary/#time"&gt;original summary&lt;/a&gt; and in the study of
&lt;a href="/!/socrata-formats/#csv"&gt;dataset formats&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id="arbitrary-metrics-are-arbitrary"&gt;Arbitrary metrics are arbitrary&lt;/h2&gt;
&lt;p&gt;Dataset count might sound like a decent metric, but it’s quite hard to say
what a dataset is. If you &lt;a href="http://appgen.me/audit/report"&gt;split up a dataset by year&lt;/a&gt;,
do you have multiple datasets? Similarly, if you create
&lt;a href="/!/socrata-genealogies"&gt;lots of different queries&lt;/a&gt; on the same dataset,
have you created new datasets? And do you have more datasets if you just copy
them from another data portal?&lt;/p&gt;

&lt;p&gt;I’m told that there is perennial competition between Chicago and New York
and between &lt;code&gt;data.gov&lt;/code&gt; and &lt;code&gt;data.gov.uk&lt;/code&gt; over the dataset count on their portals.
Given the ambiguity of this measure, it is quite hilarious that such competition
exists.&lt;/p&gt;

&lt;p&gt;On the other hand, that’s the only obvious metric that we see in data portals.
Maybe we need to make &lt;a href="/!/open-data-plans/#data-quality"&gt;new metrics&lt;/a&gt;
before people can stop caring about counts.&lt;/p&gt;

&lt;script src="d3.v3.min.js"&gt;&lt;/script&gt;

&lt;script src="graph-diagram.js"&gt;&lt;/script&gt;

</content>
  </entry>
  <entry>
    <id>tag:www.thomaslevine.com,2013-09-17:/!/open-data-plans/index.html</id>
    <title type="html">Open data possibilities</title>
    <published>2013-09-17T00:00:00Z</published>
    <updated>2013-09-17T00:00:00Z</updated>
    <link rel="alternate" href="http://www.thomaslevine.com/!/open-data-plans/index.html"/>
    <content type="html">&lt;p&gt;I’ve brief suspended my open data studies to prepare
some things for &lt;a href="http://zipfianacademy"&gt;Zipfian Academy&lt;/a&gt;’s
first twelve-week course, but I’ve been accumulating
lots of ideas of things you could do with data about
open data. I plan on doing a lot of them, but I
probably can’t do all, so you should take one of these
ideas and do something awesome.&lt;/p&gt;

&lt;h2 id="whats-in-the-portals"&gt;What’s in the portals?&lt;/h2&gt;

&lt;h3 id="socrata-v-github"&gt;Socrata v GitHub&lt;/h3&gt;
&lt;p&gt;GitHub has a pretty CSV table visualizer thingy that supports small CSV files.
What proportion of Socrata tables are small enough to be supported by GitHub?&lt;/p&gt;

&lt;h3 id="compare-official-datasets-to-derived-datasets"&gt;Compare official datasets to derived datasets&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;data.json&lt;/code&gt; file contains only the official datasets and the federated datasets.
The search of the site contains all of the derived views. Let’s compare these two to
see what sort of data gets studied within the data portal.&lt;/p&gt;

&lt;h3 id="openness-by-department-and-city"&gt;Openness by department and city&lt;/h3&gt;
&lt;p&gt;Some departments are more resistant to opening data. I’m told that the departments
that are more political tend to be more resistant to opening data. I want to use
open data release as an indicator of politicalness of a department, and I want to
see how open data release (politicalness) of equivalent departments (like education)
vary by city.&lt;/p&gt;

&lt;h2 id="better-search-and-linked-data"&gt;Better search and linked data&lt;/h2&gt;

&lt;p&gt;Let’s find links between datasets. Ideally, let’s get a bunch of datasets with rich
linked data about them and try to predict the linkages. Specific things I want to try&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Find datasets that can be joined on common variables (columns).&lt;/li&gt;
  &lt;li&gt;Determine the statistical unit (meaning of a row) of a dataset.&lt;/li&gt;
  &lt;li&gt;Aggregate datasets so that they can be joined.&lt;/li&gt;
  &lt;li&gt;Automatically union datasets by selecting datasets with the same variables (columns)
  and statistical units (rows)&lt;/li&gt;
  &lt;li&gt;Fill in the codebook (data dictionary). For datasets missing codebooks, we might be
  able to pick some of the information out of codebooks with other dictionaries.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id="data-quality"&gt;Data quality&lt;/h2&gt;

&lt;h3 id="dataset-and-data-portal-scorecard"&gt;Dataset and data portal scorecard&lt;/h3&gt;
&lt;p&gt;The current obvious metrics of dataset and data portal quality are
size, like number of records and number of datasets. There are other
things that we might want to worry about.&lt;/p&gt;

&lt;p&gt;There are a few guidelines as to how to do open data.
There’s one from &lt;a href="http://sunlightfoundation.com/opendataguidelines/"&gt;Sunlight&lt;/a&gt;,
there’s one from the &lt;a href="http://www.appgen.me/audit/"&gt;AppGen audit&lt;/a&gt;,
I have a list from a conversation from &lt;a href="http://noneck.org/"&gt;Noel&lt;/a&gt;,
there’s Tim Berners-Lee’s &lt;a href="http://5stardata.info/"&gt;five stars&lt;/a&gt;,
and there are a bunch of open data laws and policies.&lt;/p&gt;

&lt;p&gt;People like the Sunlight Foundation
have rated the openness of various places. I would like to make
such a rating in a more automated way.&lt;/p&gt;

&lt;h3 id="completeness-of-portals"&gt;Completeness of portals&lt;/h3&gt;
&lt;p&gt;By looking at data from FOIA and other portals, we can guess whether
a city has a particular dataset internally and check whether it has
been released publicly. Let’s do this across all cities and see whether
a particular sort of data is available in particular cities.&lt;/p&gt;

&lt;h3 id="dependencies"&gt;Dependencies&lt;/h3&gt;
&lt;p&gt;Maybe one dataset is only useful if we also have another dataset.
For example, maybe we need the mapping from school identifier to
school location before a school dataset is interesting. If we look
at historical release and usage of datasets, we might get an idea
of these dependencies and interactions.&lt;/p&gt;

&lt;h3 id="real-outcomes"&gt;Real outcomes&lt;/h3&gt;
&lt;p&gt;Open data isn’t useful in itself; it’s useful because good things
will happen if we do. Or at least that’s the idea. As
&lt;a href="https://twitter.com/IanJKalin"&gt;Ian Kalin&lt;/a&gt; says, the data portal
metrics should really tell us how many tax dollars the open data
saved and how many jobs the data portal created. If we’re creative,
we can come up with something like this.&lt;/p&gt;

&lt;h2 id="collect-more-data"&gt;Collect more data&lt;/h2&gt;
&lt;p&gt;There are lots of data portals, and not all
aspects of the various data portals software
are documented. Here are some things that will
help with further studies in the areas discussed
above.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Who federates whom on Socrata? Figure this out by checking the
  home pages.&lt;/li&gt;
  &lt;li&gt;Download data from other data portal software. I currently just
  have all of the Socrata portals, but it won’t be hard to get all
  of the CKAN, Junar and OpenDataSoft portals as well.&lt;/li&gt;
  &lt;li&gt;Collect data about FOIA from places like MuckRock and FOIA Machine.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id="studies-with-open-data"&gt;Studies with open data&lt;/h2&gt;
&lt;p&gt;I’ve been studying the use of open data, but we could also do some studies
that use open data. Here are two particular things that I want to do.&lt;/p&gt;

&lt;h3 id="gentrification"&gt;Gentrification&lt;/h3&gt;
&lt;p&gt;Where is gentrification gonna happen next, or where should a hipster move?
Look at hipster places in open data, and model historical data about gentrification
to find places that will become hipster soon. My current thought is that these
places are the ones where it is easy to move and start something new; they might
have efficient housing markets, decent public transit, and open social circles.
Note well: I suspect that my thinking is quite naive and that there are probably
loads of doctoral students working on this already&lt;/p&gt;

&lt;h3 id="tutorial-on-using-open-data"&gt;Tutorial on using open data&lt;/h3&gt;
&lt;p&gt;I want to do all this aforementioned stuff to make it easier to find and connect
open data. But people might not want to wait until all of that stuff exists.
So I want to make a tutorial about finding and connecting open data from different
governments in order to learn cool things.&lt;/p&gt;

&lt;h2 id="bug-me"&gt;Bug me&lt;/h2&gt;
&lt;p&gt;I have a pretty clear list of things to do, but I’ve only planned out the next
couple blog posts. If you would like to influence the order in which I do things,
please take Jason’s lead and bug me.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://twitter.com/jasonmhare/status/379739193230233600"&gt;&lt;img src="jason.png" alt="@thomaslevine tell me about my #socrata site ;)" /&gt;&lt;/a&gt;&lt;/p&gt;
</content>
  </entry>
  <entry>
    <id>tag:www.thomaslevine.com,2013-09-12:/!/statistics-with-doodles-sudoroom/index.html</id>
    <title type="html">Statistics through doodles</title>
    <published>2013-09-12T00:00:00Z</published>
    <updated>2013-09-12T00:00:00Z</updated>
    <link rel="alternate" href="http://www.thomaslevine.com/!/statistics-with-doodles-sudoroom/index.html"/>
    <content type="html">&lt;style&gt;
  .control {
    height: auto;
    border: none;
  }
  img.control {
    width: 9%;
    max-width: 50px;
    margin: 0;
    margin-right: 1%;
    padding: 0;
  }
  input[type=range].control {
    width: 75%;
    margin: 0;
    height: 40px;
  }
  video {
    margin: 0;
    border: 0;
    padding: 0;
  }
&lt;/style&gt;

&lt;p&gt;A &lt;em&gt;statistic&lt;/em&gt; is a number that describes a lot of other numbers.
By reducing many numbers into one number, we make it easier to
figure out what the numbers mean; we wouldn’t be able to fit all
of the original numbers in our brain.&lt;/p&gt;

&lt;p&gt;People usually explain statistics with symbols, but I like explaining
statistics with drawings, 
&lt;a href="https://sudoroom.org/wiki/Today_I_Learned#July_20:_Statistics_through_doodles:_Geometric_computations_of_fundamental_statistical_concepts"&gt;I doodled about statistics&lt;/a&gt;
one time in &lt;a href="http://sudoroom.org/"&gt;Sudoroom&lt;/a&gt;,
and we took videos of it. Watch them here!&lt;/p&gt;

&lt;div id="videos"&gt;
  &lt;video width="49%" class="back" src="http://bigdada.thomaslevine.com/til-statistics-back.webm"&gt;&lt;/video&gt;
  &lt;video width="49%" class="above" src="http://bigdada.thomaslevine.com/til-statistics-above.webm"&gt;&lt;/video&gt;
&lt;/div&gt;

&lt;div id="controls"&gt;
  &lt;a id="play" href="javascript:play()"&gt;&lt;img class="control" alt="Play" src="play.jpg" /&gt;&lt;/a&gt;
  &lt;a id="pause" href="javascript:pause()"&gt;&lt;img class="control" alt="Pause" src="pause.jpg" /&gt;&lt;/a&gt;
  &lt;input class="control" id="seek" type="range" min="0" max="1" value="0" step="0.01" /&gt;
&lt;/div&gt;
&lt;p style="text-align: right;"&gt;
  &lt;a href="javascript:big()"&gt;Big videos&lt;/a&gt; | 
  &lt;a href="javascript:small()"&gt;Small videos&lt;/a&gt;
&lt;/p&gt;

&lt;script&gt;document.write('&lt;script src="script.js?date=' + (new Date()).getTime() + '"&gt;&lt;' + '/script&gt;')&lt;/script&gt;

&lt;p&gt;The videos gloss over the reasons why we have these statistics,
so I discuss those reasons below.&lt;/p&gt;

&lt;h2 id="geometric-computations"&gt;Geometric computations&lt;/h2&gt;
&lt;p&gt;In the video, I geometrically computed four statistics
about the relationships between different variables.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Covariance&lt;/li&gt;
  &lt;li&gt;Variance&lt;/li&gt;
  &lt;li&gt;Correlation&lt;/li&gt;
  &lt;li&gt;Least-squares regression coefficients&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Each computation is for the simplest version of the statistic.
For those who like big words, that’s the univariate or bivariate
version, and the population statistic rather than the sample statistic.&lt;/p&gt;

&lt;h2 id="why-we-care-about-these-statistics"&gt;Why we care about these statistics&lt;/h2&gt;
&lt;p&gt;The videos show the computations of these statistics, but they
don’t really explain why we use these statistics. So I’ll explain
that here.&lt;/p&gt;

&lt;h3 id="linear-relationships"&gt;Linear relationships&lt;/h3&gt;
&lt;p&gt;A lot of relationships can be seen as linear relationships.
One such relationship is that between a person’s height and weight;
taller people are heavier, and shorter people are lighter.&lt;/p&gt;

&lt;p&gt;A relationship that isn’t very linear might be 
&lt;a href="/!/ridership-rachenitsa"&gt;public transit ridership and time&lt;/a&gt;.
As time progresses, weekly public transit ridership stays the same.
However, it does change a lot within the week, with high ridership
on the weekdays, low ridership on Saturdays and lower ridership on Sundays.&lt;/p&gt;

&lt;p&gt;The example I used in the video is locations where your friends live,
which might form clusters rather than lines.&lt;/p&gt;

&lt;p&gt;The four statistics that the videos discuss are ways of describing the
strength of a relationship, and they only make sense to use with linear
relationships.&lt;/p&gt;

&lt;h3 id="covariance"&gt;Covariance&lt;/h3&gt;
&lt;p&gt;Covariance is a basic measure of how strong the relationship is.
It is just a number that is&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;zero if there is no relationship,&lt;/li&gt;
  &lt;li&gt;really big if the two variables tend to move in the same direction, and&lt;/li&gt;
  &lt;li&gt;really negative if the two variables tend to move in opposite directions.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For example, the covariance between weight and height is a very positive
number, like maybe 9001, because taller people are heavier. This postive
covariance is represented in the video as lots of orange rectangles in
the top-right and bottom-left quadrants.&lt;/p&gt;

&lt;p&gt;&lt;img src="positive-covariance.jpg" alt="Positive covariance" /&gt;&lt;/p&gt;

&lt;p&gt;The covariance between number of times a person has eaten popcorn and her
shoe size is probably around zero, because I doubt that these are very
strongly related. In the video, we see this as orange and blue rectangles
balancing out.&lt;/p&gt;

&lt;p&gt;&lt;img src="zero-covariance.jpg" alt="Zero covariance" /&gt;&lt;/p&gt;

&lt;p&gt;And the covariance between cholesterol level and lifespan is probably
a very negative number, like maybe -1337, because people with more
cholesterol tend to live less long. That’s blue rectangles, with my
doodles’ conventions.&lt;/p&gt;

&lt;p&gt;&lt;img src="negative-covariance.jpg" alt="Negative covariance" /&gt;&lt;/p&gt;

&lt;p&gt;We tend not to use the covariance very directly in practice because
it’s hard to compare covariances directly to each other. The reasons
for this are explained in the video.&lt;/p&gt;

&lt;h3 id="variance"&gt;Variance&lt;/h3&gt;
&lt;p&gt;Variance is a measure of how spread out one variable is. It is a
positive number that gets big when the variables are more spread out.&lt;/p&gt;

&lt;p&gt;Let’s say that two people are cutting wood to build a house. They cut
10 pieces of wood each, and each piece of wood is supposed to be exactly
120 inches long.&lt;/p&gt;

&lt;p&gt;One person is very careful when he measures the wood, so his pieces come
out perfectly. They’re not exactly 120 inches long because that’s impossible,
but they’re not spot-on for the purposes of construction. The variance of the
lengths of his pieces of wood is very close to zero, like maybe 3.&lt;/p&gt;

&lt;p&gt;The other person is drunk and stoned and thus not very careful, so the
lengths of his pieces are all over the place. They’re still around 10
feet long on average, but some of them are 8 feet long, and others are
11 feet long. The variance of the lengths of this person’s pieces is
very high; maybe it’s 300.&lt;/p&gt;

&lt;p&gt;We talk about variances a lot when we are estimating the average of a
variable. When we estimate an average, we want to know how precise our
estimate is, and the variance tells us that.&lt;/p&gt;

&lt;h3 id="correlation"&gt;Correlation&lt;/h3&gt;
&lt;p&gt;Think of the correlation as a standardized version of the covariance.
The correlation is a number between -1 and 1. Like for the covariance,
positive correlations mean that the variables move together, and negative
correlations mean that the variables move oppositely.&lt;/p&gt;

&lt;p&gt;Like the covariance, the correlation tells us how strongly two variables
are related. The practical difference is that we can compare different
covariances to each other.&lt;/p&gt;

&lt;p&gt;We can compute the correlation between two variables based on the covariance
between the two variables and the respective variances of the two variables.&lt;/p&gt;

&lt;h3 id="least-squares-regression"&gt;Least-squares regression&lt;/h3&gt;
&lt;p&gt;Maybe you want to be able to guess someone’s height based on her weight.
Regression is one way of doing this.&lt;/p&gt;

&lt;p&gt;To predict height from weight, we can use a simple regression that would
tell us two statistics (numbers). In order to calculate these numbers, we
first need to measure the heights and weights of a bunch of people.&lt;/p&gt;

&lt;p&gt;Once we calculate these two numbers, we have a formula for predicting height;
you give the formula a weight, and it will give you back a predicted height.&lt;/p&gt;

&lt;p&gt;The formula is a best-fit line, as shown in this image &lt;a href="http://en.wikipedia.org/wiki/File:Linear_regression.svg"&gt;from Wikipedia&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src="regression.png" alt="Regression" /&gt;&lt;/p&gt;

&lt;p&gt;Let’s say we want to know
&lt;a href="http://en.wikipedia.org/wiki/Gas_laws#Charles.27s_law"&gt;how big a balloon gets depending on the temperature of the air&lt;/a&gt;.
We fill a bunch of balloons with air and put them in different places,
each with a different air temperature. In the plot above, each dot would
be a balloon, the numbers along the x-axis (the bottom) would be the
temperature, and the numbers along the y-axis would be the volume of the
balloon. That is the data we started with.&lt;/p&gt;

&lt;p&gt;Using these data, we calculate those two regression statistics I mentioned
above. With these, we can draw the red regression line.&lt;/p&gt;

&lt;p&gt;When we want to predict what the volume of a balloon will be at a particular temperature,
we find the temperature on the x-axis, follow it vertically up to the red line,
then follow it horizontally to the y-axis. This is our predicted volume.&lt;/p&gt;

&lt;h2 id="why-statistics-and-math-and-doodles"&gt;Why statistics and math and doodles&lt;/h2&gt;
&lt;p&gt;Statistics lets us distill our complex observations of the world into simple
numbers that are easier to understand. Covariance, variance, correlation
and least-squares regression are some statistics that are commonly used. The
text explains why we use them, and the video explains how we calculate them.&lt;/p&gt;

&lt;p&gt;The formulae for these statistics get a bit confusing when you write them
out as symbols, but math can always be drawn, and it usually makes more sense
that way.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <id>tag:www.thomaslevine.com,2013-09-10:/!/street-sign-protocol/index.html</id>
    <title type="html">The Street Sign Protocol</title>
    <published>2013-09-10T00:00:00Z</published>
    <updated>2013-09-10T00:00:00Z</updated>
    <link rel="alternate" href="http://www.thomaslevine.com/!/street-sign-protocol/index.html"/>
    <content type="html">&lt;p&gt;There are a lot of acronyms on the internet that end in “P”,
like TCP, IP, and HTTP. This “P” stands for “protocol”.
Perhaps you are wondering what a protocol is.&lt;/p&gt;

&lt;p&gt;“Protocol” is one of those words that sounds more special than it really is.
The concept of a computer protocol might sound quite complex, but
it’s a rather generic concept that is something like a language,
an expectation, and a social norm.&lt;/p&gt;

&lt;p&gt;The way a particular protocol works might be quite complicated, but
I just want to explain the generic concept of a protocol. I’m going
to do this by writing about a protocol that you might not think of
as a protocol.&lt;/p&gt;

&lt;h2 id="the-street-sign-protocol"&gt;The street sign protocol&lt;/h2&gt;
&lt;p&gt;I’m going to document a protocol that we might not usually think of a
protocol. Let’s call it the Street Sign Protocol (SSP).&lt;/p&gt;

&lt;p&gt;&lt;img src="street-signs.jpg" alt="Street signs" /&gt;
&lt;!-- http://farm3.staticflickr.com/2132/2302062601_dd0f89779d.jpg
     http://oaklandwiki.org/Street_Signs --&gt;&lt;/p&gt;

&lt;p&gt;To keep things simple, I’ll actually document a simple version of the
Street Sign Protocol. Let’s call it the Simple Street Sign Protocol (SSSP).&lt;/p&gt;

&lt;h3 id="why-we-might-want-sssp"&gt;Why we might want SSSP&lt;/h3&gt;
&lt;p&gt;You might already be familiar with how street signs work, but it took you a while
to figure it out. Also, you are probably so familiar with street signs that you
don’t really think about all of the small details that are involved in communication
through street signs.&lt;/p&gt;

&lt;p&gt;If we wanted to tell a robot how to communicate through street signs, it would be
helpful to be very explicit and precise about all of the aspects of street signs.&lt;/p&gt;

&lt;h3 id="specification"&gt;Specification&lt;/h3&gt;
&lt;p&gt;SSSP is a way of exchanging the names of &lt;em&gt;streets&lt;/em&gt;. It involves
&lt;em&gt;street signs&lt;/em&gt; mounted on &lt;em&gt;poles&lt;/em&gt; near the &lt;em&gt;corners&lt;/em&gt; of
&lt;em&gt;four-way intersections&lt;/em&gt;. Let’s define these terms.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A &lt;em&gt;street&lt;/em&gt; is a paved public thoroughfare in a built environment.
  (This definition is taken from &lt;a href="http://en.wikipedia.org/wiki/Street"&gt;Wikipedia&lt;/a&gt;.)&lt;/li&gt;
  &lt;li&gt;A &lt;em&gt;four-way intersection&lt;/em&gt; is a place where two streets cross.
  Its shape is approximately a rectangle, with one side having the
  width of one street and the other side having the width of the
  other street.&lt;/li&gt;
  &lt;li&gt;Being rectangular, a four-way intersection has four &lt;em&gt;corners&lt;/em&gt; at the
  usual places.&lt;/li&gt;
  &lt;li&gt;A &lt;em&gt;pole&lt;/em&gt; is a long cylinder sticking out of the ground.&lt;/li&gt;
  &lt;li&gt;A &lt;em&gt;street sign&lt;/em&gt; is a flat rectangular thing that has text on both faces.
  The sign is oriented such that the text reads left-to-right.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now we can define the SSSP in outrageous precision and verbosity! &lt;em&gt;You’ll probably
get confused as you read the paragraphs below, so keep in mind that this is just
a very precise and verbose explanation of what we usually consider to be street signs&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Each four-way intersection has four corners, and each pole is associated with
a corner of an intersection. Each corner must have zero or one poles, and at least
one of the corners of a four-way intersection must have a pole; thus, an
intersection must have between one and four poles.&lt;/p&gt;

&lt;p&gt;Poles must be mounted within about ten feet of a corner of the intersection
but not on the paved area of the street or the intersection. (It is okay for them
to be mounted on the sidewalk.) It must stick straight out of the ground; the long
axis of the cylinder must be approximately in line with the direction of gravity.&lt;/p&gt;

&lt;p&gt;Each street sign displays the name of one street. The street sign must include the
name of the street in very large text. This name must be written on both faces of
the sign and must be especially easy to see, even at night.&lt;/p&gt;

&lt;p&gt;Street signs are mounted on poles, each pole having exactly two street signs.
The street signs must be aligned in a particular way. One of the faces must be
just-barely-touching (tangent) the pole. The sign must also line up with its
corresponding street; that is, the wide axis of the rectangular sign must run
parallel the corresponding street.&lt;/p&gt;

&lt;p&gt;Each pole must contain two street signs, each one corresponding to a different
one of the two streets at the four-way intersection.&lt;/p&gt;

&lt;h3 id="implementing-a-sssp-writer"&gt;Implementing a SSSP writer&lt;/h3&gt;
&lt;p&gt;Here is one possible procedure for encoding street names in SSSP. This procedure
expects a four-way intersection and the names of the two streets as input. It
outputs SSSP (two street signs mounted to a pole near the intersection).&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Determine which corner(s) of the intersections the street signs should be
 mounted on. You might consider the locations of buildings, the presence
 of sidewalks, the traffic patterns and the presence of visual obstacles
 like trees.&lt;/li&gt;
  &lt;li&gt;Print the signs with the street names.&lt;/li&gt;
  &lt;li&gt;Cast a pole.&lt;/li&gt;
  &lt;li&gt;Put a pole and street signs on a truck.&lt;/li&gt;
  &lt;li&gt;Drive the truck to the intersection, and park nearby.&lt;/li&gt;
  &lt;li&gt;Carry the materials and some tools to the intersection.&lt;/li&gt;
  &lt;li&gt;Stick the pole in the ground near the chosen corner. I imagine that this
 involves putting up caution tape, digging a hole, securing the pole, pouring
 some concrete and covering it back up, but I don’t really know. If this were
 software, I’d try to use separate pole-installation library so I don’t have
 to implement the pole-installation procedure myself.&lt;/li&gt;
  &lt;li&gt;Mount the street signs to the pole. Position them about ten feet above the ground,
 with one on top of the other and with the centers of the signs touching the pole,
 and otherwise in the appropriate orientations specified by the protocol. Secure
 them with a sign bracket.&lt;/li&gt;
  &lt;li&gt;Drive the truck back to wherever you got it from.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id="implementing-a-sssp-reader"&gt;Implementing a SSSP reader&lt;/h3&gt;
&lt;p&gt;Here is a procedure for decoding SSSP. It has the opposite inputs and outputs as
the SSSP writer.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Look for poles with street signs at the four corners of the four-way intersection.&lt;/li&gt;
  &lt;li&gt;Focus on the first valid pole that you see; ignore any others.&lt;/li&gt;
  &lt;li&gt;Do the following for each of the two street signs.&lt;/li&gt;
  &lt;li&gt;Read the large text on the sign.&lt;/li&gt;
  &lt;li&gt;Associate this large text with the street that the sign lines up with.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id="hypertext-transfer-protocol"&gt;Hypertext Transfer Protocol&lt;/h2&gt;
&lt;p&gt;Now let’s talk about something that people actually call a protocol.
Here’s a simplified version of the Hypertext Transfer Protocol (HTTP).&lt;/p&gt;

&lt;h3 id="why-we-use-http"&gt;Why we use HTTP&lt;/h3&gt;
&lt;p&gt;When you look at stuff on the internet with a web browser, your web browser and
various web servers are sending messages between each other. HTTP is a common
language that all of these computers use.&lt;/p&gt;

&lt;p&gt;While all of these web servers are expecting HTTP, you can actually send them
anything you want. If it is sort of like HTTP but with a few typos, the web
server might figure out what you meant. But if you sent random garbage, the
web server would not have any idea of what you wanted.&lt;/p&gt;

&lt;p&gt;We have common internet languages for the same reasons that we have common
human languages. If you talk to me in English, I’ll understand you quite well.
I’ll make sense of it even you you are not using Queen’s English. But I won’t
really have any idea what you mean if you use sign language.&lt;/p&gt;

&lt;p&gt;We agree to use HTTP on all of these different computers because it makes it
easier for the computers to understand each other.&lt;/p&gt;

&lt;h3 id="highly-simplified-specification"&gt;Highly simplified specification&lt;/h3&gt;
&lt;p&gt;HTTP is a way of exchanging information between a web browser and a web server.
It involves &lt;em&gt;messages&lt;/em&gt;, which are very long series of words, punctuation and
spaces. HTTP prescribes who sends and receives these messages and how these
messages are formatted.&lt;/p&gt;

&lt;p&gt;Some messages have a &lt;em&gt;body&lt;/em&gt;. This is an embedded series of words, punctuation
and spaces that can be written in any format you want.&lt;/p&gt;

&lt;p&gt;Each message may have a bunch of &lt;em&gt;headers&lt;/em&gt;. Each header has a name and a value.
There are a bunch of headers that provide some information about the body
(like its size or format), and there are a bunch of headers that provide
information about the system that is sending the message. And there are others,
like the date of the message.&lt;/p&gt;

&lt;p&gt;Each message is either a request or a response. By being a request, a message
indicates that it came from a web browser and is being sent to a web server.
By being a response, a message indicates that it came from a web server and
is being sent to a web browser. Each response sent from a particular server
to a particular browser must be initiated by a request sent from the
particular browser to particular server.&lt;/p&gt;

&lt;p&gt;Every response has a body (explained above) and a status code. The status code
is a number that explains whether the request succeeded and any quirks about its
success or failure. For example, status code &lt;code&gt;200&lt;/code&gt; means that the request worked
as expected, and status code &lt;code&gt;403&lt;/code&gt; means that you don’t have permission to do
whatever you tried to do.&lt;/p&gt;

&lt;p&gt;Each request must have a method. There are a bunch of methods, and you can think
of them as different commands. Here are a few of them.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;POST&lt;/code&gt; is the method that asks the server to save a new document.
  It contains a body.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;PUT&lt;/code&gt; is the method that asks the server to edit an existing document.
  It contains a body.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;GET&lt;/code&gt; is the method that asks the server to send an existing document in the
  body of the response (and not to alter it). The request does not contain a body.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;HEAD&lt;/code&gt; is the method that asks the server to do everything that it would do in
  for an equivalent &lt;code&gt;GET&lt;/code&gt; request except for sending the body. Like the &lt;code&gt;GET&lt;/code&gt;
  request, the &lt;code&gt;HEAD&lt;/code&gt; request contains no body.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id="receiving-and-reading-and-writing-and-sending"&gt;Receiving and reading and writing and sending&lt;/h3&gt;
&lt;p&gt;A bunch of things in your web browser might initiate a request. For example, opening
a web page makes one request, and loading an image on the web page makes another request.&lt;/p&gt;

&lt;p&gt;A computer somewhere (Call it the “web server”.) is running a particular piece of software
(Call it the “HTTP server”.) that is able to receive this request.
The HTTP server receives the request, checks that it is valid, and breaks it into the
method, headers, and body.&lt;/p&gt;

&lt;p&gt;Then the HTTP server asks something else to decide what to do.
After doing everything, this other thing decides what the outcome was and tells the HTTP server.&lt;/p&gt;

&lt;p&gt;The HTTP server composes an HTTP response and
sends that back to the browser.&lt;/p&gt;

&lt;p&gt;The browser breaks that into its various parts and accordingly displays a web page,
shows an image or does whatever else was specified.&lt;/p&gt;

&lt;h2 id="a-protocol-is-an-expectation"&gt;A protocol is an expectation&lt;/h2&gt;
&lt;p&gt;“Protocol” means the same thing for computers as it does for people.
There are lots of different protocols, and some of them are very complicated,
but the word “protocol” itself has a rather simple meaning; it’s just
a way that we expect things to work.&lt;/p&gt;

&lt;p&gt;Like how we agree to speak the same language (probably English if you’re reading this)
or to drive on the same side of the street, we agree on a computer protocol because that
makes it easier for us to make computers communicate. We can choose to build computers
that don’t follow standard protocols, but that would make them hard to understand.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <id>tag:www.thomaslevine.com,2013-09-03:/!/socrata-formats/index.html</id>
    <title type="html">What file formats are on the data portals?</title>
    <published>2013-09-03T00:00:00Z</published>
    <updated>2013-09-03T00:00:00Z</updated>
    <link rel="alternate" href="http://www.thomaslevine.com/!/socrata-formats/index.html"/>
    <content type="html">&lt;style&gt;
table {
  font-size: 0.7em;
}
&lt;/style&gt;

&lt;p&gt;I found some more open data about open data to &lt;a href="/socrata"&gt;study&lt;/a&gt;!
While &lt;a href="http://www.socrata.com/blog/my-visit-to-socrata-and-data-analysis-about-data-analysis/"&gt;at Socrata’s office&lt;/a&gt; on Friday,
I learned of the &lt;a href="https://data.oregon.gov/data.json"&gt;&lt;code&gt;/data.json&lt;/code&gt;&lt;/a&gt; endpoint.
It contains an entry for each &lt;a href="/!/socrata-genealogies/#term-dataset"&gt;dataset&lt;/a&gt;,
uploaded by the data publisher; it doesn’t contain all of the other
views that are based on these source datasets.
And it has &lt;a href="http://project-open-data.github.io/schema/"&gt;this format&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id="how-many-datasets"&gt;How many datasets?&lt;/h2&gt;
&lt;p&gt;Socrata portals have &lt;a href="/!/socrata-users/#the-user-data-format"&gt;50,000 different views&lt;/a&gt;, but only
8922 are original datasets.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;/data.json&lt;/code&gt; files include federated datasets, so some of these
datasets are duplicated. I did not remove duplicates, so I’m working
with 15699 datasets, with a median of
96 datasets per portal.&lt;/p&gt;

&lt;p&gt;&lt;a href="figure/portal-counts.png"&gt;&lt;img src="figure/portal-counts.png" alt="Datasets per portal, based on the /data.json file" class="wide" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id="no-derived-datasets"&gt;No derived datasets&lt;/h3&gt;
&lt;p&gt;This is much lower than my &lt;a href="/!/socrata-summary"&gt;earlier figure&lt;/a&gt;
because the present figure does not include &lt;a href="/!/socrata-genealogies#soda-queries-filtered-views-charts-maps"&gt;derived views&lt;/a&gt; (map, charts, &amp;amp;c.).&lt;/p&gt;

&lt;p&gt;Some time, I’ll compare the within-portal counts of original datasets
and derived datasets. But not right now.&lt;/p&gt;

&lt;h3 id="federated-data"&gt;Federated data&lt;/h3&gt;
&lt;p&gt;It still includes &lt;a href="/!/socrata-genealogies/#term-federation"&gt;federated data&lt;/a&gt; (duplicates),
however, and this file doesn’t make it easy to determine which direction
the federation is in. The following plot gives us an idea of how many of
these datasets are duplicates.&lt;/p&gt;

&lt;p&gt;&lt;a href="figure/federation.png"&gt;&lt;img src="figure/federation.png" alt="The scale of data federation within this subset of Socrata datasets" class="wide" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Most of the datasets are not duplicates, but some are duplicated many times.&lt;/p&gt;

&lt;p&gt;Some day, I’ll look more at federation, probably by 
reading the federation information from home pages of the portals
or by following the links in the &lt;code&gt;/data.json&lt;/code&gt; file.&lt;/p&gt;

&lt;h3 id="cutoff-at-1000"&gt;Cutoff at 1000?&lt;/h3&gt;
&lt;p&gt;I find it highly suspicious the following nine portals have exactly 1000
datasets and no portals have more than 1000 datasets.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;paste(names(sort(table(datasets$portal), decreasing = T)[1:9]), collapse = "\n")
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href="https://bronx.lehman.cuny.edu"&gt;bronx.lehman.cuny.edu&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="https://data.cityofnewyork.us"&gt;data.cityofnewyork.us&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="https://data.hawaii.gov"&gt;data.hawaii.gov&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="https://data.illinois.gov"&gt;data.illinois.gov&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="https://data.ny.gov"&gt;data.ny.gov&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="https://data.ok.gov"&gt;data.ok.gov&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="https://data.oregon.gov"&gt;data.oregon.gov&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="https://explore.data.gov"&gt;explore.data.gov&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="https://www.metrochicagodata.org"&gt;www.metrochicagodata.org&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Half of these portals federate &lt;code&gt;explore.data.gov&lt;/code&gt;, which has quite a few datasets, and the JSON
files seem to just include some of the &lt;code&gt;explore.data.gov&lt;/code&gt; data. I think these
files have only the first 1000 datasets, and I haven’t figured out how to look
at the next pages, so I’ll focus the present analysis on portals with fewer
than 1000 datasets.&lt;/p&gt;

&lt;h2 id="my-curiousity-about-file-formats"&gt;My curiousity about file formats&lt;/h2&gt;
&lt;p&gt;I’ve recently become curious about what formats the datasets come from. When
tabular data get loaded into a Socrata data portal, they get converted to a
tabular representation within the portal software. From that, they get converted
to a range of different tabular formats.&lt;/p&gt;

&lt;p&gt;The Socrata data portal doesn’t explicitly store the source format because of
how the import process works. Most of the data &lt;a href="http://blog.scraperwiki.com/2012/07/31/do-all-analysts-use-excel/"&gt;probably come from Excel&lt;/a&gt;,
and the data that aren’t from Excel typically come from inside of a government
network where policies would make it inconvenient to expose the database to the
world. Because of this, Socrata doesn’t query database servers. Instead, data
publishers write middlemen that act as both database clients and Socrata clients.
They query the database and then make &lt;a href="http://dev.socrata.com/publishers/getting-started"&gt;web requests&lt;/a&gt;
to the Socrata portal.&lt;/p&gt;

&lt;h2 id="datajson-contains-file-format-information"&gt;&lt;code&gt;/data.json&lt;/code&gt; contains file format information&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;/data.json&lt;/code&gt; endpoint contains a file format field per the Project Open Data
schema. This refers to the format of the data as served from the Socrata portal,
not the format it was stored in before it got to the Socrata portal. But this
still tells us something about the source file formats.&lt;/p&gt;

&lt;p&gt;People sometimes upload things that Socrata doesn’t interpret as tables. PDFs are
a major example. Other times, people upload or link to
files that could be tables but don’t specify that they are tables; those are called
“external links”. Read more on dataset types &lt;a href="/!/open-by-default#types-of-visualizations-on-socrata-portals"&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Data formats are represented in two fields, &lt;code&gt;format&lt;/code&gt; and &lt;code&gt;distribution&lt;/code&gt;. &lt;code&gt;distribution&lt;/code&gt;{#distribution}
seems to contain all of the different available formats. If the data are imported as
tabular data, it contains CSV, JSON, XML, &amp;amp;c., all served from the Socrata site.
And if the data are external links, it will contain a few external links, still
specifying the file types. The &lt;code&gt;format&lt;/code&gt; field contains one of the formats that are
specified in the &lt;code&gt;distribution&lt;/code&gt; field. I think it’s just the first of the formats.
For the present analysis, I’m using the &lt;code&gt;format&lt;/code&gt; field.&lt;/p&gt;

&lt;p&gt;Recall that the present dataset of datasets counts federated datasets multiple times.
The following plot shows the file types of the deduplicated dataset dataset, across
all portals.&lt;/p&gt;

&lt;p&gt;&lt;a href="figure/deduplicated.png"&gt;&lt;img src="figure/deduplicated.png" alt="Formats of datasets across all portals" class="wide" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;And here are some of the main types by portal, counting federated datasets in all of their portals.&lt;/p&gt;

&lt;p&gt;&lt;a href="figure/all-formats.png"&gt;&lt;img src="figure/all-formats.png" alt="Data formats on all the portals" class="wide" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;csv&lt;/code&gt; mostly refers
to data that Socrata represents as a table; this is the sort of data that Socrata
can convert to a range of different tabular data formats.
It is also my &lt;a href="http://csvsoundsystem.com"&gt;preferred file format&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Everything that is not CSV appears to be an external link.&lt;/p&gt;

&lt;h2 id="csv"&gt;CSV&lt;/h2&gt;
&lt;p&gt;Most datasets are CSV (8143 of 15699).
I was curious as to how this varies by portal and over time, and the following image
addresses that.&lt;/p&gt;

&lt;p&gt;&lt;a href="figure/csv-cum-facet.png"&gt;&lt;img src="figure/csv-cum-facet.png" alt="Dataset formats by portal over time" class="wide" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The image above contains one plot per data portal. The x-axis of each plot is the date,
the y-axis is the proportion&lt;sup id="fnref:proportion"&gt;&lt;a href="#fn:proportion" class="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt; of datasets that are tabular (CSV), and the
width of the line is the number of datasets on the portal.
For example, if there were 100 datasets on a portal in June 2011 and 80 were CSV, the
line would be near the top of the graph and quite skinny at June 2011.&lt;/p&gt;

&lt;p&gt;We can thus see how many datasets each portal has and what the different formats are.&lt;/p&gt;

&lt;h2 id="some-interesting-portals"&gt;Some interesting portals&lt;/h2&gt;
&lt;p&gt;Some portals have only CSV data (like &lt;code&gt;data.medicare.gov&lt;/code&gt;), but most contain
other data. I am curious both as to what other data formats they have and what
prompted the shifts in dataset format.&lt;/p&gt;

&lt;p&gt;Missouri mostly has PDFs.&lt;/p&gt;

&lt;p&gt;&lt;a href="figure/mo.png"&gt;&lt;img src="figure/mo.png" alt="Data formats on data.mo.gov" class="wide" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Also interesting about Missouri is that it federates &lt;a href="https://data.kcmo.org/"&gt;Kansas City&lt;/a&gt;,
which didn’t appear in my list of portals.&lt;/p&gt;

&lt;p&gt;I know I said I’d focus on portals with fewer than 1000 datasets, but Lehman College is
interesting because it has lots of zipped files.&lt;/p&gt;

&lt;p&gt;&lt;a href="figure/lehman.png"&gt;&lt;img src="figure/lehman.png" alt="Data formats on bronx.lehman.cuny.edu" class="wide" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;San Francisco has a lot of CSVs, a lot of externally linked zip files,
and a lot of externally linked files of unknown format.&lt;/p&gt;

&lt;p&gt;&lt;a href="figure/sf.png"&gt;&lt;img src="figure/sf.png" alt="Data formats on data.sfgov.org" class="wide" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id="determination-of-external-link-file-formats"&gt;Determination of external link file formats&lt;/h2&gt;
&lt;p&gt;It looks like the format of external links is determined by the file name.
For example, Edmonton’s
&lt;a href="https://data.edmonton.ca/Transportation/Road-and-Traffic-Updates/5ggc-prfp?"&gt;Road and Traffic Updates&lt;/a&gt;
are marked as &lt;code&gt;application/rss+xml&lt;/code&gt; because the external link,
&lt;a href="http://www.trumba.com/calendars/construction-and-special-events-road-closures.rss"&gt;http://www.trumba.com/calendars/construction-and-special-events-road-closures.rss&lt;/a&gt;,
ends in &lt;code&gt;.rss&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;In contrast, the &lt;a href="https://data.maryland.gov/d/ywbg-ptfh"&gt;Maryland Land Use/ Land Cover: 1973, 2002, 2010&lt;/a&gt;
dataset is marked as having the format &lt;code&gt;application/octet-stream&lt;/code&gt; because the
external link, &lt;a href="http://planning.maryland.gov/OurWork/landUseDownload.shtml"&gt;http://planning.maryland.gov/OurWork/landUseDownload.shtml&lt;/a&gt;,
ends in &lt;code&gt;.shtml&lt;/code&gt;.&lt;/p&gt;

&lt;!-- sqldf("select title, 'https://' || portal || '/d/' || identifier AS portalURL, accessURL from catalog where identifier = 'ywbg-ptfh'", dbname = '/tmp/catalog.db') --&gt;

&lt;h2 id="dates-of-significant-changes"&gt;Dates of significant changes&lt;/h2&gt;
&lt;p&gt;A few portals have only CSV data since the beginning, but most have had other formats.
Looking at the plots, we can see dates where there was a sudden change in the proportion
of datasets that were CSV.&lt;/p&gt;

&lt;h3 id="sudden-changes-at-the-beginning"&gt;Sudden changes at the beginning&lt;/h3&gt;
&lt;p&gt;When the first dataset gets uploaded, the proportion of datasets that are CSV is either
zero or one. Thus, the line for all of these datasets starts either at zero or one.
Most datasets sharply change after that; &lt;code&gt;data.austintexas.gov&lt;/code&gt; and &lt;code&gt;data.mo.gov&lt;/code&gt; are
examples.&lt;/p&gt;

&lt;p&gt;Others stay at this level for quite a while because no datasets were uploaded for a
while. &lt;code&gt;data.raleighnc.gov&lt;/code&gt; is an example of this. Here are its first ten datasets.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;title&lt;/th&gt;
      &lt;th&gt;created&lt;/th&gt;
      &lt;th&gt;format&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href="https://data.raleighnc.gov/d/s68n-gffw"&gt;Building Permit Data&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;2012-03-14&lt;/td&gt;
      &lt;td&gt;text/csv&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href="https://data.raleighnc.gov/d/pep8-sb8v"&gt;Building Permit Data&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;2012-03-14&lt;/td&gt;
      &lt;td&gt;text/csv&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href="https://data.raleighnc.gov/d/fuys-kh3c"&gt;City of Raleigh Quickfacts&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;2013-02-19&lt;/td&gt;
      &lt;td&gt;text/csv&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href="https://data.raleighnc.gov/d/g3uq-k7zm"&gt;Raleigh Parking&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;2013-02-28&lt;/td&gt;
      &lt;td&gt;application/zip&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href="https://data.raleighnc.gov/d/jrpi-4amz"&gt;Raleigh Electric Utilities 2011&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;2013-02-28&lt;/td&gt;
      &lt;td&gt;application/zip&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href="https://data.raleighnc.gov/d/fcx2-d4t3"&gt;Raleigh Communications 2011&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;2013-02-28&lt;/td&gt;
      &lt;td&gt;application/zip&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href="https://data.raleighnc.gov/d/46tk-23jt"&gt;Raleigh Buildings 2011&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;2013-02-28&lt;/td&gt;
      &lt;td&gt;application/zip&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href="https://data.raleighnc.gov/d/3fmi-wyx6"&gt;Raleigh Parks and Trails&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;2013-02-28&lt;/td&gt;
      &lt;td&gt;application/zip&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href="https://data.raleighnc.gov/d/pwv5-a5ca"&gt;Raleigh Trail Areas&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;2013-02-28&lt;/td&gt;
      &lt;td&gt;application/zip&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href="https://data.raleighnc.gov/d/apbx-xr7f"&gt;Family Income In The Past 12 Months&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;2013-02-28&lt;/td&gt;
      &lt;td&gt;text/csv&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The first two datasets were uploaded in the middle of March 2012 and were CSV format,
making the datasets 100% CSV. The next was uploaded in the middle of February 2013 and
was also CSV, so the proportion was still 100% CSV. At the end of the month, six
zip files were uploaded, reducing the proportion to 33% CSV.&lt;/p&gt;

&lt;h3 id="sudden-changes-after-the-beginning"&gt;Sudden changes after the beginning&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;data.sfgov.org&lt;/code&gt; had a sudden change in the proportion of datasets that were CSV, but
it was after a long while, so a lot of related datasets might have been uploaded all
at once. Let’s look at when datasets were uploaded.&lt;/p&gt;

&lt;p&gt;&lt;a href="figure/sf-changes.png"&gt;&lt;img src="figure/sf-changes.png" alt="Formats of newly open San Francisco datasets over time" class="wide" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This has the quite similar information to the earlier plots, but it’s a bit more precise.
San Francisco added lots of datasets in January 2012, November 2012, and December 2012, and proportionately
few of these datasets were CSV. What were they?&lt;/p&gt;

&lt;p&gt;Here are ten of the January datasets.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;title&lt;/th&gt;
      &lt;th&gt;created&lt;/th&gt;
      &lt;th&gt;format&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href="https://data.sfgov.org/d/2ivi-ywmk"&gt;Arterial Streets of San Francisco (Zipped Shapefile Format)&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;2012-01-01&lt;/td&gt;
      &lt;td&gt;application/zip&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href="https://data.sfgov.org/d/2xc9-is4u"&gt;Orthophoto 1ft resolution (1993) - (Zipped MrSID Format)&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;2012-01-01&lt;/td&gt;
      &lt;td&gt;application/zip&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href="https://data.sfgov.org/d/3vyz-qy9p"&gt;City Lots (Zipped Shapefile Format)&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;2012-01-01&lt;/td&gt;
      &lt;td&gt;application/zip&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href="https://data.sfgov.org/d/4aaa-ycik"&gt;Census 2000 Block Group (No Water) (Zipped Shapefile Format)&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;2012-01-01&lt;/td&gt;
      &lt;td&gt;application/zip&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href="https://data.sfgov.org/d/4drs-6tjy"&gt;Orthophoto 1ft (1993) - Treasure Island (Zipped MrSID Format)&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;2012-01-01&lt;/td&gt;
      &lt;td&gt;application/zip&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href="https://data.sfgov.org/d/4mzs-yjt7"&gt;SFPD Sectors&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;2012-01-01&lt;/td&gt;
      &lt;td&gt;application/zip&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href="https://data.sfgov.org/d/5aii-qc4e"&gt;SFPD Crime Reporting Plots (Zipped Shapefile Format)&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;2012-01-01&lt;/td&gt;
      &lt;td&gt;application/zip&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href="https://data.sfgov.org/d/5fxg-wene"&gt;Neighborhood Marketplace Initiative Corridors (Zipped Shapefile Format)&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;2012-01-01&lt;/td&gt;
      &lt;td&gt;application/zip&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href="https://data.sfgov.org/d/5rn4-fswj"&gt;San Francisco Basemap Street Centerlines (Zipped Shapefile Format)&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;2012-01-01&lt;/td&gt;
      &lt;td&gt;application/zip&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href="https://data.sfgov.org/d/5sny-6aph"&gt;The Presidio of San Francisco (Zipped Shapefile Format)&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;2012-01-01&lt;/td&gt;
      &lt;td&gt;application/zip&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;It looks like January is mostly externally linked, zipped shapefiles. Most of the
datasets say “shapefile” in their &lt;code&gt;title&lt;/code&gt;, &lt;code&gt;description&lt;/code&gt; or &lt;a href="#distribution"&gt;&lt;code&gt;distribution&lt;/code&gt;&lt;/a&gt; fields.&lt;/p&gt;

&lt;p&gt;&lt;a href="figure/sf-shapefile.png"&gt;&lt;img src="figure/sf-shapefile.png" alt="Formats of newly open San Francisco datasets over time" class="wide" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;And a lot of the rest of the January files look like zipped shapefiles, even though the
titles and descriptions don’t say so.&lt;/p&gt;

&lt;p&gt;So San Francisco suddenly uploaded a bunch of shapefiles in January and November/December.&lt;/p&gt;

&lt;h3 id="plateaus-of-dataset-counts"&gt;Plateaus of dataset counts&lt;/h3&gt;
&lt;p&gt;Some of the plots make it look like lots of datasets were suddenly uploaded one time
and no datasets were uploaded again. This is mainly for the
&lt;a href="#cutoff-at-1000"&gt;portals with more than 1000 datasets&lt;/a&gt;,
so I think this is because we’re seeing only the first 1000 datasets.&lt;/p&gt;

&lt;h2 id="csv-pdf-zip-and-octet-stream"&gt;CSV, pdf, zip and octet-stream&lt;/h2&gt;
&lt;p&gt;Based on the examples above, it seems like a lot of datasets are PDF, zip or unknown
external links. I made the following series of plots to check it. It is just like the
&lt;a href="#csv"&gt;similar image above&lt;/a&gt; except for the y-axes; instead of representing the
proportion of datasets that are CSV, each y-axis represents the proportion of datasets
that are CSV, PDF, zip or unknown external links.&lt;/p&gt;

&lt;p&gt;&lt;a href="figure/csv-pdf-zip-octet-cum-facet.png"&gt;&lt;img src="figure/csv-pdf-zip-octet-cum-facet.png" alt="Dataset formats by portal over time" class="wide" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Most of the curves are pretty straight and stay near 1, meaning that the proportion doesn’t
change much and that the proportion is quite high. Thus, it looks like most datasets are
either CSV, pdf, zip or external links of unknown format.&lt;/p&gt;

&lt;h2 id="conclusions"&gt;Conclusions&lt;/h2&gt;

&lt;h3 id="formats"&gt;Formats&lt;/h3&gt;
&lt;p&gt;I was mainly wondering about the source formats of the data. It turns out that most datasets
are tables. Aside from tables, there are lots of externally linked files, mostly PDFs and zip archives.&lt;/p&gt;

&lt;h3 id="shapefiles"&gt;Shapefiles&lt;/h3&gt;
&lt;p&gt;I’ve been thinking recently about how to infer the organizational structure of a municipality
based on the open data that they release, and the finding with the San Francisco shapefiles
alludes to this. It might be that one department within the San Francisco government manages
and uses most of the geospatial data and that the open data team happened to work with them
in January, November and December of 2012.&lt;/p&gt;

&lt;h2 id="future-study"&gt;Future study&lt;/h2&gt;
&lt;p&gt;This got me thinking about other ways of studying file formats.&lt;/p&gt;

&lt;h3 id="the-attribution-field"&gt;The attribution field&lt;/h3&gt;
&lt;p&gt;Socrata’s SODA 1 API, which I’ve &lt;a href="/!/socrata-summary/#download-dataset-metadata"&gt;used before&lt;/a&gt;,
contains an &lt;code&gt;attribution&lt;/code&gt; field, which references the URL from
which the dataset was taken.&lt;sup id="fnref:attribution"&gt;&lt;a href="#fn:attribution" class="footnote"&gt;2&lt;/a&gt;&lt;/sup&gt; This would be one
way of figuring out the source format, or at least some related information about the source.&lt;/p&gt;

&lt;h3 id="externally-linked-csvs"&gt;Externally linked CSVs&lt;/h3&gt;
&lt;p&gt;Not all of the CSV-formatted datasets necessarily come from Socrata; some might be links
to external CSV files. There is enough information in &lt;code&gt;/data.json&lt;/code&gt; to determine which of
these categories a CSV dataset falls into.&lt;/p&gt;

&lt;h3 id="determining-the-formats-of-external-links"&gt;Determining the formats of external links&lt;/h3&gt;
&lt;p&gt;It looks to me like the format type of external links is determined based on the file
extension of the URL; &lt;code&gt;octet-stream&lt;/code&gt; datasets seem to correspond to URLs without file
extensions or with file extensions like &lt;code&gt;aspx&lt;/code&gt; that don’t clearly correspond to a
particular file types. One could determine the formats of these datasets by
downloading the files.&lt;/p&gt;

&lt;h2 id="footnotes"&gt;Footnotes&lt;/h2&gt;

&lt;div class="footnotes"&gt;
  &lt;ol&gt;
    &lt;li id="fn:proportion"&gt;
      &lt;p&gt;It’s actually a tad bit more complicated than that. These dates are the
creation dates of the datasets that are available today; I do not know about datasets
that were historically on the portal and have since been deleted.&lt;a href="#fnref:proportion" class="reversefootnote"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id="fn:attribution"&gt;
      &lt;p&gt;I presume that this is entered manually.&lt;a href="#fnref:attribution" class="reversefootnote"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</content>
  </entry>
</feed>

