<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>http://www.thomaslevine.com/</id>
  <title>Thomas Levine</title>
  <updated>2013-12-02T00:00:00Z</updated>
  <link rel="alternate" href="http://www.thomaslevine.com/"/>
  <link rel="self" href="http://www.thomaslevine.com/!/feed.xml"/>
  <author>
    <name>Thomas Levine</name>
    <uri>http://www.thomaslevine.com</uri>
  </author>
  <entry>
    <id>tag:www.thomaslevine.com,2013-12-02:/!/data-about-open-data-talk-december-2-2013/index.html</id>
    <title type="html">100,000 open data across 100 portal</title>
    <published>2013-12-02T00:00:00Z</published>
    <updated>2013-12-02T00:00:00Z</updated>
    <link rel="alternate" href="http://www.thomaslevine.com/!/data-about-open-data-talk-december-2-2013/index.html"/>
    <content type="html">&lt;p&gt;Here are some materials for
&lt;a href="http://www.meetup.com/NYC-Open-Data/events/147380312/"&gt;my talk at NYC Open Data&lt;/a&gt;,
but they’re written in normal language, so they’ll probably serve as a decent summary
of my work thus far to people who are reading this on the internet.&lt;/p&gt;

&lt;h2 id="schedule"&gt;Schedule&lt;/h2&gt;
&lt;p&gt;The schedule for the talk will go sort of like this&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;7:00 to 7:50 I’ll talk about what I did&lt;/li&gt;
  &lt;li&gt;7:50 to 8:00 Questions about what I did&lt;/li&gt;
  &lt;li&gt;8:00 to 8:10 Introduce the exercises&lt;/li&gt;
  &lt;li&gt;8:10 to 8:45 Work on the exercises&lt;/li&gt;
  &lt;li&gt;8:45 to 9:00 Present findings from the exercises&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here’s how the exercises will work. Attendees will break into groups of four people
each. Each group will choose one of the two exercises and work on it for half an hour. 
At 8:45, each of the groups will have one minute to present its conclusions to the
larger group.&lt;/p&gt;

&lt;h2 id="outline-of-the-talk"&gt;Outline of the talk&lt;/h2&gt;

&lt;p&gt;Introduction (4 minutes)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;What are “open data”? Show the video.&lt;/li&gt;
  &lt;li&gt;Data about open data, data-driven open data&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Two approaches (2 minutes)&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;We have all of these data, so something interesting must be in it.&lt;/li&gt;
  &lt;li&gt;We are interested in something. Let’s collect data that will tell us about that thing.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;My process and findings (6 minutes each)&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Data about open data.&lt;/li&gt;
  &lt;li&gt;Data about people who use open data.&lt;/li&gt;
  &lt;li&gt;Finding data is hard.&lt;/li&gt;
  &lt;li&gt;File formats of open data&lt;/li&gt;
  &lt;li&gt;Licensing of “open” data&lt;/li&gt;
  &lt;li&gt;Updating of data&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Future things (3 minutes)&lt;/p&gt;

&lt;h2 id="data-about-open-data"&gt;Data about open data&lt;/h2&gt;
&lt;p&gt;Let’s talk about some things I’ve been &lt;a href="/open-data"&gt;learning about open data&lt;/a&gt;.
For the longest time, I had no idea what people meant when they were talking
about “open data”. But now I have &lt;a href="/!/open-data-in-plain-english"&gt;this video&lt;/a&gt;
that gives a decent explanation, at least for government data.&lt;/p&gt;

&lt;p&gt;To some degree, “open data” is just the sharing of data, but we have a word for
it because people aren’t used to this idea. Sharing data within just company is
already pretty hard, but good things might happen once you do it.&lt;/p&gt;

&lt;p&gt;One benefit of open data might be the ability for people to use lots of different
datasets in order to make data-driven decisions. The people who are releasing open
data surely get this, so they’re obviously using data to make decisions about their
open data initiatives, right?&lt;/p&gt;

&lt;p&gt;Actually, they’re not, so I started doing that. Also, I’m doing it quite publicly,
so you could say this is open data about open data.&lt;/p&gt;

&lt;h2 id="my-process-and-findings"&gt;My process and findings&lt;/h2&gt;
&lt;p&gt;I was taught in school that you come up with your question and then collect data
that perfectly answer that question. This way works, but you can often learn more
faster and with less work if you’re a bit sloppier.&lt;/p&gt;

&lt;p&gt;I like to think of two approaches of deciding what to study.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;We have all of these data, so something interesting must be in it.&lt;/li&gt;
  &lt;li&gt;We are interested in something. Let’s collect data that will tell us about that thing.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I think the former is more obvious: Initially, I found it quite odd that
nobody had looked at the data about the data. So I did.&lt;/p&gt;

&lt;p&gt;Let’s talk a bit about the latter. Let’s say we want to study someone’s sleep patterns.
In order to do this, we wind to find out when the person is sleeping. We could do this
by having the person record on paper the times at which she goes to sleep and wakes up,
but that would be a lot of work. Other ideas&lt;/p&gt;

&lt;p&gt;&lt;a href="http://yihui.name/en/2009/10/50000-revisions-committed-to-r/"&gt;Version control commits&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src="r-commits.gif" alt="R commits" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://hackpad.com/Measuring-Socioeconomic-Indicators-in-Arabic-Tweets-IZ5ByP2LvIt"&gt;Tweets&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src="tweet-times.jpg" alt="Bar plot of Tweet times" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;If the person is me, we can use shell history activity.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/sh
# This file is history.sh
for epochtime in $(grep '^#[0-9]\{10\}$' ~/.history/sh-2013-1[12]*|cut -d\# -f2); do
  date --date=@$epochtime +%H
done | sort | uniq -c | awk '{print $2, "%"$1"s"}' &amp;gt; /tmp/formatted

while read line; do
  # Remove the first space
  nospace=$(echo $line | sed 's/ //')
  printf "$line\n" | tr \  -|sed s/----------------------------------------/=/g|sed -e s/-//g -e 's/=/ =/'
done &amp;lt; /tmp/formatted
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here’s the resulting histogram.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./history.sh
00 =========
01 =======
02 =====
03 ====
04 =======
05 ============
06 ======
07 ====
08 =
09 =
10 =====
11 ===
12 ==
13 ======
14 =====
15 ======
16 ==============
17 ==============================
18 =======================================
19 ============================
20 ===============
21 ==========
22 ==================
23 ==================
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that these times are in UTC because that’s how I roll.&lt;/p&gt;

&lt;p&gt;In this approach of deciding what to study, the idea is that we can
answer our curiosities by building on some existing data collection.
Also, I have some brief thoughts on brainstorming &lt;a href="/!/brainstorming"&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you want to really understand how I did all of this, pay attention to the following things throughout the talk.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;How I outlined my programs&lt;/li&gt;
  &lt;li&gt;How I created simple variables to represent grand concepts&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id="data-about-data"&gt;Data about data&lt;/h3&gt;

&lt;h4 id="getting-the-data"&gt;Getting the data&lt;/h4&gt;
&lt;p&gt;&lt;a href="/!/socrata-summary"&gt;&lt;img src="/!/socrata-summary/architecture.jpg" alt="Diagram about downloading Socrata data" class="wide" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Now I have a spreadsheet of datasets.&lt;/p&gt;

&lt;p&gt;&lt;a href="/!/dataset-as-datapoint"&gt;&lt;img src="/!/dataset-as-datapoint/spreadsheet-spreadsheet.png" alt="A spreadsheet of spreadsheets" class="wide" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Here are the some of fields I get from that.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;portal&lt;/li&gt;
  &lt;li&gt;id&lt;/li&gt;
  &lt;li&gt;name&lt;/li&gt;
  &lt;li&gt;attribution&lt;/li&gt;
  &lt;li&gt;averageRating&lt;/li&gt;
  &lt;li&gt;category&lt;/li&gt;
  &lt;li&gt;createdAt&lt;/li&gt;
  &lt;li&gt;description&lt;/li&gt;
  &lt;li&gt;displayType&lt;/li&gt;
  &lt;li&gt;downloadCount&lt;/li&gt;
  &lt;li&gt;numberOfComments&lt;/li&gt;
  &lt;li&gt;oid&lt;/li&gt;
  &lt;li&gt;publicationAppendEnabled&lt;/li&gt;
  &lt;li&gt;publicationDate&lt;/li&gt;
  &lt;li&gt;publicationStage&lt;/li&gt;
  &lt;li&gt;publicationGroup&lt;/li&gt;
  &lt;li&gt;rowsUpdatedBy&lt;/li&gt;
  &lt;li&gt;rowsUpdatedAt&lt;/li&gt;
  &lt;li&gt;signed&lt;/li&gt;
  &lt;li&gt;tableId&lt;/li&gt;
  &lt;li&gt;totalTimesRated&lt;/li&gt;
  &lt;li&gt;viewCount&lt;/li&gt;
  &lt;li&gt;viewLastModified&lt;/li&gt;
  &lt;li&gt;viewType&lt;/li&gt;
  &lt;li&gt;nrow&lt;/li&gt;
  &lt;li&gt;column names and types&lt;/li&gt;
  &lt;li&gt;owner.id&lt;/li&gt;
  &lt;li&gt;owner.displayName&lt;/li&gt;
  &lt;li&gt;owner.emailUnsubscribed&lt;/li&gt;
  &lt;li&gt;owner.privacyControl&lt;/li&gt;
  &lt;li&gt;owner.profileLastModified&lt;/li&gt;
  &lt;li&gt;owner.roleName&lt;/li&gt;
  &lt;li&gt;owner.screenName&lt;/li&gt;
  &lt;li&gt;owner.rights&lt;/li&gt;
  &lt;li&gt;tableAuthor.id&lt;/li&gt;
  &lt;li&gt;tableAuthor.displayName&lt;/li&gt;
  &lt;li&gt;tableAuthor.emailUnsubscribed&lt;/li&gt;
  &lt;li&gt;tableAuthor.privacyControl&lt;/li&gt;
  &lt;li&gt;tableAuthor.profileLastModified&lt;/li&gt;
  &lt;li&gt;tableAuthor.roleName&lt;/li&gt;
  &lt;li&gt;tableAuthor.screenName&lt;/li&gt;
  &lt;li&gt;tableAuthor.rights&lt;/li&gt;
  &lt;li&gt;displayFormat&lt;/li&gt;
  &lt;li&gt;flags&lt;/li&gt;
  &lt;li&gt;metadata&lt;/li&gt;
  &lt;li&gt;rights&lt;/li&gt;
  &lt;li&gt;tags&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id="what-i-found"&gt;What I found&lt;/h4&gt;
&lt;p&gt;First, nobody has any idea of what is going on in open data.
This was my main conclusion after I tweeted about &lt;a href="/!/socrata-summary/"&gt;this article&lt;/a&gt;;
I thought it would not be that interesting, but people strangely liked it.
Many people know about datasets that are relevant to their work,
municipality, &amp;amp;c., but nobody seems to know about the availability of
data on broader topics, and nobody seems to have a good way of
finding out what is available. And nobody has a great idea of who
is using which data.&lt;/p&gt;

&lt;p&gt;Second, resolving &lt;a href="/!/socrata-genealogies/#types-of-duplicate-datasets"&gt;duplicate datasets&lt;/a&gt; is annoying. Three types of duplication&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;SODA queries: Filtered views, charts, maps&lt;/li&gt;
  &lt;li&gt;Federation&lt;/li&gt;
  &lt;li&gt;Uploaded twice&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id="data-about-people-who-use-data"&gt;Data about people who use data&lt;/h3&gt;
&lt;p&gt;Let’s look a bit at how people interact with these data. One of Socrata’s
features is built-in charting tools that are supposed to
“&lt;a href="http://www.socrata.com/open-innovation/"&gt;consumeriz[e] the data experience&lt;/a&gt;”
Basically, you can go to &lt;code&gt;data.cityofnewyork.us&lt;/code&gt; or any Socrata site, find
an existing dataset, and make a new chart, map, query, &amp;amp;c. from it.
It turns out that Socrata exposes a lot of knowledge about how this feature
gets used.&lt;/p&gt;

&lt;h4 id="getting-the-data-1"&gt;Getting the data&lt;/h4&gt;
&lt;p&gt;Notice the “owner” and “tableAuthor” fields in the previous download.
These refer to user accounts in Socrata.&lt;/p&gt;

&lt;p&gt;Internally, each new chart is represented as a “view” on the underlying
data “table”.&lt;/p&gt;

&lt;p&gt;&lt;a href="/!/socrata-genealogies#term-table"&gt;&lt;img src="/!/socrata-genealogies/family.jpg" alt="A date table family in Socrata" class="wide" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Anyway, if I use just these columns,
I now have a dataset of users. I didn’t use SQL, but if I had, the
query would have been sort of like this.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SELECT * FROM (
  SELECT 
    "owner.id",
    "owner.displayName",
    "owner.emailUnsubscribed",
    "owner.privacyControl",
    "owner.profileLastModified",
    "owner.roleName",
    "owner.screenName",
    "owner.rights"
  FROM "datasets"
  UNION ALL
  SELECT 
    "tableAuthor.id",
    "tableAuthor.displayName",
    "tableAuthor.emailUnsubscribed",
    "tableAuthor.privacyControl",
    "tableAuthor.profileLastModified",
    "tableAuthor.roleName",
    "tableAuthor.screenName",
    "tableAuthor.rights"
  FROM "datasets"
)
GROUP BY "id"
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That is, I combine stack the owner columns and tableAuthor columns into one
table and then remove duplicates based on the &lt;code&gt;id&lt;/code&gt; field. If I didn’t remove
duplicates, I would have multiple rows per user.
(The query would actually be a bit more complicated than this because it would
have to count how many times a user owns a view and has authored a table.)&lt;/p&gt;

&lt;p&gt;Don’t worry if that didn’t make sense to you; the point is that we can use
datasets in different ways than they seem to be intended.&lt;/p&gt;

&lt;h4 id="what-i-found-1"&gt;What I found&lt;/h4&gt;
&lt;p&gt;My main conclusion is that people don’t use these charting tools all that much.&lt;/p&gt;

&lt;h5 id="big-users"&gt;Big users&lt;/h5&gt;
&lt;p&gt;Most of the users in the dataset (7790 to be exact) had made exactly one view.
Actually, there are probably even more with no views, but I don’t have the
data on them.&lt;/p&gt;

&lt;p&gt;&lt;img src="/!/socrata-users/figure/n.views.png" alt="" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;Similarly, the users who have owned and authored the most tables tend to work
for either Socrata or clients of Socrata.&lt;/p&gt;

&lt;p&gt;Neither of these discoveries should be a surprise; you can call it the
&lt;a href="http://en.wikipedia.org/wiki/Pareto_principle"&gt;Pareto principle&lt;/a&gt; if you want.&lt;/p&gt;

&lt;h5 id="consumerizing"&gt;Consumerizing&lt;/h5&gt;
&lt;p&gt;I wanted to see examples of this consumerized data analysis that was being
advertised, so I tried to find users who were not employed by Socrata or its
clients. I eventually &lt;a href="/!/socrata-users/#also-no-tables"&gt;found some&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As I said above, my main conclusion is that people don’t use these charting
tools all that much. More specifically,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The people who create the most charts are people who maintain data portals&lt;/li&gt;
  &lt;li&gt;Aside from those who maintain data portals, the people who create the most
 charts are usually making different charts of the same data.&lt;/li&gt;
  &lt;li&gt;I found a small number of people who seem to be using the charts for broader
 things. I haven’t really talked to any of them, but the little I do know of
 their stories is interesting.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id="finding-data-is-hard"&gt;Finding data is hard&lt;/h3&gt;
&lt;p&gt;I realized that I using my spreadsheet rather than Socrata’s search tool to look
up data. This was funny, and it pointed out to me an interesting phenomenon about
the sharing of government data. As I said earlier, nobody has any idea of what is
going on with open data. At a most basic level, even though we have these catalogs
of datasets, people can’t really figure out what is in the catalog.&lt;/p&gt;

&lt;p&gt;I have &lt;a href="/!/openprism"&gt;identified&lt;/a&gt;
two broad categories of issues related to this.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Naive search method&lt;/li&gt;
  &lt;li&gt;Siloed open data portals&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Let’s talk about the second one.&lt;/p&gt;

&lt;p&gt;&lt;img src="unsilo.jpg" alt="Diagram about siloed open data portals and some layer to un-silo them" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;I made a &lt;a href="http://openprism.thomaslevine.com"&gt;rather simple site&lt;/a&gt; to demonstrate this idea.&lt;/p&gt;

&lt;h3 id="file-formats"&gt;File formats&lt;/h3&gt;

&lt;p&gt;We’re supposed to use certain file formats.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;“Mandate open formats for government data” (&lt;a href="http://sunlightfoundation.com/opendataguidelines/#open-formats"&gt;Sunlight Foundation&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;“structured data” (&lt;a href="http://inkdroid.org/journal/2010/06/04/the-5-stars-of-open-linked-data/"&gt;5 stars&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;“Data Must Be Machine processable” (&lt;a href="http://www.opengovdata.org/home/8principles"&gt;Open Government Working Group&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id="getting-the-data-2"&gt;Getting the data&lt;/h4&gt;
&lt;p&gt;This time, I used the &lt;code&gt;data.json&lt;/code&gt; endpoint, which is supposed to return
a &lt;a href="http://project-open-data.github.io/schema/"&gt;DCAT&lt;/a&gt; listing of all of
the datasets. It turns out that this endpoint
&lt;a href="/!/socrata-formats/#cutoff-at-1000"&gt;isn’t implemented properly&lt;/a&gt;,
but we’ll make do&lt;/p&gt;

&lt;h4 id="what-i-found-2"&gt;What I found&lt;/h4&gt;
&lt;p&gt;What are the file formats?&lt;/p&gt;

&lt;p&gt;&lt;img src="/!/socrata-formats/figure/all-formats.png" alt="Bar plot of file formats by portal" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;It turns out that file formats tell you quite a bit about the type of data too.
Take a look at &lt;a href="/!/missouri-data-licensing/"&gt;Missouri&lt;/a&gt;&lt;/p&gt;

&lt;h3 id="licensing"&gt;Licensing&lt;/h3&gt;
&lt;p&gt;Other data catalog software &lt;a href="https://github.com/tlevine/open-data-download"&gt;works differently&lt;/a&gt;
than Socrata, but the process it isn’t any more fancy. I downloaded data from catalogs running
these software.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Socrata&lt;/li&gt;
  &lt;li&gt;CKAN&lt;/li&gt;
  &lt;li&gt;OpenDataSoft&lt;/li&gt;
  &lt;li&gt;Junar&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And then I &lt;a href="http://thomaslevine.com/!/open-data-licensing/"&gt;looked at&lt;/a&gt;
the licenses that different datasets have.&lt;/p&gt;

&lt;p&gt;&lt;img src="/!/open-data-licensing/p2.png" alt="Licenses across all portals" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;Most data catalogs either have a license on everything or a license on nothing.)&lt;/p&gt;

&lt;p&gt;&lt;img src="/!/open-data-licensing/p1.png" alt="Bar graph of proportion of datasets" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;As I said before, &lt;a href="/!/missouri-data-licensing/"&gt;Missouri&lt;/a&gt; is interesting.
Also, they get this licensing right.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;a href="http://opendatacommons.org/faq/"&gt;Licensing is important because it reduces uncertainty.&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id="updating"&gt;Updating&lt;/h3&gt;
&lt;p&gt;Open government data are supposed to be kept up-to-date.
&lt;a href="http://thomaslevine.com/!/data-updatedness/#even-simpler"&gt;Pretty much nobody&lt;/a&gt; does this.&lt;/p&gt;

&lt;h4 id="getting-the-data-3"&gt;Getting the data&lt;/h4&gt;
&lt;p&gt;Recall that there were some date fields in those Socrata data.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;createdAt&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;publicationDate&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;rowsUpdatedAt&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;viewLastModified&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Once I figured out what these meant and dealt with &lt;a href="/!/data-updatedness#removing-duplicates"&gt;duplicates&lt;/a&gt;,
I could check whether datasets were being updated.&lt;/p&gt;

&lt;h4 id="what-i-found-3"&gt;What I found&lt;/h4&gt;
&lt;p&gt;First, hardly any datasets ever get updated.&lt;/p&gt;

&lt;p&gt;&lt;img src="/!/data-updatedness/figure/any_update.png" alt="Hardly any datasets get updated" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;Second, the ones that have been updated were mostly updated two years ago.
There might have been some bulk Socrata migration at the beginning of September 2011.&lt;/p&gt;

&lt;p&gt;&lt;img src="/!/data-updatedness/figure/publish_v_update.png" alt="Bulk migration?" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;Here are the datasets that got published before 2013 and got updated during 2013.&lt;/p&gt;

&lt;p&gt;&lt;img src="/!/data-updatedness/figure/publish_v_update_2013.png" alt="Old data still kept up-to-date" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;It’s only 13 datasets.&lt;/p&gt;

&lt;p&gt;&lt;img src="/!/data-updatedness/figure/updates_2013_url.png" alt="Those 13 datasets, by portal" class="wide" /&gt;&lt;/p&gt;

&lt;h2 id="future-things"&gt;Future things&lt;/h2&gt;
&lt;p&gt;The general thing I’m doing here is just studying data about open data.
People haven’t done much of this, so it’s turning up some interesting thing.&lt;/p&gt;

&lt;p&gt;I’ve started seeing four perspectives I could take in future study,
and the general idea for all of these is to automate existing manual processes.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Check how well open data guidelines are being followed.&lt;/li&gt;
  &lt;li&gt;Help people find data of interest to them; come up with something better than our current search bars.&lt;/li&gt;
  &lt;li&gt;Fill in blank metadata fields.&lt;/li&gt;
  &lt;li&gt;Figure out what makes for good data sharing; what are the impacts of organizational structures,
 hackathons, data catalog software, and open data policies on things that we care about?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;They’re really all the same thing, actually, but
I’m focusing on the first of these for the immediate future.&lt;/p&gt;

&lt;h2 id="exercises"&gt;Exercises&lt;/h2&gt;
&lt;p&gt;Attendees of this &lt;a href="http://www.meetup.com/NYC-Open-Data/events/147380312/"&gt;NYC Open Data meetup&lt;/a&gt;
typically want to learn exactly how to do things, rather than just getting a general
idea of some new idea. (At least, this is the impression I get.) It’s sort of “open data”
from a different angle; if everyone knows how to do things with data, then even messy data
would be quite open in a sense. But I digress.&lt;/p&gt;

&lt;p&gt;Let’s learn how to plan a crazy project like this. I’ve prepared two exercises.&lt;/p&gt;

&lt;h3 id="outlining-a-program"&gt;Outlining a program&lt;/h3&gt;
&lt;p&gt;Choose an open data catalog from this list.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href="http://data.dc.gov"&gt;Washington, District of Columbia&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="http://www.civicapps.org/datasets"&gt;Greater Portland, Oregon&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="http://www.utah.gov/open"&gt;Utah&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="http://nhopengov.org"&gt;New Hampshire&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="http://portal.louisvilleky.gov/service/data"&gt;Louisville, Kentucky&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="http://www.opendataphilly.org"&gt;Philidelphia, Pennsylvania&lt;/a&gt; (It runs &lt;a href="https://github.com/azavea/Open-Data-Catalog/"&gt;this software&lt;/a&gt;.)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;First, diagram how a person could manually download all of the datasets.
You want to get the most raw form available, not the sort of aggregates
that you might see in a plot.&lt;/p&gt;

&lt;p&gt;After you’ve done that, change the labels in the diagram so that it describes
a computer program that downloads the datasets.&lt;/p&gt;

&lt;p&gt;If you’re lucky, you’ll find API documentation, but you don’t need it;
figure out what the API is, and write the documentation yourself.&lt;/p&gt;

&lt;p&gt;For the one-minute presentation, walk through your outline of your program.
You can draw a diagram, write out steps in words, click through the website,
or just explain it without any visuals.&lt;/p&gt;

&lt;h3 id="using-simple-variables-to-represent-grand-concepts"&gt;Using simple variables to represent grand concepts&lt;/h3&gt;
&lt;p&gt;Select a document from this list, then select a single guideline within
the document. Brainstorm ways that you could test how well the guideline
is being followed. Try to come up with approaches that don’t involve
much manual work.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Open Knowledge Foundation &lt;a href="http://census.okfn.org/"&gt;Open Data Census&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Tim Berners-Lee &lt;a href="http://inkdroid.org/journal/2010/06/04/the-5-stars-of-open-linked-data/"&gt;Five Stars&lt;/a&gt; of open linked data.
  &lt;!-- http://opendata.stackexchange.com/a/529 --&gt;&lt;/li&gt;
  &lt;li&gt;Open Government Working Group &lt;a href="http://www.opengovdata.org/home/8principles"&gt;8 Principles of Open Government Data&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Sunlight Foundation &lt;a href="http://sunlightfoundation.com/opendataguidelines/"&gt;Open Data Policy Guidelines&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Open Data Institute &lt;a href="https://certificates.theodi.org/"&gt;Certificates&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For the one-minute presentation, show us the open data guidelines that you
chose and explain the approaches you came up with.&lt;/p&gt;

&lt;p&gt;You’ll probably have time to look at more than one guideline, but you probably
won’t have time to talk about more than two. If this is the case, choose
one or two that you though were most interesting.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <id>tag:www.thomaslevine.com,2013-11-27:/!/open-data-500-global-open-data-initiative-survey/index.html</id>
    <title type="html">Can we open our open data questionnaires?</title>
    <published>2013-11-27T00:00:00Z</published>
    <updated>2013-11-27T00:00:00Z</updated>
    <link rel="alternate" href="http://www.thomaslevine.com/!/open-data-500-global-open-data-initiative-survey/index.html"/>
    <content type="html">&lt;p&gt;On September 19, I got
&lt;a href="http://us6.campaign-archive2.com/?u=1a990feb5c&amp;amp;id=46f97c829f&amp;amp;e=f59483af9a"&gt;this email&lt;/a&gt;
from the &lt;a href="http://thegovlab.org/"&gt;GovLab&lt;/a&gt;.
It requested that tell them about companies who are using open data
by filling out a questionnaire within two weeks (September 27).&lt;/p&gt;

&lt;p&gt;On November 24, I saw a tweet about a
&lt;a href="http://sunlightfoundation.com/blog/2013/11/20/the-global-open-data-initiative-needs-your-input/"&gt;strangely similar request&lt;/a&gt;
from the Sunlight Foundation,
with the questionnaire due November 29.&lt;/p&gt;

&lt;h2 id="sharing-data-to-save-time"&gt;Sharing data to save time&lt;/h2&gt;
&lt;p&gt;These questionnaires are quite similar; could we (GovLab, Sunlight,
and people filling out questionnaires) maybe save some work if GovLab
and Sunlight shared the questionnaire data?&lt;/p&gt;

&lt;p&gt;The questionnaires are not identical, so the questionnaire of one of
these groups might not satisfy the needs of the other, but I think it
would at least help.&lt;/p&gt;

&lt;h2 id="open-our-data-about-open-data"&gt;Open our data about open data&lt;/h2&gt;
&lt;p&gt;It’s quite possible that Sunlight has seen GovLab’s data. I hope they
have, as that probably would have saved some time. But even if they did,
I find it hilarious that &lt;strong&gt;I&lt;/strong&gt; haven’t seen the results of at least the
GovLab questionnaire.&lt;/p&gt;

&lt;p&gt;Lots of people say that much of open data is about
&lt;a href="http://sunlightfoundation.com/opendataguidelines/#open-by-default"&gt;setting the default to open&lt;/a&gt;,
so you would think that raw results of these questionnaires would be published.
I guess Sunlight might want to wait until it closes the questionnaire before it
publishes anything, but GovLab could totally have released its data by now.&lt;/p&gt;

&lt;h2 id="sharing-data-to-get-people-to-contribute-data"&gt;Sharing data to get people to contribute data?&lt;/h2&gt;
&lt;p&gt;When I saw each of these questionnaires, I considered sending it to
people, but nine days didn’t seem like enough time for it to be worth
me telling people about them.&lt;/p&gt;

&lt;p&gt;Actually, that’s not really true; that’s what I say to myself, but
it’s really an excuse; the real reason I didn’t send them around is
that I’m generally quite skeptical that anyone is going to look
closely at the questionnaire. Assembling the questionnaire and
sending it out is a lot of work already; are they actually going
to have any energy left once they get the results back?&lt;/p&gt;

&lt;p&gt;I wonder: Would opening the results of questionnaires would get me
to fill out the questionnaires?&lt;/p&gt;
</content>
  </entry>
  <entry>
    <id>tag:www.thomaslevine.com,2013-11-22:/!/data-updatedness/index.html</id>
    <title type="html">Updating of data catalogs</title>
    <published>2013-11-22T00:00:00Z</published>
    <updated>2013-11-22T00:00:00Z</updated>
    <link rel="alternate" href="http://www.thomaslevine.com/!/data-updatedness/index.html"/>
    <content type="html">&lt;p&gt;&lt;a href="http://project-open-data.github.io/principles/"&gt;Project Open Data&lt;/a&gt;,
the &lt;a href="http://sunlightfoundation.com/opendataguidelines/#real-time-updates"&gt;Sunlight Foundation&lt;/a&gt;,
a bunch of
&lt;a href="http://le.utah.gov/~2013/bills/sbillenr/SB0283.pdf"&gt;open&lt;/a&gt;
&lt;a href="http://www.scribd.com/fullscreen/26442622?access_key=key-20rfsh26eu0ob66xlbmu"&gt;data&lt;/a&gt;
&lt;a href="http://www.governor.ny.gov/executiveorder/95"&gt;policies&lt;/a&gt;,
and probably everyone else says that it’s important that open data
be kept up-to-date. So I was wondering how up-to-date they actually are.&lt;/p&gt;

&lt;p&gt;I collected some data about the updatedness of open data.
Based on these data about open data,
the open data don’t seem very up-to-date.&lt;/p&gt;

&lt;h2 id="the-data-about-data"&gt;The data about data&lt;/h2&gt;
&lt;p&gt;I &lt;a href="/!/socrata-summary/"&gt;downloaded&lt;/a&gt;
all the metadata about all of the public datasets that are hosted on
58
data portals run by &lt;a href="http://socrata.com"&gt;Socrata&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id="removing-duplicates"&gt;Removing duplicates&lt;/h3&gt;
&lt;p&gt;I had to do a lot to remove duplicate datasets.
I did these two things.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Remove federated datasets&lt;/li&gt;
  &lt;li&gt;Deal with derived views&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Dealing with federation was annoying, but I had already
done that; you can read about that
&lt;a href="/!/socrata-deduplicate/#dealing-with-federation"&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Dealing with derived views is more complicated.
Socrata has this feature where you can create multiple
&lt;a href="/!/socrata-genealogies#term-view"&gt;views&lt;/a&gt; from a single
&lt;a href="/!/socrata-genealogies#term-table"&gt;table&lt;/a&gt;.
It is hard to separate the official views, created by someone in the
government, from the unofficial, derived views. I did not try to determine
which view/dataset was the original, official one, but I did separate
them based on their &lt;code&gt;tableId&lt;/code&gt;, which links all of the different views
that use the same data. Because I didn’t figure out which view is the
original one for each table, I can’t tell you things like the name of
the original dataset. On the other hand, I can tell you a lot of things.
Most importantly, this manner of removing duplicates doesn’t hinder
my approach for determining updatedness.&lt;/p&gt;

&lt;h3 id="dates"&gt;Dates&lt;/h3&gt;
&lt;p&gt;The Socrata API provides four date fields for each view.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;createdAt&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;publicationDate&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;rowsUpdatedAt&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;viewLastModified&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;createdAt&lt;/code&gt; is when someone clicked the button to include the dataset;
it’s always the oldest one. It is possible to create a dataset but keep
it private, for internal review. If you spend a couple days reviewing
the dataset privately before you post it publically, the &lt;code&gt;publicationDate&lt;/code&gt;
will reflect this latter date, while &lt;code&gt;createdAt&lt;/code&gt; will still be the time
when you added the private dataset. Also, &lt;code&gt;createdAt&lt;/code&gt; is different for
different views of the same data, but &lt;code&gt;publicationDate&lt;/code&gt; is not.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;table(subset(socrata, tableId == 926641)$publicationDate)
2013-06-18 
        46


table(subset(socrata, tableId == 926641)$createdAt)
2011-09-26 2011-10-25 2011-12-22 2012-01-03 2012-01-04 2012-01-30 
         2         17          1          1          1          6 
2012-01-31 2012-02-06 2013-04-08 2013-05-30 
         7          9          1          1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;rowsUpdatedAt&lt;/code&gt; and &lt;code&gt;viewLastModified&lt;/code&gt; presumably have something to do
with the time the dataset was updated. I don’t really know what those
mean, but I think &lt;code&gt;rowsUpdatedAt&lt;/code&gt; is the one I want. Recall from above
that views with the same &lt;code&gt;tableId&lt;/code&gt; share the share the underlying data.
If the data get updated, it should be reflected in all of the datasets.
This is the case for &lt;code&gt;rowsUpdatedAt&lt;/code&gt;; for all
46
datasets with an &lt;code&gt;tableId&lt;/code&gt; of &lt;code&gt;926641&lt;/code&gt;, the &lt;code&gt;rowsUpdatedAt&lt;/code&gt; field
is the same.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; table(subset(socrata.deduplicated.orig, tableId == 926641)$rowsUpdatedAt)
1371584568 
        46
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;On the other hand, the &lt;code&gt;viewLastModified&lt;/code&gt; field is quite different.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; table(subset(socrata.deduplicated.orig, tableId == 926641)$viewLastModified)
1320681652 1320681806 1320683489 1320683529 1320683572 1320683615 
         1          1          1          1          1          1 
1320683652 1320683694 1320683740 1320683775 1320683818 1320684414 
         1          1          1          1          1          1 
1320684459 1320684504 1320684542 1320684575 1320684625 1325697249 
         1          1          1          1          1          1 
1327951380 1327951474 1327951571 1327951655 1327951743 1327951864 
         1          1          1          1          1          1 
1328026858 1328027111 1328027387 1328027479 1328028650 1328031765 
         1          1          1          1          1          1 
1328037223 1328544614 1328544754 1328544905 1328545059 1328545187 
         1          1          1          1          1          1 
1328545294 1328545431 1328545524 1328545643 1328734273 1330367612 
         1          1          1          1          1          1 
1365458282 1365463347 1369872686 1371584597 
         1          1          1          1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thus, I’m using &lt;code&gt;rowsUpdatedAt&lt;/code&gt; for calculating the updatedness measure.&lt;/p&gt;

&lt;h2 id="what-is-an-update"&gt;What is an “update”&lt;/h2&gt;
&lt;p&gt;I’m going to use the term &lt;em id="term-update"&gt;update&lt;/em&gt; to mean a reasonably particular
thing that excludes some other things that you might consider an update.&lt;/p&gt;

&lt;p&gt;The Socrata data portal software has 
&lt;a href="http://dev.socrata.com/publishers/getting-started"&gt;this publisher API&lt;/a&gt;
that lets the publisher (usually a government) upload data as a
&lt;a href="/!/socrata-genealogies/#term-dataset"&gt;dataset&lt;/a&gt;-type &lt;a href="/!/socrata-genealogies#term-view"&gt;view&lt;/a&gt;,
with its own unique URL based on its
&lt;a href="http://dev.socrata.com/docs/endpoints"&gt;4x4 identifier&lt;/a&gt;.
I presume that there’s some web interface for this too.&lt;/p&gt;

&lt;p&gt;An update only occurs when you use the API (or the presumed web interface)
in a particular manner. I’ll tell you about three ways you might use it,
and I only count the third way as an update.&lt;/p&gt;

&lt;h3 id="upload-a-new-version-and-delete-the-old-version"&gt;Upload a new version and delete the old version&lt;/h3&gt;
&lt;p&gt;Let’s say you have a dataset of
&lt;a href="https://data.oregon.gov/dataset/Contracts-Lottery-Fiscal-Year-2011/dwi6-thje"&gt;lottery contracts for 2011&lt;/a&gt;
and you notice a spelling error in one of the fields. You fix it on your own computer,
but how do you get it to the main site? You could upload the new one and delete the old
one, but then the URL will change.&lt;/p&gt;

&lt;p&gt;When someone uploads a new version of the dataset to Socrata
but it has a different URL, I do not count it as an update.&lt;/p&gt;

&lt;h3 id="upload-new-data-as-a-separate-dataset"&gt;Upload new data as a separate dataset&lt;/h3&gt;
&lt;p&gt;What if you want to add the 2012 data but keep the 2011 data the same?
You could upload the 2012 data as a
&lt;a href="https://data.oregon.gov/Revenue-Expense/Contracts-Lottery-Fiscal-Year-2012/i3ri-n6hq"&gt;separate dataset&lt;/a&gt;.
The old dataset would still have the same URL,
but but the new one would be hard to find; the open data catalog doesn’t make a link between these two datasets.&lt;/p&gt;

&lt;p&gt;Within the present article, I do not consider this to be an update either.&lt;/p&gt;

&lt;h3 id="using-the-append-and-replace-api-methods"&gt;Using the append and replace API methods&lt;/h3&gt;
&lt;p&gt;The publisher API lets you
&lt;a href="http://dev.socrata.com/publishers/importing#append_replace"&gt;append to or replace&lt;/a&gt;
the data in an existing dataset. That is, you can upload new data for an
existing dataset while keeping its 4x4 identifier and all of its metadata.&lt;/p&gt;

&lt;p&gt;This approach is much better than the other two because it
makes it much easier to find the new data; we can get the
new data while still referencing the same URL. This is the
only of the three approaches that I count as an update.&lt;/p&gt;

&lt;h2 id="data-summary"&gt;Data summary&lt;/h2&gt;
&lt;p&gt;We have data from mid-2011 to mid-2013. In order to fit this all on
one plot, let’s look first at which datasets were updated; we’re ignoring
the dates when they were updated. Each horizontal band is a data portal,
each dot is a data &lt;a href="/!/socrata-genealogies#term-table"&gt;table&lt;/a&gt;
inside of a data portal,
and the dots are colored based on whether the data were ever updated.&lt;/p&gt;

&lt;p&gt;&lt;img src="figure/summary.png" alt="" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;This plot shows us that older datasets are more likely to have been
updated. It also subtly points out that different data portals have
different amounts of datasets and that they have grown at different
rates over time. Let’s plot that more clearly.&lt;/p&gt;

&lt;p&gt;&lt;img src="figure/dataset_counts_over_time.png" alt="" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;Data portals grow over time, often in sudden bursts. Two notes about
that plot:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;It assumes that no data have ever been deleted from a portal;
 it’s all based on the publication date.&lt;/li&gt;
  &lt;li&gt;It is counting the number of &lt;a href="/!/socrata-genealogies#term-table"&gt;tables&lt;/a&gt;
 rather than the number of &lt;a href="/!/socrata-genealogies#term-view"&gt;views&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Let’s also relate the update dates to the publication dates.&lt;/p&gt;

&lt;p&gt;&lt;img src="figure/up_to_date.png" alt="" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;It looks like most data were “updated” only within an hour of their
initial publications. Colloquially, we’d say that they haven’t been
updated ever. But there are some spikes here and there.&lt;/p&gt;

&lt;p&gt;Also, it appears that only certain kinds of data get updated.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; table(socrata.deduplicated$viewType, is.na(socrata.deduplicated$rowsUpdatedAt))
        FALSE TRUE
blobby      0 1752
geo         0  400
href        0  571
tabular  5552  151
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tabular data (the stuff you’re used to) get updated, but links to
external data (&lt;code&gt;href&lt;/code&gt;), geospatial data (&lt;code&gt;geo&lt;/code&gt;) and ordinary files (&lt;code&gt;blobby&lt;/code&gt;)
don’t seem to get updates. I don’t know whether this is a feature of
Socrata or just a strong tendency for those sorts of data.&lt;/p&gt;

&lt;h2 id="motivating-an-updatedness-measure"&gt;Motivating an updatedness measure&lt;/h2&gt;
&lt;p&gt;The update date can tell us how up-to-date a particular dataset is,
but I want to get a bigger picture. I want to quantitatively compare
datasets and groups of datasets. I tried to come up with a simple
statistic that would let us do that.&lt;/p&gt;

&lt;p&gt;One measure that I came up with is &lt;strong&gt;proportion of datasets older than
a year that have been updated within the year&lt;/strong&gt;. (The timespan of a
year is pretty arbitrary, though.) Here’s how I arrived at that measure.&lt;/p&gt;

&lt;h3 id="general-formulation"&gt;General formulation&lt;/h3&gt;
&lt;p&gt;In producing a measure of updateness, we are trying to come up with a
simple number that matches our intuition about whether something is
up-to-date. Let’s think a bit about what would count as up-to-date.&lt;/p&gt;

&lt;p&gt;A dataset first gets published at some time. For a little while, the
data will be up-to-date even if no updates are sent. For example, if
data about wifi usage per month are uploaded on the first of the month,
we’ll consider them to be up-to-date two weeks later.&lt;/p&gt;

&lt;p&gt;After some point, we must have received an update in order for the
data to be up-to-date. In the case of monthly wifi usage, we need an
update on the first of the second month; if we haven’t received one
by then, we’ll consider the data out-of-date. Thus, I arrive at the
general concept of comparing the date of initial publication to the
date at which the dataset was last updated.&lt;/p&gt;

&lt;h3 id="differences-by-dataset"&gt;Differences by dataset&lt;/h3&gt;
&lt;p&gt;If a dataset of scores on standardized math tests taken in schools
was uploaded two weeks ago and hasn’t been touched since, I’d say it’s
up-to-date. A particular standardized math tests might be administered
yearly, so I don’t expect the data to have changed in a couple of weeks.&lt;/p&gt;

&lt;p&gt;On the other hand, if a dataset of 311 calls hasn’t been updated in
two weeks, we could say that it is out-of-date because 311 calls are
always coming in.&lt;/p&gt;

&lt;p&gt;A dataset about something that happens often needs to be refreshed
more often for us to consider it up-to-date. A very cool measure of
updatedness would account for out how often new data come in and
check whether those new data show up in the dataset. But that would
be a lot of work, so let’s start with something simpler.&lt;/p&gt;

&lt;h3 id="keep-it-simple"&gt;Keep it simple&lt;/h3&gt;
&lt;p&gt;Rather than worring about the exact time period, let’s address a simpler
issue. I’ve heard complaints that some data get uploaded data once and
never updated, ever. Rather than worring about the exact time since update,
let’s just check whether the dataset has been updated ever.&lt;/p&gt;

&lt;p&gt;I’m going to use the format of &lt;em&gt;datasets older than X that have been updated since&lt;/em&gt;
and just choose an appropriate time X. The time probably has to be kind of
long, so that we don’t count datasets as out-of-date when they just have
long update cycles (like yearly data). On the other hand, the time can’t be
too long, because then we’ll wind up saying that everything is out-of-date.&lt;/p&gt;

&lt;h3 id="choosing-a-less-arbitrary-cutoff"&gt;Choosing a less arbitrary cutoff&lt;/h3&gt;
&lt;p&gt;We can check whether different values will give us the same value on our
updatedness measure. To say it fancy-like, we can check whether our updatedness
measure is robust to the cutoff.&lt;/p&gt;

&lt;p&gt;A simple way of checking this is to plot the value of our updatedness
measure for different cutoff values. If we find a region where the value
of the updatedness measure doesn’t change very much when we change the
cutoff, we could say that cutoffs in that region are safe to use.&lt;/p&gt;

&lt;p&gt;I also had to decide whether to use &lt;code&gt;rowsUpdatedAt&lt;/code&gt; or &lt;code&gt;viewLastModified&lt;/code&gt;
as the date of update. I don’t really know what either of these means, so
let’s just try both.&lt;/p&gt;

&lt;p&gt;&lt;img src="figure/robustness.png" alt="" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;The main thing I notice is that the value of this statistic
(again, proportion of datasets older than some date that have been updated since)
is pretty close to zero for all of the portals at any time.
This is very bad; it means that we pretty much never update
data that we put on data portals.&lt;/p&gt;

&lt;p&gt;The next thing I notice is that the value varies widely when there aren’t
very many datasets on the portal. Visually, this shows itself as skinny
lines (few datasets) jumping up and down very suddenly, like for
&lt;code&gt;data.cms.gov&lt;/code&gt;. When there are very few datasets on a portal
(like 12), an addition of a dataset or an update to a dataset will have
a large impact on the value of our updatedness measure (the height of the line).
This is not interesting, and the plot below should explain why.&lt;/p&gt;

&lt;p&gt;&lt;img src="figure/small_samples.png" alt="" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;In these two plots, we’re only looking at &lt;code&gt;data.cms.gov&lt;/code&gt;. The first
plot is the same as the corresponding plot in the previous image,
only larger. The second shows the dates at which each
&lt;a href="/!/socrata-genealogies#term-table"&gt;data table&lt;/a&gt; was
published and most recently updated. Red dots are publication dates,
and blue dots are update dates. If there was no update or the most
recent update was within a day of the publication date, no blue dot
is shown.&lt;/p&gt;

&lt;p&gt;The bumps in the top plot only occur when the portal contains very
few datasets; the figure stabilizes at the end of March 2013, when
a bunch of datasets get published. (In case you’re curious, these
datasets are all derived from the
&lt;a href="http://www.resdac.org/cms-data/files/bsf"&gt;Beneficiary Summary File&lt;/a&gt;,
according to their respective descriptions in the portal.)&lt;/p&gt;

&lt;h2 id="even-simpler"&gt;Even simpler&lt;/h2&gt;
&lt;p&gt;I’d assumed that people actually update their data, but it looks
like people don’t.
I think we need something even simpler to drive this conclusion home.&lt;/p&gt;

&lt;p&gt;Let’s look at the number of datasets that have ever been updated, by
portal. We’ll count a dataset as having been updated if it has a value
in &lt;code&gt;rowsUpdatedAt&lt;/code&gt; that is more than one day greater than the value
in &lt;code&gt;publicationDate&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src="figure/any_update.png" alt="" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;Keep my &lt;a href="#term-update"&gt;definition of “update”&lt;/a&gt; in mind;
in the above count, I’m including neither situations where old datasets
were deleted and replaced with new ones nor situations where new records
were uploaded as a separate dataset.&lt;/p&gt;

&lt;h3 id="which-datasets-have-been-updated"&gt;Which datasets have been updated?&lt;/h3&gt;
&lt;p&gt;Only 655 of the
8426 datasets have ever been updated.&lt;/p&gt;

&lt;p&gt;These 655 datasets are
contained within only 29
portals, out of the 58
that I looked at. Here are the updates over time by portal.&lt;/p&gt;

&lt;p&gt;&lt;img src="figure/updates_over_time_by_portal.png" alt="" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;A few datasets are still updated today (indicated by the tiny bumps
towards the right of the graphs), but most were only updated two years
ago.&lt;/p&gt;

&lt;p&gt;You might have guessed already, but these recent updates tend to be for
datasets that were recently updated, rather than old datasets that have
been maintained for a while. You can see that in this plot of publication
date versus update date. The update date is along the x-axis, and the
publication date is along the y-axis. The diagonal line is where the
publication date and update date are the same, and it is impossible for
a dataset to appear above the diagonal line. Datasets towards the bottom-left
of the plot were published and updated a long time ago. Datasets towards
the top-right were published and updated recently. Datasets towards the
bottom-right were published a long time ago and updated recently.&lt;/p&gt;

&lt;p&gt;&lt;img src="figure/publish_v_update.png" alt="" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;I didn’t really look into why there were so many updates a long
time ago and so few in between, but I suspect it has something to do
with how all of these portals are managed by Socrata; there might have
been some change in software or in technical support practices that
would have impacted all of the portals.&lt;/p&gt;

&lt;h3 id="which-old-datasets-are-still-kept-up-to-date"&gt;Which old datasets are still kept up-to-date?&lt;/h3&gt;
&lt;p&gt;Recall that the points at the bottom-right of the previous plot
correspond to old datasets that have been updated recently. Let’s
look more closely at these.&lt;/p&gt;

&lt;p&gt;I selected the datasets that were uploaded before 2013 and have
been updated during 2013; they are represented the ones contained
by the rectangle in the plot below.&lt;/p&gt;

&lt;p&gt;&lt;img src="figure/publish_v_update_2013.png" alt="" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;There are, in fact, only 13 such datasets,
and they’re in 8 portals.&lt;/p&gt;

&lt;p&gt;&lt;img src="figure/updates_2013_hist.png" alt="" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;Here they are by url.&lt;/p&gt;

&lt;p&gt;&lt;img src="figure/updates_2013_url.png" alt="" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;I was wondering whether anything was special about these. Maybe
they have a lot of records or get downloaded a lot? For both of
these statistics, I’m using the total across all &lt;a href="/!/socrata-genealogies#term-view"&gt;views&lt;/a&gt; in the
&lt;a href="/!/socrata-genealogies#term-table"&gt;table/family&lt;/a&gt;, not just the value for that particular view.&lt;/p&gt;

&lt;p&gt;&lt;img src="figure/updates_2013_specialness.png" alt="" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;I didn’t find anything special about these, but I didn’t look very hard.
In case you want to give it a shot, here they are.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;nmfs.socrata.com&lt;/code&gt;: &lt;a href="https://nmfs.socrata.com/-/-/zg3h-r2t8"&gt;Pre-Approval for Public Comments: NOAA Aquaculture Listening Sessions (2010)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;nmfs.socrata.com&lt;/code&gt;: &lt;a href="https://nmfs.socrata.com/-/-/u5id-8nqp"&gt;2011 Aquaculture Public Comments Form&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;nmfs.socrata.com&lt;/code&gt;: &lt;a href="https://nmfs.socrata.com/-/-/49sy-iumi"&gt;Map of U.S. Department of Commerce Comments&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;nmfs.socrata.com&lt;/code&gt;: &lt;a href="https://nmfs.socrata.com/-/-/ncry-xzwf"&gt;Photo Directory Example&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;data.cityofchicago.org&lt;/code&gt;: &lt;a href="https://data.cityofchicago.org/-/-/zfg3-p7xv"&gt;Performance Metrics - Innovation &amp;amp; Technology - Site Availability&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;data.seattle.gov&lt;/code&gt;: &lt;a href="https://data.seattle.gov/-/-/gigx-p9h7"&gt;Heatmap - Assault&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;data.kingcounty.gov&lt;/code&gt;: &lt;a href="https://data.kingcounty.gov/-/-/cuea-cmxg"&gt;Damage report form for primary residences&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;data.ok.gov&lt;/code&gt;: &lt;a href="https://data.ok.gov/-/-/2d47-x2mx"&gt;Library Map&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;data.honolulu.gov&lt;/code&gt;: &lt;a href="https://data.honolulu.gov/-/-/a96q-gyhq"&gt;Crime Incidents&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;data.honolulu.gov&lt;/code&gt;: &lt;a href="https://data.honolulu.gov/-/-/ix32-iw26"&gt;Traffic Incidents&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;data.oregon.gov&lt;/code&gt;: &lt;a href="https://data.oregon.gov/-/-/6y98-3xjn"&gt;GPL Documents&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;data.honolulu.gov&lt;/code&gt;: &lt;a href="https://data.honolulu.gov/-/-/a3ah-kpkr"&gt;Data Catalog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;data.austintexas.gov&lt;/code&gt;: &lt;a href="https://data.austintexas.gov/-/-/nmp9-45v2"&gt;Austin Green Infrastructure Inventory&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note well: These links go to arbitrary &lt;a href="/!/socrata-genealogies#term-view"&gt;views&lt;/a&gt; on the particular
&lt;a href="/!/socrata-genealogies#term-table"&gt;table&lt;/a&gt; that was updated; you’ll have to follow a few links to get to
the original dataset.&lt;/p&gt;

&lt;h3 id="updated-datasets-in-context"&gt;Updated datasets in context&lt;/h3&gt;
&lt;p&gt;Maybe there actually is something different about the sort of dataset that
gets updated.&lt;/p&gt;

&lt;p&gt;It turns out that data that have been updated tend to get more downloads than
data that data that haven’t. (This again uses the family/table totals rather
that the values for the particular views.)&lt;/p&gt;

&lt;p&gt;&lt;img src="figure/update_download.png" alt="" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;To give you some more concrete numbers, the median download count was
88 among data tables that got updated and 15
among data tables that didn’t. A Wilcoxon rank sum test says that this
difference is significant (assuming that our 8426
datasets are a representative sample of some super-population, yadda yadda),
with a p-value far less than 0.001.&lt;/p&gt;

&lt;p&gt;There is a similar relationship with number of records.
It turns out that data that have been updated tend to contain more records than
data that data that haven’t.&lt;/p&gt;

&lt;p&gt;&lt;img src="figure/update_nrow.png" alt="" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;The median record count was
66 among data tables that got updated and 17
among data tables that didn’t. A Wilcoxon rank sum test says that this
difference too is significant.&lt;/p&gt;

&lt;h2 id="a-proposal-for-an-updatedness-measure"&gt;A proposal for an updatedness measure&lt;/h2&gt;
&lt;p&gt;I thought I was going to find out which datasets are more up-to-date,
when they get updated, and so on, but I really just found that the data
are completely out-of-date.&lt;/p&gt;

&lt;p&gt;Perhaps this says something about the need for more measures of the progress of
our open data efforts. We have a crude measure of the size of a data catalog
(the number of datasets it contains), so we can check that number and make sure
that it is increasing. But that’s pretty much the only one we have, and there
are surely other things that are worth measuring. Nobody is measuring updatedness,
so how can we expect anyone to know that the data are out-of-date?
I think that we’ll keep these portals more up-to-date if we come up with a
measure of their updateness that is easy to calculate and easy to understand.&lt;/p&gt;

&lt;p&gt;Having found hardly any datasets that get updated, it’s difficult for me to
say whether any measure I come up with will be all that helpful. But here’s
my best guess as to the measure we should try to arrive at.&lt;/p&gt;

&lt;h3 id="update-cycles"&gt;Update cycles&lt;/h3&gt;
&lt;p&gt;As I explained above, different datasets have different update cycles
(daily, weekly, quarterly, yearly, &amp;amp;c.), and we can know what this cycle
is for most datasets. In many cases, we don’t know exactly how often a
dataset will be updated, but we can be reasonably confident that it will
be within a certain range.&lt;/p&gt;

&lt;h3 id="what-we-do-in-treasuryio"&gt;What we do in treasury.io&lt;/h3&gt;
&lt;p&gt;We do something like this in &lt;a href="http://treasury.io"&gt;treasury.io&lt;/a&gt;.
The Financial Management Service (FMS) provides a daily treasury
statement. One statement is provided per work day, so we should
get updates approximately every day. In order to make sure that
our daily importer is working, we check every day that the
resulting dataset is up-to-date.&lt;/p&gt;

&lt;p&gt;We know from experience that
the file doesn’t always come out on time; that is, we might only recieve
today’s statement two days from now. On the other hand, we’ve
rarely (never?) seen it take longer than that. Our daily updatedness
test checks how far behind the data are, and it sends us an email
if the data are
&lt;a href="https://github.com/csvsoundsystem/federal-treasury-api/blob/master/tests/is_it_running.py#L47"&gt;more than three days behind&lt;/a&gt;;
if this happens,
we’ll suspect that it’s something wrong with our importer rather
than just a late upload from the FMS.&lt;/p&gt;

&lt;h3 id="possibilities"&gt;Possibilities&lt;/h3&gt;
&lt;p&gt;I brainstormed a bit with &lt;a href="https://twitter.com/criscristina"&gt;Cris Cristina&lt;/a&gt;
about how we could present this sort of information inside of a data catalog
website to help people maintain it.&lt;/p&gt;

&lt;p&gt;If we indicate how often each dataset needs to
be updated, the data portal software could make us aware of which datasets
need updating. Aside from comparing datasets to datasets, we could compare
other groupings, like departments.&lt;/p&gt;

&lt;p&gt;&lt;img src="mockups/dataset.jpg" alt="" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;Rather than just presenting a bunch of raw statistics as in the previous
sketch, we could aggregate these so that you can look at just one thing
to get a quick idea of whether a dataset or group of datasets is in good
shape. For example, we could have icons for the general health of a dataset:
Sad face for bad, smily face for good, and big smiley face for awesome.&lt;/p&gt;

&lt;p&gt;&lt;img src="mockups/datasets.jpg" alt="" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;We could present much more detail about the updates to a dataset.
Rather than simply saying the date at which it was last updated, we could
have a timeline of the updates, and we could let you look at old versions
of the dataset. (Actually, it’s super important to have access to old
versions, but I won’t get into that now.) Perhaps you could put multiple
datasets or groups of datasets on this timeline and compare them.&lt;/p&gt;

&lt;p&gt;&lt;img src="mockups/timeline.jpg" alt="A sketch of a timeline of dataset updates" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;Also, we can present specific action items. I don’t know exactly what
these would be, but they might include datasets that are out-of-date
and datasets that will soon be out-of-date. Imagine the following table
being presented in some administration page in a data catalog.&lt;/p&gt;

&lt;p&gt;&lt;img src="mockups/notifications.jpg" alt="" class="wide" /&gt;&lt;/p&gt;

&lt;h2 id="you-get-what-you-measure"&gt;You get what you measure&lt;/h2&gt;
&lt;p&gt;I thought this article was going to be about which datasets are more up-to-date,
when they get updated, and so on, but I really just found that the data
are completely out-of-date.&lt;/p&gt;

&lt;p&gt;The phrase “You get what you measure.” (or any of its variants) seems to
describe our present situation quite well.
We say a lot of things about how people should be doing open data,
and people count &lt;a href="http://census.okfn.org/"&gt;some of these things once-in-a-while&lt;/a&gt;.
Aside from efforts like the Open Data Census, we don’t really do anything.&lt;/p&gt;

&lt;h3 id="current-metrics"&gt;Current metrics&lt;/h3&gt;
&lt;p&gt;Well actually we sort of do at least count the number of datasets. There are
&lt;a href="http://technickle.nicklin.info/post/58169276432/accurately-counting-socrata-datasets"&gt;problems&lt;/a&gt;,
with this metric, but it does vaguely point us in the right direction.&lt;/p&gt;

&lt;h3 id="other-metrics"&gt;Other metrics&lt;/h3&gt;
&lt;p&gt;We measure the number of datasets, and we get more datasets. There are a bunch
of other things that we want. We don’t measure them, and we don’t get them.
As you saw above, we don’t measure updatedness, and we don’t get up-to-date data.
Similarly, we don’t measure &lt;a href="http://theunitedstates.io/licensing/"&gt;license-free-ness&lt;/a&gt;,
and we &lt;a href="/!/open-data-licensing/"&gt;don’t get license-free data&lt;/a&gt;. Also, we don’t
check whether different datasets could be combined into one, and we get a lot
of situations, at least in &lt;a href="http://appgen.me/audit/report"&gt;New York City&lt;/a&gt; and
&lt;a href="/!/missouri-data-licensing/"&gt;Missouri&lt;/a&gt;, several datasets could be combined
into one.&lt;/p&gt;

&lt;h3 id="data-driven-open-data"&gt;Data-driven open data&lt;/h3&gt;
&lt;p&gt;Considering that we’re doing open &lt;em&gt;data&lt;/em&gt;, it’s only reasonable that we make
data-driven decisions about our open data efforts. We obviously need data
about open data to make these data-driven decisions, so we had better start
measuring the all of these things that we care about.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <id>tag:www.thomaslevine.com,2013-11-18:/!/open-data-in-plain-english/index.html</id>
    <title type="html">Open Data in Plain English</title>
    <published>2013-11-18T00:00:00Z</published>
    <updated>2013-11-18T00:00:00Z</updated>
    <link rel="alternate" href="http://www.thomaslevine.com/!/open-data-in-plain-english/index.html"/>
    <content type="html">&lt;video poster="screenshot.png" src="open-data-in-plain-english.webm" controls="" width="100%"&gt;&lt;/video&gt;

&lt;p&gt;I found this video &lt;a href="http://www.youtube.com/watch?v=aHxv_2BMJfw"&gt;on YouTube&lt;/a&gt;
and then &lt;a href="https://github.com/tlevine/open-data-a-la-loupe"&gt;translated&lt;/a&gt;
the audio to English.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <id>tag:www.thomaslevine.com,2013-10-29:/!/missouri-data-licensing/index.html</id>
    <title type="html">License-free data in Missouri's data portal</title>
    <published>2013-10-29T00:00:00Z</published>
    <updated>2013-10-29T00:00:00Z</updated>
    <link rel="alternate" href="http://www.thomaslevine.com/!/missouri-data-licensing/index.html"/>
    <content type="html">&lt;p&gt;I recently &lt;a href="/!/open-data-licensing"&gt;found&lt;/a&gt; that most datasets on the
open data portals I’ve been looking at don’t have open licenses or
public domain dedications.&lt;/p&gt;

&lt;p&gt;But a lot of datasets do have licenses or public domain dedication, and
licensing this varies within portals. I wanted to see which datasets get
have some sort of licensing metadata and which ones
don’t.
Or this is what I thought I wanted;
I started looking at this on one data portal, and I
wound up going down a slightly different rabbit hole.&lt;/p&gt;

&lt;h2 id="the-missouri-data-portal"&gt;The Missouri Data Portal&lt;/h2&gt;
&lt;p&gt;I looked at the Missouri Data Portal,
&lt;a href="https://data.mo.gov"&gt;data.mo.gov&lt;/a&gt;. Here’s why I chose it.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;To make it easy to download the license information, I wanted to use
 &lt;a href="/!/socrata-formats/"&gt;Socrata’s data.json endpoint&lt;/a&gt;.
 This endpoint only returns up to 1000
 datasets, so I needed a portal with no more than 1000 datasets.
 Missouri’s portal has 897 datasets.&lt;/li&gt;
  &lt;li&gt;I think I’ve figured out how to
 &lt;a href="/!/socrata-deduplicate"&gt;deduplicate federated datasets&lt;/a&gt;,
 but I didn’t want to deal with that.
 The Missouri portal federates only the
 &lt;a href="https://data.kcmo.org"&gt;Kansas City data portal&lt;/a&gt;,
 so no federal data should be duplicated in this portal.&lt;/li&gt;
  &lt;li&gt;Missouri has &lt;a href="/!/socrata-formats"&gt;a lot of PDF files&lt;/a&gt;,
 so I thought there might be something in there that would be more
 original than mere facts and covered under copyright; this would
 make a license more important.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I wound up categorizing the datasets quite a bit more than I originally
intended, so now I know a lot about the sorts of data on the portal.&lt;/p&gt;

&lt;h2 id="licenses"&gt;Licenses&lt;/h2&gt;
&lt;p&gt;Most datasets (619) on the
Missouri portal are listed as in the public domain.&lt;/p&gt;

&lt;p&gt;&lt;img src="figure/publicdomain.png" alt="plot of chunk publicdomain" /&gt; &lt;/p&gt;

&lt;p&gt;The rest (278 datasets) have nothing in the “license” metadata field.&lt;/p&gt;

&lt;h2 id="formats"&gt;Formats&lt;/h2&gt;
&lt;p&gt;Proportionately more PDF files are public domain.&lt;/p&gt;

&lt;p&gt;&lt;img src="figure/publicdomain_pdf.png" alt="plot of chunk publicdomain_pdf" /&gt; &lt;/p&gt;

&lt;p&gt;I noticed a bunch of PDF files with names like these.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href="https://data.mo.gov/d/nfur-xwk2"&gt;  SWTWESTP EW 5 HR 2009 A PCW&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="https://data.mo.gov/d/5zv3-u47y"&gt;  SHAWNEEWARD 5 HR 2009 A TMC&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="https://data.mo.gov/d/i2ip-cykb"&gt;PROSPECTSWOPE 5 HR 2009 A TMC&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="https://data.mo.gov/d/6gz3-267n"&gt;    OAKVOLKER 5 HR 2009 A TMC&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="https://data.mo.gov/d/yjr5-e96s"&gt;         OAK9 5 HR 2009 A TMC&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="https://data.mo.gov/d/sqpd-738e"&gt;        OAK12 5 HR 2009 A TMC&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="https://data.mo.gov/d/9ddu-xy68"&gt;       MAIN13 5 HR 2009 A TMC&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="https://data.mo.gov/d/eyii-3nuq"&gt;        MAIN8 5 HR 2009 A TMC&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="https://data.mo.gov/d/n2kq-rg69"&gt;    MADISON43 5 HR 2009 A TMC&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These are all traffic surveys, and it turns out that they account for
almost all of the PDF files on the portal.&lt;/p&gt;

&lt;p&gt;&lt;img src="figure/pdf_trafficsurvey.png" alt="plot of chunk pdf_trafficsurvey" /&gt; &lt;/p&gt;

&lt;h2 id="dataset-kinds"&gt;Dataset kinds&lt;/h2&gt;
&lt;p&gt;A lot of PDF files had public domain licenses, but it’s not clear-cut,
so I looked more specifically at what datasets are on the portal. The only
field in the dataset that really told me much about this was the
&lt;code&gt;title&lt;/code&gt; field, which is what I used above to find the traffic surveys.
I kept going with that for the other datasets.&lt;/p&gt;

&lt;p&gt;I just used a bunch of reasonably simple
&lt;a href="https://github.com/tlevine/socrata-catalog/blob/master/src/missouri.license.r"&gt;string matches&lt;/a&gt;.
For example, a bunch of datasets came from the American Community Survey,
and these said “ACS” or “American Community Survey” in the title.
Data from the federal said “Census”. And liquor license data said either
“Liquor” or “Alcohol”.&lt;/p&gt;

&lt;p&gt;In the plot below, I map out the different kinds of datasets, whether
they are PDF files and whether they have public domain licenses.&lt;/p&gt;

&lt;p&gt;&lt;img src="figure/kinds_counts.png" alt="plot of chunk kinds_counts" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;Here’s the same plot but with proportions instead of counts. This makes
the small bars more apparent.&lt;/p&gt;

&lt;p&gt;&lt;img src="figure/kinds_prop.png" alt="plot of chunk kinds_prop" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;I didn’t manage to get it much more precise than this, but I did find that
the uncategorized datasets containing the word “Missouri” tended not to
have a license.&lt;/p&gt;

&lt;p&gt;&lt;img src="figure/says_missouri.png" alt="plot of chunk says_missouri" class="wide" /&gt;&lt;/p&gt;

&lt;h2 id="the-merits-of-explicitly-stating-that-a-dataset-is-in-the-public-domain"&gt;The merits of explicitly stating that a dataset is in the public domain&lt;/h2&gt;
&lt;p&gt;A few years ago, a few open government advocates came up with
&lt;a href="http://www.opengovdata.org/home/8principles"&gt;eight principles of open government data&lt;/a&gt;.
Among these is the principle that the data be free of license.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Data are not subject to any copyright, patent, trademark or trade secret regulation. Reasonable privacy, security and privilege restrictions may be allowed as governed by other statutes.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In some cases, government data already meet this criterion by default.
As the /unitedstates project &lt;a href="http://theunitedstates.io/licensing/"&gt;explains&lt;/a&gt;,&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Copyright applies to some government data but not others (and never to U.S. law). When it applies, it must be waived by the agency (or owner if not the agency) for the data to meet the “license-free” principle. In this case, data is dedicated to the public domain, not licensed.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In some cases, copyright does not apply at all because it is a government
product. In other cases, it might not apply because the data might be
facts, which can’t be copyrighted. However, it can be quite difficult for
a consumer of data to determine whether copyright applies to a dataset.
Thus, it makes sense to be explicit. As the /unitedstates project explains,&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Data is more valuable when its copyright status is clear through an explicit statement.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id="the-importance-of-an-explicit-statement-for-traffic-surveys"&gt;The importance of an explicit statement for traffic surveys&lt;/h2&gt;
&lt;p&gt;Traffic surveys are a perfect example of where you need to be explicit.&lt;/p&gt;

&lt;h3 id="are-traffic-surveys-facts"&gt;Are traffic surveys facts?&lt;/h3&gt;
&lt;p&gt;In some cases, copyright might not apply to a dataset simply because the
dataset isn’t sufficiently original; if the dataset contains only facts,
copyright won’t apply. This might be okay for parts of the traffic survey
files. Here’s part of a page from
&lt;a href="https://data.mo.gov/Traffic/OAKVOLKER-5-HR-2009-A-TMC/6gz3-267n?"&gt;this traffic survey&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src="OAKVOLKER-5-HR-2009-A-TMC-table.png" alt="A data table" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;The page above just has a table of numbers. This might count as facts, so
it is possible that it would not be original enough for copyright to apply.
This next page is a bit fancier.&lt;/p&gt;

&lt;p&gt;&lt;img src="OAKVOLKER-5-HR-2009-A-TMC-diagram.png" alt="A diagram of a street intersection" /&gt;&lt;/p&gt;

&lt;p&gt;While this diagram contains facts (counts of cars taking different routes
through the intersection), it also contains a fancy drawing of the intersection.
The numbers are facts, and those should be free from copyright, but does
copyright apply to the diagram?&lt;/p&gt;

&lt;h3 id="are-traffic-surveys-government-works"&gt;Are traffic surveys government works?&lt;/h3&gt;
&lt;p&gt;Copyright protection does not apply to government works, but it could apply
to works produced by contractors. From the file on the data portal website,
we can’t tell whether it was produced entirely by government workers, so
we can’t use this law for assurance that the data are free of a license.&lt;/p&gt;

&lt;h3 id="yay-for-explicit-statement"&gt;Yay for explicit statement&lt;/h3&gt;
&lt;p&gt;Given the potential ambiguities above, it is quite important that these traffic
survey files have explicit notices where it’s possible. I commend the state of
Missouri for providing such notices.&lt;/p&gt;

&lt;h2 id="what-i-really-learned-from-this"&gt;What I really learned from this&lt;/h2&gt;
&lt;p&gt;I used this analysis to provide an example of a situation where it is
important to have an explicit statement about the license of a dataset
or about the absence of license. Beyond that, this analysis doesn’t
really tell us much about licensing.&lt;/p&gt;

&lt;p&gt;On the other hand, I feel like I’ve come up with a reasonably complete
grouping of the datasets in the Missouri data portal.&lt;/p&gt;

&lt;p&gt;In the more granular groupings, I think
I’ve arrived at datasets that could naturally be combined into one data
table. This is even true for the traffic survey data; each particular PDF
traffic survey report could be converted into a single row in a data table.&lt;/p&gt;

&lt;p&gt;I arrived at these groupings pretty much entirely by looking at the tables,
but the groupings seem to relate to trends in other variables, like the
license field of the dataset. I’ve been wanting to come up with methods for
relating datasets to each other. I thought these methods would need to be
&lt;a href="/!/open-data-plans#better-search-and-linked-data"&gt;somewhat involved&lt;/a&gt;, but
maybe we can start with something simpler.&lt;/p&gt;

&lt;h2 id="future-study"&gt;Future study&lt;/h2&gt;
&lt;p&gt;In the Missouri portal, I used only title to group datasets; I didn’t
look at the long-form description, the author or the schema. There were
pretty clear-cut divisions between titles, but I would like to see how
reasonable this approach really is; I want to whether groups of similarly
named datasets tend to have similar schemas.&lt;/p&gt;
</content>
  </entry>
</feed>

