<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>http://www.thomaslevine.com/</id>
  <title>Thomas Levine</title>
  <updated>2013-12-18T00:00:00Z</updated>
  <link rel="alternate" href="http://www.thomaslevine.com/"/>
  <link rel="self" href="http://www.thomaslevine.com/!/feed.xml"/>
  <author>
    <name>Thomas Levine</name>
    <uri>http://www.thomaslevine.com</uri>
  </author>
  <entry>
    <id>tag:www.thomaslevine.com,2013-12-18:/!/get-meta/index.html</id>
    <title type="html">Get Meta! (Hire me.)</title>
    <published>2013-12-18T00:00:00Z</published>
    <updated>2013-12-18T00:00:00Z</updated>
    <link rel="alternate" href="http://www.thomaslevine.com/!/get-meta/index.html"/>
    <content type="html">&lt;p&gt;As you may have realized, I’ve recently been &lt;a href="/open-data"&gt;studying&lt;/a&gt;
data about &lt;a href="/!/what-is-open-data"&gt;open data&lt;/a&gt;
so that we can use data to drive our opening of
data. Because data. Data, data, data!&lt;/p&gt;

&lt;p&gt;I want to keep doing this research all day.&lt;/p&gt;

&lt;h2 id="things-i-can-do-for-you"&gt;Things I can do for you&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;If you work for a data-related software company&lt;/strong&gt;, I can prototype or implement new
features in your product to help your users find value in open data. This could involve
&lt;a href="/!/data-updatedness/"&gt;identification of out-of-date data&lt;/a&gt; and
&lt;a href="/!/openprism/"&gt;new ways of searching for data&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;If you work for an organization that wants to release open data&lt;/strong&gt;
or share data internally, I can help you do that better.
By collecting data about your data, I can figure out what’s working
and what isn’t to help you decide what to do next. For example,
I can determine
&lt;a href="/!/missouri-data-licensing/"&gt;what sorts of data you have&lt;/a&gt;,
&lt;a href="/!/open-data-licensing/"&gt;whether your datasets are licensed properly&lt;/a&gt;, and
&lt;a href="/!/socrata-users/"&gt;how people are using your data&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;If you are giving grants in relation to open data&lt;/strong&gt;, we should talk.
I have a &lt;a href="https://github.com/tlevine/open-data-things/tree/master/.plans/proposed"&gt;long list&lt;/a&gt;
of things that I want to do.&lt;/p&gt;

&lt;h2 id="interested"&gt;Interested?&lt;/h2&gt;
&lt;p&gt;If you want any of the things above,
email me at &lt;a href="&amp;#109;&amp;#097;&amp;#105;&amp;#108;&amp;#116;&amp;#111;:&amp;#095;&amp;#064;&amp;#116;&amp;#104;&amp;#111;&amp;#109;&amp;#097;&amp;#115;&amp;#108;&amp;#101;&amp;#118;&amp;#105;&amp;#110;&amp;#101;&amp;#046;&amp;#099;&amp;#111;&amp;#109;"&gt;&amp;#095;&amp;#064;&amp;#116;&amp;#104;&amp;#111;&amp;#109;&amp;#097;&amp;#115;&amp;#108;&amp;#101;&amp;#118;&amp;#105;&amp;#110;&amp;#101;&amp;#046;&amp;#099;&amp;#111;&amp;#109;&lt;/a&gt;.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <id>tag:www.thomaslevine.com,2013-12-17:/!/what-is-open-data/index.html</id>
    <title type="html">What is open data?</title>
    <published>2013-12-17T00:00:00Z</published>
    <updated>2013-12-17T00:00:00Z</updated>
    <link rel="alternate" href="http://www.thomaslevine.com/!/what-is-open-data/index.html"/>
    <content type="html">&lt;p&gt;Part of the reason for &lt;a href="/open-data"&gt;my recent studies&lt;/a&gt; on open data
has been unclarity as to what open data really is. This unclarity has
recently become more clear to me.&lt;/p&gt;

&lt;p&gt;As far as I can tell, open data is the phenomenon of multiple people
creating, sharing, and reading with the same data. We call data “open”
when anyone in the world is allowed to interact with the data, but the
same issues arise even if only some people in the world are allowed
to interact with the data.&lt;/p&gt;

&lt;h2 id="open-data-in-government"&gt;Open data in government&lt;/h2&gt;
&lt;p&gt;&lt;a href="/!/open-data-in-plain-english/"&gt;This&lt;/a&gt; is my favorite video introduction
to the concept of open-data, at least in the context of government.&lt;/p&gt;

&lt;video poster="/!/open-data-in-plain-english/screenshot.png" src="/!/open-data-in-plain-english/open-data-in-plain-english.webm" controls="" width="100%"&gt;&lt;/video&gt;

&lt;p&gt;The general thought is that we can find data that can be released safely
and that releasing these data might allow new good things could happen.&lt;/p&gt;

&lt;p&gt;Because the above video is focusing on governments who are trying to
start getting data out somewhere, it doesn’t discuss more involved sorts
of sharing.  Different government departments, companies, and ordinary
people could all create their own datasets and assist in the collection
or auditing of shared datasets.&lt;/p&gt;

&lt;h2 id="sharing-data-in-general"&gt;Sharing data in general&lt;/h2&gt;
&lt;p&gt;You don’t have to be a government releasing data to the public in
order to be sharing data. For example, if you use a lot of proprietary
data in an organization of at least ten people, your situation is
probably quite similar to that of this open data ecosystem. You have
enough different data sources that not everyone will know everything
about all of the data sources, so you’ll come up with approaches for
storing all of these data, versioning them, finding them, and so on.
Francis Irving and Rufus Pollock would call this a
&lt;a href="http://blog.okfn.org/2012/03/09/from-cms-to-dms-c-is-for-content-d-is-for-data/"&gt;data management system&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;While they don’t share data with the public, large financial institutions
might have some of the strongest open data ecosystems internally.
&lt;a href="http://www.datagotham.com/speakers/#newport"&gt;Bill Newport&lt;/a&gt; spoke at
Data Gotham this year about mananging data at Goldman Sachs. We attendees
were asked not to share what he had said, so I’ll just say that they
were doing everything that I’d been wanting in the area of open data.
And I’ve heard similary awesome things about data management at
Bridgewater and JP Morgan.&lt;/p&gt;

&lt;h2 id="concept-of-ownership"&gt;Concept of ownership&lt;/h2&gt;
&lt;p&gt;When a bunch of people are working collectively on the same dataset,
the concept of ownership changes. One person or group may initially
collect most of the data, but other people may continuously audit the
data, structure the dataset differently, or collect more data. We wind
up with something more like
&lt;a href="http://www.catb.org/~esr/writings/cathedral-bazaar/cathedral-bazaar/"&gt;a bazaar than a cathedral&lt;/a&gt;,
just as we do in free software, wikis, and other forms of free culture.&lt;/p&gt;

&lt;h2 id="plan-for-sharing"&gt;Plan for sharing&lt;/h2&gt;
&lt;p&gt;Now that we are interacting with so much structured digital content,
we’re finding some value in sharing it. Open data is about accepting
that there is value in sharing and collaborating on data and then
planning for this sharing.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <id>tag:www.thomaslevine.com,2013-12-12:/!/my-datasets-about-open-data/index.html</id>
    <title type="html">Datasets about open data</title>
    <published>2013-12-12T00:00:00Z</published>
    <updated>2013-12-12T00:00:00Z</updated>
    <link rel="alternate" href="http://www.thomaslevine.com/!/my-datasets-about-open-data/index.html"/>
    <content type="html">&lt;p&gt;I’ve recently been &lt;a href="/open-data"&gt;studying data about open data&lt;/a&gt;,
and I’m hearing that other people want to do the same. Yay!&lt;/p&gt;

&lt;p&gt;Some people have asked for the data that I’ve collected so that
they can study things without spending a month downloading all
the data and formatting them properly and whatnot.
I haven’t had particularly good answers for these requests, but
the present writing should change that.&lt;/p&gt;

&lt;h2 id="metadata-about-data-about-metadata-from-socrata"&gt;Metadata about data about metadata from Socrata&lt;/h2&gt;
&lt;p&gt;I’ve been using data from Socrata data portal websites for most
of these studies. I think I’ve
provided almost enough information so that people can reuse these
data, and I hope that this article provides the remaining bits
so that people can actually use the data to study open data.&lt;/p&gt;

&lt;h3 id="what-ive-provided-so-far"&gt;What I’ve provided so far&lt;/h3&gt;
&lt;p&gt;There are a lot of annoying quirks in the way
data are modeled, and I’ve explained much of this before.&lt;/p&gt;

&lt;p&gt;I posted spreadsheets of these data quite a long while ago, and
people have been able to download them.&lt;/p&gt;

&lt;h3 id="what-i-havent-provided"&gt;What I haven’t provided&lt;/h3&gt;
&lt;p&gt;I haven’t explained which of the annoying quirks are fixed in the
various spreadsheets.&lt;/p&gt;

&lt;p&gt;Also, I haven’t prominently linked to these
spreadsheets.&lt;/p&gt;

&lt;h2 id="files-ive-produced"&gt;Files I’ve produced&lt;/h2&gt;
&lt;p&gt;Here are some files that I’ve already produced so that you don’t have to
create your own. I’ve referenced the programs that produced these files,
but haven’t been trying to download the data more than once, so I suspect
that the programs won’t work quite as you expect them to.&lt;/p&gt;

&lt;p&gt;These files include only Socrata data.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href="https://github.com/tlevine/socrata-analysis/blob/33af3a27f58a20828801fe750524cb687d66d4e9/socrata.csv"&gt;&lt;code&gt;socrata.csv&lt;/code&gt;&lt;/a&gt;
  is based on metadata files from all views as of July.
  It includes duplicates from both federation and derived views.
  I produced it by searching all the portals
  &lt;a href="#not-removing-duplicates"&gt;without attempting to remove duplicates&lt;/a&gt;.
  The code is &lt;a href=""&gt;here&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;a href="https://github.com/tlevine/socrata-analysis/blob/33af3a27f58a20828801fe750524cb687d66d4e9/users.csv"&gt;&lt;code&gt;users.csv&lt;/code&gt;&lt;/a&gt;
  is a transformation of &lt;code&gt;socrata.csv&lt;/code&gt; such that each row is a user.
  It includes counts of views and tables belonging to each user, but
  I don’t remember how I handled duplicates there. The code for that
  is &lt;a href="https://github.com/tlevine/socrata-analysis/blob/33af3a27f58a20828801fe750524cb687d66d4e9/numbers/run.py#L207"&gt;here&lt;/a&gt;,
  and a high-level explanation is &lt;a href="/!/data-about-open-data-talk-december-2-2013/#data-about-people-who-use-data"&gt;here&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;a href="https://github.com/tlevine/socrata-catalog/blob/3a7d72b5b7332a26dd6066edb44749a634bfe48c/catalogs-2013-08-28.db"&gt;&lt;code&gt;catalogs-2013-08-28.db&lt;/code&gt;&lt;/a&gt; is a table of all of the Socrata DCAT files.
  It has resolved these quirks in these ways, but it still has these quirks&lt;/li&gt;
  &lt;li&gt;&lt;a href="https://github.com/tlevine/socrata-analysis/blob/33af3a27f58a20828801fe750524cb687d66d4e9/socrata-deduplicated.csv"&gt;&lt;code&gt;socrata-deduplicated.csv&lt;/code&gt;&lt;/a&gt;
  is &lt;code&gt;socrata.csv&lt;/code&gt; with
  &lt;a href="#dealing-with-federation"&gt;federation-induced duplicates removed&lt;/a&gt;.
  That is, if data were federated from one portal to three portals,
  this file includes the data from only the first portal. It still
  includes derived data. The code is &lt;a href="https://github.com/tlevine/socrata-defederate/blob/c2c32f5f1db3563db8c4f671d7ebae6ab91c73ec/dedupe.py"&gt;here&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;a href="https://github.com/tlevine/socrata-defederate/blob/c2c32f5f1db3563db8c4f671d7ebae6ab91c73ec/federation.json"&gt;&lt;code&gt;federation.json&lt;/code&gt;&lt;/a&gt; is also produced by
  &lt;a href="https://github.com/tlevine/socrata-defederate/blob/c2c32f5f1db3563db8c4f671d7ebae6ab91c73ec/dedupe.py"&gt;&lt;code&gt;dedupe.py&lt;/code&gt;&lt;/a&gt;.
  (The &lt;a href="https://github.com/tlevine/socrata-defederate/blob/c2c32f5f1db3563db8c4f671d7ebae6ab91c73ec/Makefile"&gt;Makefile&lt;/a&gt; may be informative.)
  I used it to make &lt;a href="/!/socrata-deduplicate/#graph-diagram"&gt;this plot&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These files include data from Socrata, CKAN, Junar, and OpenDataSoft.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href="https://github.com/tlevine/open-data-download/blob/8886bd048e1aafd9e5e64106c87f41394b6a7bcb/licensing.csv"&gt;&lt;code&gt;licensing.csv&lt;/code&gt;&lt;/a&gt;
  contains a record for each dataset across a bunch of catalogs.
  The column &lt;code&gt;license_standard&lt;/code&gt; is the binned license.
  (See &lt;a href="/!/open-data-licensing/#which-licenses"&gt;this plot&lt;/a&gt;.)
  &lt;a href="https://github.com/tlevine/open-data-download/blob/8886bd048e1aafd9e5e64106c87f41394b6a7bcb/query-license.py"&gt;&lt;code&gt;query-license.py&lt;/code&gt;&lt;/a&gt; might get you thinking about how it was produced.&lt;/li&gt;
  &lt;li&gt;&lt;a href="http://openprism.thomaslevine.com"&gt;OpenPrism&lt;/a&gt; lists a bunch of
  data catalog websites &lt;a href="https://github.com/tlevine/openprism/blob/gh-pages/src/index.js"&gt;in its source code&lt;/a&gt;.
  I assembled these lists by checking &lt;a href="http://datacatalogs.org"&gt;datacatalogs.org&lt;/a&gt;,
  by checking the since-changed Socrata status page
  and by contacting Junar and OpenDataSoft directly.&lt;/li&gt;
  &lt;li&gt;It turns out that a bunch of CKAN instances listed on
  &lt;a href="http://datacatalogs.org"&gt;datacatalogs.org&lt;/a&gt; aren’t up anymore.
  &lt;a href="https://github.com/tlevine/open-data-download/blob/3115221f193e08d2e83eb753e8154ea9593fec55/ckan_workingness.csv"&gt;&lt;code&gt;ckan_workingness.csv&lt;/code&gt;&lt;/a&gt; has a list of what’s still working,
  and &lt;a href="https://github.com/tlevine/open-data-download/blob/3115221f193e08d2e83eb753e8154ea9593fec55/download_ckan.py#L62"&gt;&lt;code&gt;download_ckan.py&lt;/code&gt;&lt;/a&gt; shows how that list was assembled.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The remainder of this article summarizes the various quirks that I’ve dealt
with and haven’t dealt with.&lt;/p&gt;

&lt;h2 id="how-data-are-stored-on-socrata"&gt;How data are stored on Socrata&lt;/h2&gt;
&lt;p&gt;Data on Socrata is displayed in &lt;a href="/!/socrata-genealogies#term-view"&gt;views&lt;/a&gt;,
which are queries on &lt;a href="/!/socrata-genealogies#term-table"&gt;tables&lt;/a&gt;.
When you put a new data source on a Socrata portal, you get both
a new table and a &lt;a href="/!/socrata-genealogies#term-dataset"&gt;dataset&lt;/a&gt;
view that is a query of the full table.
(The dataset view would be &lt;code&gt;SELECT * FROM [table]&lt;/code&gt; in SQL.)&lt;/p&gt;

&lt;h3 id="derivatives"&gt;Derivatives&lt;/h3&gt;
&lt;p&gt;In the Socrata web interface, you can create lots of different views
on the same source data. You can make different queries on the same
table, and you can visualize them as charts, maps, and a
&lt;a href="/!/socrata-calendars"&gt;bunch of other things&lt;/a&gt;.
When you save your new view, it gets added to the data portal, and
it will show up in searches.&lt;/p&gt;

&lt;h3 id="federation"&gt;Federation&lt;/h3&gt;
&lt;p&gt;Data are uploaded to one Socrata portal, but they can be
&lt;a href="/!/socrata-genealogies#term-federation"&gt;federated&lt;/a&gt;
to other Socrata portals. Thus, if you get all of the datasets from
all of the portals, you can easily have duplicates and not know which
portal they came from.&lt;/p&gt;

&lt;h3 id="privacy"&gt;Privacy&lt;/h3&gt;
&lt;p&gt;Some data on Socrata are private. In many cases, the source data are
private and a query on the data (a &lt;a href="/!/socrata-genealogies#term-filtered-view"&gt;filtered view&lt;/a&gt;)
is public. Thus, it is possible that the most complete form of the data
that a data publisher is releasing would be in a different type of view
than dataset.&lt;/p&gt;

&lt;h3 id="view-metadata-files"&gt;View metadata files&lt;/h3&gt;
&lt;p&gt;Regardless of whether it’s a dataset, filter, chart, map, or whatever,
each view has a metadata file at a URL like this.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;https://data.maryland.gov/api/views/${id}.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For example, here’s the file for
&lt;a href="https://data.maryland.gov/Energy-and-Environment/Certified-Cover-Crops-Planted-in-the-Chesapeake-Ba/w6r7-apye?"&gt;this view&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;a href="https://data.maryland.gov/api/views/w6r7-apye.json"&gt;https://data.maryland.gov/api/views/w6r7-apye.json&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I’ll refer to this “metadata file” quite a bit in the sections below.&lt;/p&gt;

&lt;h2 id="removing-duplicates-from-derived-view"&gt;Removing duplicates from derived view&lt;/h2&gt;
&lt;p&gt;If we want just the source data, we have to remove these derivative
datasets. I’ll explain removing federated data later.&lt;/p&gt;

&lt;h3 id="not-removing-duplicates"&gt;Not removing duplicates&lt;/h3&gt;
&lt;p&gt;At first, I
&lt;a href="/!/socrata-summary#acquiring-the-data"&gt;didn’t remove duplicates&lt;/a&gt;.
I just searched browsed through all of the search results, followed
the links to the datasets, and downloaded all of the metadata files.
This has problems, as I’ve explained above; below are some better
approaches.&lt;/p&gt;

&lt;h3 id="searching-for-only-dataset-views"&gt;Searching for only dataset views&lt;/h3&gt;
&lt;p&gt;If you run an ordinary search on a portal, filtering for only dataset
views, you’ll get none of the derivatives. It’s possible that you’ll miss
some data where the original dataset is private and only a filter on the
dataset is public.&lt;/p&gt;

&lt;p&gt;If you use the ordinary search webpages rather than the API, you can also
deal with federation in the same step. In the HTML search interface that
is intended for humans, federated datasets are shaded blue and shown as
links to other data portals, rather than being shaded white and linking
within the same portal. When searching the site, simply ignore these
federated datasets, and you’re good.&lt;/p&gt;

&lt;p&gt;All of the links to datasets include the 4x4 identifier for the dataset,
and you can use that to determine the URL of the corresponding metadata file.&lt;/p&gt;

&lt;h3 id="using-apidcatjson"&gt;Using &lt;code&gt;/api/dcat.json&lt;/code&gt;&lt;/h3&gt;
&lt;!-- https://twitter.com/chrismetcalf/status/376079563240898560 --&gt;
&lt;p&gt;The &lt;a href="https://data.oaklandnet.com/api/dcat.json"&gt;&lt;code&gt;/api/dcat.json&lt;/code&gt;&lt;/a&gt; page
is supposed to give us a listing of all of the official data in a portal.
It provides different data from the metadata files I discuss above, but
it includes the dataset identifiers, and you can use those to look up
the corresponding metadata files.&lt;/p&gt;

&lt;p&gt;I was told that the &lt;code&gt;/api/dcat.json&lt;/code&gt; endpoint shows only the source data.
I haven’t checked this myself, and I don’t know how it handles datasets
where the original upload is private but a filter on it is public.
Anyway, let’s just hope that the &lt;code&gt;/api/dcat.json&lt;/code&gt; endpoint lets deals with
the derivatives properly for us.&lt;/p&gt;

&lt;p&gt;In that case, we just have to deal with federation. The
&lt;code&gt;/api/dcat.json&lt;/code&gt; endpoint doesn’t give you any indication as to whether a
dataset is federated, but you can use the method that is described
&lt;a href="#dealing-with-federation"&gt;below&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The bigger problem with &lt;code&gt;/api/dcat.json&lt;/code&gt; is that it only returns the
first 1,000 datasets. I am told that you can use query arguments to
get more, but I’ve never gotten this working.&lt;/p&gt;

&lt;h3 id="collapsing-by-tableid"&gt;Collapsing by tableId&lt;/h3&gt;
&lt;p&gt;This is the approach I wind up using most of the time, mainly because
the initial download of all the metadata took a long time and I didn’t
want to run one again.&lt;/p&gt;

&lt;p&gt;The metadata
file for each view contains a &lt;code&gt;tableId&lt;/code&gt; field corresponding to the
table that stores the data corresponding to the particular view.&lt;/p&gt;

&lt;p&gt;If you download &lt;a href="#not-removing-duplicates"&gt;without removing duplicates&lt;/a&gt;,
you’ll wind up with multiple files that all have the same &lt;code&gt;tableId&lt;/code&gt; field.
If you put all of these files into a spreadsheet, with one row per metadata
file, you’ll wind up with a &lt;code&gt;tableId&lt;/code&gt; column.&lt;/p&gt;

&lt;p&gt;If you group the table by &lt;code&gt;tableId&lt;/code&gt;, you will wind up with one record
per table. It’s still difficult to figure out which view is the
original, but you can aggregate the various fields in different ways
depending on what you need.&lt;/p&gt;

&lt;p&gt;For example, I &lt;a href="https://github.com/tlevine/socrata-analysis/blob/33af3a27f58a20828801fe750524cb687d66d4e9/words/15-dates.r#L17"&gt;aggregated&lt;/a&gt;
to maximum family-wide row count and the sum of family-wide download
counts when I looked at the
&lt;a href="/!/data-updatedness/"&gt;updating of datasets&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id="approaches-i-havent-tried-yet"&gt;Approaches I haven’t tried yet&lt;/h3&gt;
&lt;p&gt;I’ve tried not to concern myself so much with fixing Socrata’s APIs,
so I’ve been trying to deal with duplication in ways that are easy
rather than best. Here are two things you could do to figure out
exactly which view is the original view within a table.&lt;/p&gt;

&lt;h4 id="parsing-queries"&gt;Parsing queries&lt;/h4&gt;
&lt;p&gt;Some view metadata files include a query, presumably the query that
is run on the original table to get the particular subset. By parsing
these, you might be able to determine which view of the views belonging
to one table is the original view.&lt;/p&gt;

&lt;!-- XXX Link to examples. --&gt;

&lt;h4 id="parsing-more-web-pages"&gt;Parsing more web pages&lt;/h4&gt;
&lt;p&gt;I can figure out the genealogy by looking at the web pages, so a program
could be written that does this. I think the site does some AJAXy something
that I haven’t figured out yet, but would be easy to parse this with
something like Selenium, PhantomJS, or jsdom that renders the whole page.&lt;/p&gt;

&lt;h2 id="dealing-with-federation"&gt;Dealing with federation&lt;/h2&gt;
&lt;p&gt;We can deal with federation by looking at the network of federation from the
portal hope pages. Each portal’s homepage has a box like this that says
which portals it federates.&lt;/p&gt;

&lt;p&gt;&lt;img src="/!/socrata-deduplicate/federated-domains.png" alt="Federated Domains" /&gt;&lt;/p&gt;

&lt;p&gt;The particular portal contains all of the datasets that all of these
federated portals contain. If you construct the network of federation
from these portal homepages, you can resolve duplicates properly. If
the same dataset is on a few different portals, it’s because the
different portals all federate the same portal. Since you know what
the network of federation is from the homepages, you can figure out
which portal was the original portal.&lt;/p&gt;

&lt;p&gt;Regardless of the method you used to assemble your dataset of datasets,
the resulting dataset will have a column for portal and a column for
identifier. Once you determine what this network of federation is, you
can look for identical datasets that are present in multiple portals
and then remove datasets that are not from the original portal.
I did that &lt;a href="/!/socrata-deduplicate"&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id="determining-the-size-of-data-views"&gt;Determining the size of data views&lt;/h2&gt;
&lt;p&gt;The metadata file does not explicitly state the number of columns and the
number of rows in the dataset, but there’s enough in the file to figure out
what these two numbers are.&lt;/p&gt;

&lt;p&gt;The metadata files contain a “columns” field, which has a list
of dictionaries. Each of these dictionaries has a count of null
values and a count non-null values, and the sum of these two
counts is the number of columns.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://github.com/tlevine/socrata-analysis/blob/master/numbers/socrata/__init__.py#L93"&gt;This&lt;/a&gt;
is the relevant code, I think. If I’m reading my own code correctly,
the &lt;code&gt;original_data&lt;/code&gt; variable is just a parse of the JSON metadata file.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <id>tag:www.thomaslevine.com,2013-12-02:/!/data-about-open-data-talk-december-2-2013/index.html</id>
    <title type="html">100,000 open data across 100 portal</title>
    <published>2013-12-02T00:00:00Z</published>
    <updated>2013-12-02T00:00:00Z</updated>
    <link rel="alternate" href="http://www.thomaslevine.com/!/data-about-open-data-talk-december-2-2013/index.html"/>
    <content type="html">&lt;p&gt;Here are some materials for
&lt;a href="http://www.meetup.com/NYC-Open-Data/events/147380312/"&gt;my talk at NYC Open Data&lt;/a&gt;,
but they’re written in normal language, so they’ll probably serve as a decent summary
of my work thus far to people who are reading this on the internet.&lt;/p&gt;

&lt;h2 id="schedule"&gt;Schedule&lt;/h2&gt;
&lt;p&gt;The schedule for the talk will go sort of like this&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;7:00 to 7:50 I’ll talk about what I did&lt;/li&gt;
  &lt;li&gt;7:50 to 8:00 Questions about what I did&lt;/li&gt;
  &lt;li&gt;8:00 to 8:10 Introduce the exercises&lt;/li&gt;
  &lt;li&gt;8:10 to 8:45 Work on the exercises&lt;/li&gt;
  &lt;li&gt;8:45 to 9:00 Present findings from the exercises&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here’s how the exercises will work. Attendees will break into groups of four people
each. Each group will choose one of the two exercises and work on it for half an hour. 
At 8:45, each of the groups will have one minute to present its conclusions to the
larger group.&lt;/p&gt;

&lt;h2 id="outline-of-the-talk"&gt;Outline of the talk&lt;/h2&gt;

&lt;p&gt;Introduction (4 minutes)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;What are “open data”? Show the video.&lt;/li&gt;
  &lt;li&gt;Data about open data, data-driven open data&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Two approaches (2 minutes)&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;We have all of these data, so something interesting must be in it.&lt;/li&gt;
  &lt;li&gt;We are interested in something. Let’s collect data that will tell us about that thing.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;My process and findings (6 minutes each)&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Data about open data.&lt;/li&gt;
  &lt;li&gt;Data about people who use open data.&lt;/li&gt;
  &lt;li&gt;Finding data is hard.&lt;/li&gt;
  &lt;li&gt;File formats of open data&lt;/li&gt;
  &lt;li&gt;Licensing of “open” data&lt;/li&gt;
  &lt;li&gt;Updating of data&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Future things (3 minutes)&lt;/p&gt;

&lt;h2 id="data-about-open-data"&gt;Data about open data&lt;/h2&gt;
&lt;p&gt;Let’s talk about some things I’ve been &lt;a href="/open-data"&gt;learning about open data&lt;/a&gt;.
For the longest time, I had no idea what people meant when they were talking
about “open data”. But now I have &lt;a href="/!/open-data-in-plain-english"&gt;this video&lt;/a&gt;
that gives a decent explanation, at least for government data.&lt;/p&gt;

&lt;p&gt;To some degree, “open data” is just the sharing of data, but we have a word for
it because people aren’t used to this idea. Sharing data within just company is
already pretty hard, but good things might happen once you do it.&lt;/p&gt;

&lt;p&gt;One benefit of open data might be the ability for people to use lots of different
datasets in order to make data-driven decisions. The people who are releasing open
data surely get this, so they’re obviously using data to make decisions about their
open data initiatives, right?&lt;/p&gt;

&lt;p&gt;Actually, they’re not, so I started doing that. Also, I’m doing it quite publicly,
so you could say this is open data about open data.&lt;/p&gt;

&lt;h2 id="my-process-and-findings"&gt;My process and findings&lt;/h2&gt;
&lt;p&gt;I was taught in school that you come up with your question and then collect data
that perfectly answer that question. This way works, but you can often learn more
faster and with less work if you’re a bit sloppier.&lt;/p&gt;

&lt;p&gt;I like to think of two approaches of deciding what to study.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;We have all of these data, so something interesting must be in it.&lt;/li&gt;
  &lt;li&gt;We are interested in something. Let’s collect data that will tell us about that thing.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I think the former is more obvious: Initially, I found it quite odd that
nobody had looked at the data about the data. So I did.&lt;/p&gt;

&lt;p&gt;Let’s talk a bit about the latter. Let’s say we want to study someone’s sleep patterns.
In order to do this, we wind to find out when the person is sleeping. We could do this
by having the person record on paper the times at which she goes to sleep and wakes up,
but that would be a lot of work. Other ideas&lt;/p&gt;

&lt;p&gt;&lt;a href="http://yihui.name/en/2009/10/50000-revisions-committed-to-r/"&gt;Version control commits&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src="r-commits.gif" alt="R commits" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://hackpad.com/Measuring-Socioeconomic-Indicators-in-Arabic-Tweets-IZ5ByP2LvIt"&gt;Tweets&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src="tweet-times.jpg" alt="Bar plot of Tweet times" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;If the person is me, we can use shell history activity.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/sh
# This file is history.sh
for epochtime in $(grep '^#[0-9]\{10\}$' ~/.history/sh-2013-1[12]*|cut -d\# -f2); do
  date --date=@$epochtime +%H
done | sort | uniq -c | awk '{print $2, "%"$1"s"}' &amp;gt; /tmp/formatted

while read line; do
  # Remove the first space
  nospace=$(echo $line | sed 's/ //')
  printf "$line\n" | tr \  -|sed s/----------------------------------------/=/g|sed -e s/-//g -e 's/=/ =/'
done &amp;lt; /tmp/formatted
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here’s the resulting histogram.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./history.sh
00 =========
01 =======
02 =====
03 ====
04 =======
05 ============
06 ======
07 ====
08 =
09 =
10 =====
11 ===
12 ==
13 ======
14 =====
15 ======
16 ==============
17 ==============================
18 =======================================
19 ============================
20 ===============
21 ==========
22 ==================
23 ==================
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that these times are in UTC because that’s how I roll.&lt;/p&gt;

&lt;p&gt;In this approach of deciding what to study, the idea is that we can
answer our curiosities by building on some existing data collection.
Also, I have some brief thoughts on brainstorming &lt;a href="/!/brainstorming"&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you want to really understand how I did all of this, pay attention to the following things throughout the talk.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;How I outlined my programs&lt;/li&gt;
  &lt;li&gt;How I created simple variables to represent grand concepts&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id="data-about-data"&gt;Data about data&lt;/h3&gt;

&lt;h4 id="getting-the-data"&gt;Getting the data&lt;/h4&gt;
&lt;p&gt;&lt;a href="/!/socrata-summary"&gt;&lt;img src="/!/socrata-summary/architecture.jpg" alt="Diagram about downloading Socrata data" class="wide" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Now I have a spreadsheet of datasets.&lt;/p&gt;

&lt;p&gt;&lt;a href="/!/dataset-as-datapoint"&gt;&lt;img src="/!/dataset-as-datapoint/spreadsheet-spreadsheet.png" alt="A spreadsheet of spreadsheets" class="wide" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Here are the some of fields I get from that.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;portal&lt;/li&gt;
  &lt;li&gt;id&lt;/li&gt;
  &lt;li&gt;name&lt;/li&gt;
  &lt;li&gt;attribution&lt;/li&gt;
  &lt;li&gt;averageRating&lt;/li&gt;
  &lt;li&gt;category&lt;/li&gt;
  &lt;li&gt;createdAt&lt;/li&gt;
  &lt;li&gt;description&lt;/li&gt;
  &lt;li&gt;displayType&lt;/li&gt;
  &lt;li&gt;downloadCount&lt;/li&gt;
  &lt;li&gt;numberOfComments&lt;/li&gt;
  &lt;li&gt;oid&lt;/li&gt;
  &lt;li&gt;publicationAppendEnabled&lt;/li&gt;
  &lt;li&gt;publicationDate&lt;/li&gt;
  &lt;li&gt;publicationStage&lt;/li&gt;
  &lt;li&gt;publicationGroup&lt;/li&gt;
  &lt;li&gt;rowsUpdatedBy&lt;/li&gt;
  &lt;li&gt;rowsUpdatedAt&lt;/li&gt;
  &lt;li&gt;signed&lt;/li&gt;
  &lt;li&gt;tableId&lt;/li&gt;
  &lt;li&gt;totalTimesRated&lt;/li&gt;
  &lt;li&gt;viewCount&lt;/li&gt;
  &lt;li&gt;viewLastModified&lt;/li&gt;
  &lt;li&gt;viewType&lt;/li&gt;
  &lt;li&gt;nrow&lt;/li&gt;
  &lt;li&gt;column names and types&lt;/li&gt;
  &lt;li&gt;owner.id&lt;/li&gt;
  &lt;li&gt;owner.displayName&lt;/li&gt;
  &lt;li&gt;owner.emailUnsubscribed&lt;/li&gt;
  &lt;li&gt;owner.privacyControl&lt;/li&gt;
  &lt;li&gt;owner.profileLastModified&lt;/li&gt;
  &lt;li&gt;owner.roleName&lt;/li&gt;
  &lt;li&gt;owner.screenName&lt;/li&gt;
  &lt;li&gt;owner.rights&lt;/li&gt;
  &lt;li&gt;tableAuthor.id&lt;/li&gt;
  &lt;li&gt;tableAuthor.displayName&lt;/li&gt;
  &lt;li&gt;tableAuthor.emailUnsubscribed&lt;/li&gt;
  &lt;li&gt;tableAuthor.privacyControl&lt;/li&gt;
  &lt;li&gt;tableAuthor.profileLastModified&lt;/li&gt;
  &lt;li&gt;tableAuthor.roleName&lt;/li&gt;
  &lt;li&gt;tableAuthor.screenName&lt;/li&gt;
  &lt;li&gt;tableAuthor.rights&lt;/li&gt;
  &lt;li&gt;displayFormat&lt;/li&gt;
  &lt;li&gt;flags&lt;/li&gt;
  &lt;li&gt;metadata&lt;/li&gt;
  &lt;li&gt;rights&lt;/li&gt;
  &lt;li&gt;tags&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id="what-i-found"&gt;What I found&lt;/h4&gt;
&lt;p&gt;First, nobody has any idea of what is going on in open data.
This was my main conclusion after I tweeted about &lt;a href="/!/socrata-summary/"&gt;this article&lt;/a&gt;;
I thought it would not be that interesting, but people strangely liked it.
Many people know about datasets that are relevant to their work,
municipality, &amp;amp;c., but nobody seems to know about the availability of
data on broader topics, and nobody seems to have a good way of
finding out what is available. And nobody has a great idea of who
is using which data.&lt;/p&gt;

&lt;p&gt;Second, resolving &lt;a href="/!/socrata-genealogies/#types-of-duplicate-datasets"&gt;duplicate datasets&lt;/a&gt; is annoying. Three types of duplication&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;SODA queries: Filtered views, charts, maps&lt;/li&gt;
  &lt;li&gt;Federation&lt;/li&gt;
  &lt;li&gt;Uploaded twice&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id="data-about-people-who-use-data"&gt;Data about people who use data&lt;/h3&gt;
&lt;p&gt;Let’s look a bit at how people interact with these data. One of Socrata’s
features is built-in charting tools that are supposed to
“&lt;a href="http://www.socrata.com/open-innovation/"&gt;consumeriz[e] the data experience&lt;/a&gt;”
Basically, you can go to &lt;code&gt;data.cityofnewyork.us&lt;/code&gt; or any Socrata site, find
an existing dataset, and make a new chart, map, query, &amp;amp;c. from it.
It turns out that Socrata exposes a lot of knowledge about how this feature
gets used.&lt;/p&gt;

&lt;h4 id="getting-the-data-1"&gt;Getting the data&lt;/h4&gt;
&lt;p&gt;Notice the “owner” and “tableAuthor” fields in the previous download.
These refer to user accounts in Socrata.&lt;/p&gt;

&lt;p&gt;Internally, each new chart is represented as a “view” on the underlying
data “table”.&lt;/p&gt;

&lt;p&gt;&lt;a href="/!/socrata-genealogies#term-table"&gt;&lt;img src="/!/socrata-genealogies/family.jpg" alt="A date table family in Socrata" class="wide" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Anyway, if I use just these columns,
I now have a dataset of users. I didn’t use SQL, but if I had, the
query would have been sort of like this.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SELECT * FROM (
  SELECT 
    "owner.id",
    "owner.displayName",
    "owner.emailUnsubscribed",
    "owner.privacyControl",
    "owner.profileLastModified",
    "owner.roleName",
    "owner.screenName",
    "owner.rights"
  FROM "datasets"
  UNION ALL
  SELECT 
    "tableAuthor.id",
    "tableAuthor.displayName",
    "tableAuthor.emailUnsubscribed",
    "tableAuthor.privacyControl",
    "tableAuthor.profileLastModified",
    "tableAuthor.roleName",
    "tableAuthor.screenName",
    "tableAuthor.rights"
  FROM "datasets"
)
GROUP BY "id"
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That is, I combine stack the owner columns and tableAuthor columns into one
table and then remove duplicates based on the &lt;code&gt;id&lt;/code&gt; field. If I didn’t remove
duplicates, I would have multiple rows per user.
(The query would actually be a bit more complicated than this because it would
have to count how many times a user owns a view and has authored a table.)&lt;/p&gt;

&lt;p&gt;Don’t worry if that didn’t make sense to you; the point is that we can use
datasets in different ways than they seem to be intended.&lt;/p&gt;

&lt;h4 id="what-i-found-1"&gt;What I found&lt;/h4&gt;
&lt;p&gt;My main conclusion is that people don’t use these charting tools all that much.&lt;/p&gt;

&lt;h5 id="big-users"&gt;Big users&lt;/h5&gt;
&lt;p&gt;Most of the users in the dataset (7790 to be exact) had made exactly one view.
Actually, there are probably even more with no views, but I don’t have the
data on them.&lt;/p&gt;

&lt;p&gt;&lt;img src="/!/socrata-users/figure/n.views.png" alt="" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;Similarly, the users who have owned and authored the most tables tend to work
for either Socrata or clients of Socrata.&lt;/p&gt;

&lt;p&gt;Neither of these discoveries should be a surprise; you can call it the
&lt;a href="http://en.wikipedia.org/wiki/Pareto_principle"&gt;Pareto principle&lt;/a&gt; if you want.&lt;/p&gt;

&lt;h5 id="consumerizing"&gt;Consumerizing&lt;/h5&gt;
&lt;p&gt;I wanted to see examples of this consumerized data analysis that was being
advertised, so I tried to find users who were not employed by Socrata or its
clients. I eventually &lt;a href="/!/socrata-users/#also-no-tables"&gt;found some&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As I said above, my main conclusion is that people don’t use these charting
tools all that much. More specifically,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The people who create the most charts are people who maintain data portals&lt;/li&gt;
  &lt;li&gt;Aside from those who maintain data portals, the people who create the most
 charts are usually making different charts of the same data.&lt;/li&gt;
  &lt;li&gt;I found a small number of people who seem to be using the charts for broader
 things. I haven’t really talked to any of them, but the little I do know of
 their stories is interesting.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id="finding-data-is-hard"&gt;Finding data is hard&lt;/h3&gt;
&lt;p&gt;I realized that I using my spreadsheet rather than Socrata’s search tool to look
up data. This was funny, and it pointed out to me an interesting phenomenon about
the sharing of government data. As I said earlier, nobody has any idea of what is
going on with open data. At a most basic level, even though we have these catalogs
of datasets, people can’t really figure out what is in the catalog.&lt;/p&gt;

&lt;p&gt;I have &lt;a href="/!/openprism"&gt;identified&lt;/a&gt;
two broad categories of issues related to this.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Naive search method&lt;/li&gt;
  &lt;li&gt;Siloed open data portals&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Let’s talk about the second one.&lt;/p&gt;

&lt;p&gt;&lt;img src="unsilo.jpg" alt="Diagram about siloed open data portals and some layer to un-silo them" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;I made a &lt;a href="http://openprism.thomaslevine.com"&gt;rather simple site&lt;/a&gt; to demonstrate this idea.&lt;/p&gt;

&lt;h3 id="file-formats"&gt;File formats&lt;/h3&gt;

&lt;p&gt;We’re supposed to use certain file formats.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;“Mandate open formats for government data” (&lt;a href="http://sunlightfoundation.com/opendataguidelines/#open-formats"&gt;Sunlight Foundation&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;“structured data” (&lt;a href="http://inkdroid.org/journal/2010/06/04/the-5-stars-of-open-linked-data/"&gt;5 stars&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;“Data Must Be Machine processable” (&lt;a href="http://www.opengovdata.org/home/8principles"&gt;Open Government Working Group&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id="getting-the-data-2"&gt;Getting the data&lt;/h4&gt;
&lt;p&gt;This time, I used the &lt;code&gt;data.json&lt;/code&gt; endpoint, which is supposed to return
a &lt;a href="http://project-open-data.github.io/schema/"&gt;DCAT&lt;/a&gt; listing of all of
the datasets. It turns out that this endpoint
&lt;a href="/!/socrata-formats/#cutoff-at-1000"&gt;isn’t implemented properly&lt;/a&gt;,
but we’ll make do&lt;/p&gt;

&lt;h4 id="what-i-found-2"&gt;What I found&lt;/h4&gt;
&lt;p&gt;What are the file formats?&lt;/p&gt;

&lt;p&gt;&lt;img src="/!/socrata-formats/figure/all-formats.png" alt="Bar plot of file formats by portal" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;It turns out that file formats tell you quite a bit about the type of data too.
Take a look at &lt;a href="/!/missouri-data-licensing/"&gt;Missouri&lt;/a&gt;&lt;/p&gt;

&lt;h3 id="licensing"&gt;Licensing&lt;/h3&gt;
&lt;p&gt;Other data catalog software &lt;a href="https://github.com/tlevine/open-data-download"&gt;works differently&lt;/a&gt;
than Socrata, but the process it isn’t any more fancy. I downloaded data from catalogs running
these software.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Socrata&lt;/li&gt;
  &lt;li&gt;CKAN&lt;/li&gt;
  &lt;li&gt;OpenDataSoft&lt;/li&gt;
  &lt;li&gt;Junar&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And then I &lt;a href="http://thomaslevine.com/!/open-data-licensing/"&gt;looked at&lt;/a&gt;
the licenses that different datasets have.&lt;/p&gt;

&lt;p&gt;&lt;img src="/!/open-data-licensing/p2.png" alt="Licenses across all portals" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;Most data catalogs either have a license on everything or a license on nothing.)&lt;/p&gt;

&lt;p&gt;&lt;img src="/!/open-data-licensing/p1.png" alt="Bar graph of proportion of datasets" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;As I said before, &lt;a href="/!/missouri-data-licensing/"&gt;Missouri&lt;/a&gt; is interesting.
Also, they get this licensing right.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;a href="http://opendatacommons.org/faq/"&gt;Licensing is important because it reduces uncertainty.&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id="updating"&gt;Updating&lt;/h3&gt;
&lt;p&gt;Open government data are supposed to be kept up-to-date.
&lt;a href="http://thomaslevine.com/!/data-updatedness/#even-simpler"&gt;Pretty much nobody&lt;/a&gt; does this.&lt;/p&gt;

&lt;h4 id="getting-the-data-3"&gt;Getting the data&lt;/h4&gt;
&lt;p&gt;Recall that there were some date fields in those Socrata data.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;createdAt&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;publicationDate&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;rowsUpdatedAt&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;viewLastModified&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Once I figured out what these meant and dealt with &lt;a href="/!/data-updatedness#removing-duplicates"&gt;duplicates&lt;/a&gt;,
I could check whether datasets were being updated.&lt;/p&gt;

&lt;h4 id="what-i-found-3"&gt;What I found&lt;/h4&gt;
&lt;p&gt;First, hardly any datasets ever get updated.&lt;/p&gt;

&lt;p&gt;&lt;img src="/!/data-updatedness/figure/any_update.png" alt="Hardly any datasets get updated" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;Second, the ones that have been updated were mostly updated two years ago.
There might have been some bulk Socrata migration at the beginning of September 2011.&lt;/p&gt;

&lt;p&gt;&lt;img src="/!/data-updatedness/figure/publish_v_update.png" alt="Bulk migration?" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;Here are the datasets that got published before 2013 and got updated during 2013.&lt;/p&gt;

&lt;p&gt;&lt;img src="/!/data-updatedness/figure/publish_v_update_2013.png" alt="Old data still kept up-to-date" class="wide" /&gt;&lt;/p&gt;

&lt;p&gt;It’s only 13 datasets.&lt;/p&gt;

&lt;p&gt;&lt;img src="/!/data-updatedness/figure/updates_2013_url.png" alt="Those 13 datasets, by portal" class="wide" /&gt;&lt;/p&gt;

&lt;h2 id="future-things"&gt;Future things&lt;/h2&gt;
&lt;p&gt;The general thing I’m doing here is just studying data about open data.
People haven’t done much of this, so it’s turning up some interesting thing.&lt;/p&gt;

&lt;p&gt;I’ve started seeing four perspectives I could take in future study,
and the general idea for all of these is to automate existing manual processes.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Check how well open data guidelines are being followed.&lt;/li&gt;
  &lt;li&gt;Help people find data of interest to them; come up with something better than our current search bars.&lt;/li&gt;
  &lt;li&gt;Fill in blank metadata fields.&lt;/li&gt;
  &lt;li&gt;Figure out what makes for good data sharing; what are the impacts of organizational structures,
 hackathons, data catalog software, and open data policies on things that we care about?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;They’re really all the same thing, actually, but
I’m focusing on the first of these for the immediate future.&lt;/p&gt;

&lt;h2 id="exercises"&gt;Exercises&lt;/h2&gt;
&lt;p&gt;Attendees of this &lt;a href="http://www.meetup.com/NYC-Open-Data/events/147380312/"&gt;NYC Open Data meetup&lt;/a&gt;
typically want to learn exactly how to do things, rather than just getting a general
idea of some new idea. (At least, this is the impression I get.) It’s sort of “open data”
from a different angle; if everyone knows how to do things with data, then even messy data
would be quite open in a sense. But I digress.&lt;/p&gt;

&lt;p&gt;Let’s learn how to plan a crazy project like this. I’ve prepared two exercises.&lt;/p&gt;

&lt;h3 id="outlining-a-program"&gt;Outlining a program&lt;/h3&gt;
&lt;p&gt;Choose an open data catalog from this list.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href="http://data.dc.gov"&gt;Washington, District of Columbia&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="http://www.civicapps.org/datasets"&gt;Greater Portland, Oregon&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="http://www.utah.gov/open"&gt;Utah&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="http://nhopengov.org"&gt;New Hampshire&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="http://portal.louisvilleky.gov/service/data"&gt;Louisville, Kentucky&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="http://www.opendataphilly.org"&gt;Philidelphia, Pennsylvania&lt;/a&gt; (It runs &lt;a href="https://github.com/azavea/Open-Data-Catalog/"&gt;this software&lt;/a&gt;.)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;First, diagram how a person could manually download all of the datasets.
You want to get the most raw form available, not the sort of aggregates
that you might see in a plot.&lt;/p&gt;

&lt;p&gt;After you’ve done that, change the labels in the diagram so that it describes
a computer program that downloads the datasets.&lt;/p&gt;

&lt;p&gt;If you’re lucky, you’ll find API documentation, but you don’t need it;
figure out what the API is, and write the documentation yourself.&lt;/p&gt;

&lt;p&gt;For the one-minute presentation, walk through your outline of your program.
You can draw a diagram, write out steps in words, click through the website,
or just explain it without any visuals.&lt;/p&gt;

&lt;h3 id="using-simple-variables-to-represent-grand-concepts"&gt;Using simple variables to represent grand concepts&lt;/h3&gt;
&lt;p&gt;Select a document from this list, then select a single guideline within
the document. Brainstorm ways that you could test how well the guideline
is being followed. Try to come up with approaches that don’t involve
much manual work.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Open Knowledge Foundation &lt;a href="http://census.okfn.org/"&gt;Open Data Census&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Tim Berners-Lee &lt;a href="http://inkdroid.org/journal/2010/06/04/the-5-stars-of-open-linked-data/"&gt;Five Stars&lt;/a&gt; of open linked data.
  &lt;!-- http://opendata.stackexchange.com/a/529 --&gt;&lt;/li&gt;
  &lt;li&gt;Open Government Working Group &lt;a href="http://www.opengovdata.org/home/8principles"&gt;8 Principles of Open Government Data&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Sunlight Foundation &lt;a href="http://sunlightfoundation.com/opendataguidelines/"&gt;Open Data Policy Guidelines&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Open Data Institute &lt;a href="https://certificates.theodi.org/"&gt;Certificates&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For the one-minute presentation, show us the open data guidelines that you
chose and explain the approaches you came up with.&lt;/p&gt;

&lt;p&gt;You’ll probably have time to look at more than one guideline, but you probably
won’t have time to talk about more than two. If this is the case, choose
one or two that you though were most interesting.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <id>tag:www.thomaslevine.com,2013-11-27:/!/open-data-500-global-open-data-initiative-survey/index.html</id>
    <title type="html">Can we open our open data questionnaires?</title>
    <published>2013-11-27T00:00:00Z</published>
    <updated>2013-11-27T00:00:00Z</updated>
    <link rel="alternate" href="http://www.thomaslevine.com/!/open-data-500-global-open-data-initiative-survey/index.html"/>
    <content type="html">&lt;p&gt;On September 19, I got
&lt;a href="http://us6.campaign-archive2.com/?u=1a990feb5c&amp;amp;id=46f97c829f&amp;amp;e=f59483af9a"&gt;this email&lt;/a&gt;
from the &lt;a href="http://thegovlab.org/"&gt;GovLab&lt;/a&gt;.
It requested that tell them about companies who are using open data
by filling out a questionnaire within two weeks (September 27).&lt;/p&gt;

&lt;p&gt;On November 24, I saw a tweet about a
&lt;a href="http://sunlightfoundation.com/blog/2013/11/20/the-global-open-data-initiative-needs-your-input/"&gt;strangely similar request&lt;/a&gt;
from the Sunlight Foundation,
with the questionnaire due November 29.&lt;/p&gt;

&lt;h2 id="sharing-data-to-save-time"&gt;Sharing data to save time&lt;/h2&gt;
&lt;p&gt;These questionnaires are quite similar; could we (GovLab, Sunlight,
and people filling out questionnaires) maybe save some work if GovLab
and Sunlight shared the questionnaire data?&lt;/p&gt;

&lt;p&gt;The questionnaires are not identical, so the questionnaire of one of
these groups might not satisfy the needs of the other, but I think it
would at least help.&lt;/p&gt;

&lt;h2 id="open-our-data-about-open-data"&gt;Open our data about open data&lt;/h2&gt;
&lt;p&gt;It’s quite possible that Sunlight has seen GovLab’s data. I hope they
have, as that probably would have saved some time. But even if they did,
I find it hilarious that &lt;strong&gt;I&lt;/strong&gt; haven’t seen the results of at least the
GovLab questionnaire.&lt;/p&gt;

&lt;p&gt;Lots of people say that much of open data is about
&lt;a href="http://sunlightfoundation.com/opendataguidelines/#open-by-default"&gt;setting the default to open&lt;/a&gt;,
so you would think that raw results of these questionnaires would be published.
I guess Sunlight might want to wait until it closes the questionnaire before it
publishes anything, but GovLab could totally have released its data by now.&lt;/p&gt;

&lt;h2 id="sharing-data-to-get-people-to-contribute-data"&gt;Sharing data to get people to contribute data?&lt;/h2&gt;
&lt;p&gt;When I saw each of these questionnaires, I considered sending it to
people, but nine days didn’t seem like enough time for it to be worth
me telling people about them.&lt;/p&gt;

&lt;p&gt;Actually, that’s not really true; that’s what I say to myself, but
it’s really an excuse; the real reason I didn’t send them around is
that I’m generally quite skeptical that anyone is going to look
closely at the questionnaire. Assembling the questionnaire and
sending it out is a lot of work already; are they actually going
to have any energy left once they get the results back?&lt;/p&gt;

&lt;p&gt;I wonder: Would opening the results of questionnaires would get me
to fill out the questionnaires?&lt;/p&gt;
</content>
  </entry>
</feed>

