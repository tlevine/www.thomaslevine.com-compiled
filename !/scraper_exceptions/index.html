<!DOCTYPE html>
<!--[if lt IE 7]> <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>    <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>    <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!-->
<html class='no-js'>
  <!--<![endif]-->
  <head>
    <meta charset='utf-8'>
    <title>Handling exceptions in scrapers</title>
    <meta content='' name='description'>
    <meta content='Thomas Levine' name='author'>
    <link href='http://domain/humans.txt' rel='author' type='text/plain'>
    <meta content='nanoc 3.6.4' name='generator'>
    <meta content='width=device-width' name='viewport'>
    <meta content='summary' name='twitter:card'>
    <meta content='@thomaslevine' name='twitter:site'>
    <meta content='Handling exceptions in scrapers' name='twitter:title'>
    <meta content='' name='twitter:description'>
    <meta content='@thomaslevine' name='twitter:creator'>
    <meta content='http://thomaslevine.com/apple-touch-icon-144x144-precomposed.png' name='twitter:image:src'>
    <meta content='thomaslevine.com' name='twitter:domain'>
    <meta content='' name='twitter:app:name:iphone'>
    <meta content='' name='twitter:app:name:ipad'>
    <meta content='' name='twitter:app:name:googleplay'>
    <meta content='' name='twitter:app:url:iphone'>
    <meta content='' name='twitter:app:url:ipad'>
    <meta content='' name='twitter:app:url:googleplay'>
    <meta content='' name='twitter:app:id:iphone'>
    <meta content='' name='twitter:app:id:ipad'>
    <meta content='' name='twitter:app:id:googleplay'>
    <meta content='http://thomaslevine.com/!/scraper_exceptions/' property='og:url'>
    <meta content='thomaslevine.com' property='og:site_name'>
    <meta content='' property='og:description'>
    <meta content='Handling exceptions in scrapers' property='og:title'>
    <meta content='http://thomaslevine.com/apple-touch-icon-144x144-precomposed.png' property='og:image'>
    <link href='/favicon.ico' rel='icon' type='image/x-icon'>
    <link href='/!/feed.xml' rel='alternate' title='Thomas Levine' type='application/atom+xml'>
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:400italic,400,700' rel='stylesheet' type='text/css'>
    <script src='https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML' type='text/javascript'></script>
    <link href='/css/style-cb653401acb.css' rel='stylesheet'>
    <script src='/js/modernizr-cb42306a279.js'></script>
  </head>
  <body>
    <!--[if lt IE 7 ]>
      <p class='chromeframe'>
        You are using an <strong>outdated</strong> browser.
        Please <a href="http://browsehappy.com/">upgrade your browser</a> or
        <a href="http://www.google.com/chromeframe/?redirect=true">activate Google Chrome Frame</a>
        to improve your experience.
      </p>
    <![endif]-->
    <div id='wrapper'>
      <div id='container'>
        <nav>
          <ul class='nobullet'>
            <li class='link'>
              <a href='/'>
                <div>~</div>
              </a>
            </li>
            <li class='link'>
              <a href='/!/'>
                <div>!</div>
              </a>
            </li>
            <li class='link'>
              <a href='/!/about/'>
                <div>?</div>
              </a>
            </li>
          </ul>
        </nav>
        <div id='article-wrapper'>
          <article>
            <header>
              <h1>Handling exceptions in scrapers</h1>
              <em>
                <time datetime='2012-04-17'>
                  April 17, 2012
                </time>
              </em>
              <span class='tags'>
              </span>
            </header>
            <hr>
            <p>When requesting and parsing data from a source with unknown properties
            and random behavior (i.e., scraping), I expect all kinds of bizarrities to occur.
            Managing exceptions is particularly helpful in such cases.</p>
            
            <p>Here is some ways that an exception might be raised.</p>
            
            <pre><code>[][0] #The list has no zeroth element, so this raises an IndexError&#x000A;{}['foo'] #The dictionary has no foo element, so this raises a KeyError&#x000A;</code></pre>
            
            <p>I would generally try to prevent the exception
            from coming up in the first place,
            but catching the exception might make sense.</p>
            
            <h2 id="example-1-inconsistant-date-formats">Example 1: Inconsistant date formats</h2>
            <p>Let’s say we’re parsing dates.</p>
            
            <pre><code>import datetime&#x000A;</code></pre>
            
            <p>This doesn’t raise an error.</p>
            
            <pre><code>datetime.datetime.strptime('2012-04-19', '%Y-%m-%d')&#x000A;</code></pre>
            
            <p>But this does.</p>
            
            <pre><code>datetime.datetime.strptime('April 19, 2012', '%Y-%m-%d')&#x000A;</code></pre>
            
            <p>It raises a ValueError because the date formats don’t match.
            So what do we do if we’re scraping a data source with multiple date formats?</p>
            
            <h3 id="ignoring-unexpected-date-formats">Ignoring unexpected date formats</h3>
            <p>A simple thing is to ignore the date formats that we didn’t expect.</p>
            
            <pre><code>import lxml.html&#x000A;import datetime&#x000A;&#x000A;def parse_date1(source):&#x000A;    rawdate = lxml.html.fromstring(source).get_element_by_id('date').text&#x000A;&#x000A;    try:&#x000A;         cleandate = datetime.datetime.strptime(rawdate, '%Y-%m-%d')&#x000A;    except ValueError:&#x000A;         cleandate = None&#x000A;&#x000A;    return cleandate&#x000A;&#x000A;print parse_date1('&lt;div id="date"&gt;2012-04-19&lt;/div&gt;')&#x000A;</code></pre>
            
            <p>If we make a clean date column in a database and put this in there,
            we’ll have some rows with dates and some rows with nulls. If there
            are only a few nulls, we might just parse those by hand.</p>
            
            <h3 id="trying-multiple-date-formats">Trying multiple date formats</h3>
            <p>Maybe we have determined that this particular data source uses
            three different date formats. We can try all three.</p>
            
            <pre><code>import lxml.html&#x000A;import datetime&#x000A;&#x000A;def parse_date2(source):&#x000A;    rawdate = lxml.html.fromstring(source).get_element_by_id('date').text&#x000A;&#x000A;    for date_format in ['%Y-%m-%d', '%B %d, %Y', '%d %B, %Y']:&#x000A;        try:&#x000A;             cleandate = datetime.datetime.strptime(rawdate, date_format)&#x000A;        except ValueError:&#x000A;             cleandate = None&#x000A;        else:&#x000A;             break&#x000A;&#x000A;    return cleandate&#x000A;&#x000A;print parse_date2('&lt;div id="date"&gt;19 April, 2012&lt;/div&gt;')&#x000A;</code></pre>
            
            <p>This loops through three different date formats and returns the first
            one that doesn’t raise the error.</p>
            
            <h2 id="example-2-unreliable-http-connection">Example 2: Unreliable HTTP connection</h2>
            <p>If you’re scraping an unreliable website or you are behind
            an unreliable internet connection, you may sometimes get
            HTTPErrors or URLErrors for valid URLs. Trying again later
            might help.</p>
            
            <pre><code>import urllib2&#x000A;&#x000A;def load(url):&#x000A;    retries = 3&#x000A;    for i in range(retries):&#x000A;        try:&#x000A;            handle = urllib2.urlopen(url)&#x000A;        except urllib2.URLError:&#x000A;            if i + 1 == retries:&#x000A;                raise&#x000A;            else:&#x000A;                time.sleep(42)&#x000A;        else:&#x000A;            break&#x000A;    return handle.read()&#x000A;&#x000A;print load('http://thomaslevine.com')&#x000A;</code></pre>
            
            <p>This function tries thrice to download the page.
            On the first two fails, it waits 42 seconds and tries again.
            On the third failure, it raises the error.
            On a success, it returs the content of the page.</p>
            
            <h2 id="example-3-logging-errors-rather-than-raising-them">Example 3: Logging errors rather than raising them</h2>
            <p>For more complicated parses, you might find loads
            of errors popping up in weird places, so you might
            want to go through all of the documents before deciding
            which to fix first or whether to do some of them manually.</p>
            
            <pre><code>import scraperwiki&#x000A;&#x000A;for document_name in document_names:&#x000A;    try:&#x000A;        parse_document(document_name)&#x000A;    except Exception as e:&#x000A;        scraperwiki.sqlite.save([], {&#x000A;            'documentName': document_name,&#x000A;            'exceptionType': str(type(e)),&#x000A;            'exceptionMessage': str(e)&#x000A;        }, 'errors')&#x000A;</code></pre>
            
            <p>This catches any exception raised by a particular document,
            stores it in the database and then continues with the next document.
            Looking at the database afterwards, you might notice some trends
            in the errors that you can easily fix and some others where you
            might hard-code the correct parse.</p>
            
            <h2 id="example-4-exiting-gracefully">Example 4: Exiting gracefully</h2>
            
            <p>When I’m scraping over 9000 pages and my script fails
            on page 8765, I like to be able to resume where I left off.
            I can often figure out where I left off based on
            the previous row that I saved to a database or file,
            but sometimes I can’t, particularly when I don’t have
            a unique index.</p>
            
            <pre><code>for bar in bars:&#x000A;    try:&#x000A;        foo(bar)&#x000A;    except:&#x000A;        print('Failure at bar = "%s"' % bar)&#x000A;        raise&#x000A;</code></pre>
            
            <p>This will tell me which bar I left off on.
            It’s fancier if I save the information to the database,
            so here is how I might do that with ScraperWiki.</p>
            
            <pre><code>import scraperwiki&#x000A;&#x000A;resume_index = scraperwiki.sqlite.get_var('resume_index', 0)&#x000A;for i, bar in enumerate(bars[resume_index:]):&#x000A;    try:&#x000A;        foo(bar)&#x000A;    except:&#x000A;        scraperwiki.sqlite.save_var('resume_index', i)&#x000A;        raise&#x000A;scraperwiki.sqlite.save_var('resume_index', 0)&#x000A;</code></pre>
            
            <p>ScraperWiki has a limit on CPU time, so an error that often concerns me is the
            <a href="https://scraperwiki.com/docs/python/python_help_documentation/">scraperwiki.CPUTimeExceededError</a>.
            This error is raised after the script has used 80 seconds of CPU time;
            if you catch the exception, you have two CPU seconds to clean up.
            You might want to handle this error differently from other errors.</p>
            
            <pre><code>import scraperwiki&#x000A;&#x000A;resume_index = scraperwiki.sqlite.get_var('resume_index', 0)&#x000A;for i, bar in enumerate(bars[resume_index:]):&#x000A;    try:&#x000A;        foo(bar)&#x000A;    except scraperwiki.CPUTimeExceededError:&#x000A;        scraperwiki.sqlite.save_var('resume_index', i)&#x000A;        raise&#x000A;    except Exception as e:&#x000A;        scraperwiki.sqlite.save_var('resume_index', i)&#x000A;        scraperwiki.sqlite.save([], {&#x000A;            'bar': bar,&#x000A;            'exceptionType': str(type(e)),&#x000A;            'exceptionMessage': str(e)&#x000A;        }, 'errors')&#x000A;scraperwiki.sqlite.save_var('resume_index', 0)&#x000A;</code></pre>
            
            <h2 id="tldr">tl;dr</h2>
            <p>Expect exceptions to occur when you are scraping a
            randomly unreliable website with randomly inconsistent content,
            and consider handling them in ways that allow the script
            to keep running when one document of interest is bizarrely
            formatted or not available.</p>
          </article>
        </div>
        <div id='pagination'>
          <div class='base-little-card'>
            <a href="https://github.com/tlevine/www.thomaslevine.com/tree/master/content/!/scraper_exceptions/index.md">View source</a>
            
            <a href="https://twitter.com/thomaslevine">Discuss</a>
          </div>
        </div>
      </div>
    </div>
    <script src='/js/application-cb286d6f677.js'></script>
    <!-- Piwik -->
    <script type="text/javascript">
    var pkBaseURL = (("https:" == document.location.protocol) ? "https://piwik.thomaslevine.com/" : "http://piwik.thomaslevine.com/");
    document.write(unescape("%3Cscript src='" + pkBaseURL + "piwik.js' type='text/javascript'%3E%3C/script%3E"));
    </script><script type="text/javascript">
    try {
    var piwikTracker = Piwik.getTracker(pkBaseURL + "piwik.php", 2);
    piwikTracker.trackPageView();
    piwikTracker.enableLinkTracking();
    } catch( err ) {}
    </script><noscript><p><img src="http://piwik.thomaslevine.com/piwik.php?idsite=2" style="border:0" alt="Piwik tracking image" /></p></noscript>
    <!-- End Piwik Tracking Code -->
  </body>
</html>
