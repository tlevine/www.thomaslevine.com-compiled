<!DOCTYPE html>
<!--[if lt IE 7]> <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>    <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>    <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!-->
<html class='no-js'>
  <!--<![endif]-->
  <head>
    <meta charset='utf-8'>
    <meta content='nanoc 3.6.4' name='generator'>
    <meta content='width=device-width' name='viewport'>
    <title>Cool things happen when you look at lots of spreadsheets at once</title>
    <meta content='A whirlwind tour of my studies about open data' name='description'>
    <meta content='Thomas Levine' name='author'>
    <link href='http://domain/humans.txt' rel='author' type='text/plain'>
    <link href='https://www.google.com/accounts/o8/ud?source=profiles' rel='openid2.provider'>
    <link href='https://profiles.google.com/112237825767532686869' rel='openid2.local_id'>
    <meta content='summary' name='twitter:card'>
    <meta content='@thomaslevine' name='twitter:site'>
    <meta content='Cool things happen when you look at lots of spreadsheets at once' name='twitter:title'>
    <meta content='A whirlwind tour of my studies about open data' name='twitter:description'>
    <meta content='@thomaslevine' name='twitter:creator'>
    <meta content='http://thomaslevine.com/!/cool-things-happen-when-you-have-lots-of-spreadsheets/unsilo.png' name='twitter:image:src'>
    <meta content='thomaslevine.com' name='twitter:domain'>
    <meta content='' name='twitter:app:name:iphone'>
    <meta content='' name='twitter:app:name:ipad'>
    <meta content='' name='twitter:app:name:googleplay'>
    <meta content='' name='twitter:app:url:iphone'>
    <meta content='' name='twitter:app:url:ipad'>
    <meta content='' name='twitter:app:url:googleplay'>
    <meta content='' name='twitter:app:id:iphone'>
    <meta content='' name='twitter:app:id:ipad'>
    <meta content='' name='twitter:app:id:googleplay'>
    <meta content='http://thomaslevine.com/!/cool-things-happen-when-you-have-lots-of-spreadsheets/' property='og:url'>
    <meta content='thomaslevine.com' property='og:site_name'>
    <meta content='A whirlwind tour of my studies about open data' property='og:description'>
    <meta content='Cool things happen when you look at lots of spreadsheets at once' property='og:title'>
    <meta content='http://thomaslevine.com/!/cool-things-happen-when-you-have-lots-of-spreadsheets/unsilo.png' property='og:image'>
    <link href='/favicon.ico' rel='icon' type='image/x-icon'>
    <link href='/!/feed.xml' rel='alternate' title='Thomas Levine' type='application/atom+xml'>
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:400italic,400,700' rel='stylesheet' type='text/css'>
    <script src='https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML' type='text/javascript'></script>
    <link href='/css/style-cb653401acb.css' rel='stylesheet'>
    <script src='/js/modernizr-cb42306a279.js'></script>
  </head>
  <body>
    <!--[if lt IE 7 ]>
      <p class='chromeframe'>
        You are using an <strong>outdated</strong> browser.
        Please <a href="http://browsehappy.com/">upgrade your browser</a> or
        <a href="http://www.google.com/chromeframe/?redirect=true">activate Google Chrome Frame</a>
        to improve your experience.
      </p>
    <![endif]-->
    <div id='wrapper'>
      <div id='container'>
        <nav>
          <ul class='nobullet'>
            <li class='link'>
              <a href='/'>
                <div>~</div>
              </a>
            </li>
            <li class='link'>
              <a href='/!/'>
                <div>!</div>
              </a>
            </li>
            <li class='link'>
              <a href='/!/about/'>
                <div>?</div>
              </a>
            </li>
          </ul>
        </nav>
        <div id='article-wrapper'>
          <article>
            <header>
              <h1>Cool things happen when you look at lots of spreadsheets at once</h1>
              <em>
                <time datetime='2014-02-24'>
                  February 24, 2014
                </time>
              </em>
              <span class='tags'>
                <a href="/open-data">open-data</a>
              </span>
            </header>
            <hr>
            <p>Here are some materials for
            <a href="">my talk at Open Austin</a>,
            but they’re written in normal language, so they’ll probably serve as a decent summary
            of my work thus far to people who are reading this on the internet.</p>
            
            <h2 id="before-we-start-why-i-can-be-here">Before we start: Why I can be here.</h2>
            
            <p>https://github.com/tlevine/undervalued-sublets</p>
            
            <h2 id="outline-of-the-talk">Outline of the talk</h2>
            
            <p>Introduction</p>
            
            <ul>
              <li>What are “open data”? Show the video.</li>
              <li>Data about open data, data-driven open data</li>
            </ul>
            
            <p>Two approaches</p>
            
            <ol>
              <li>We have all of these data, so something interesting must be in it.</li>
              <li>We are interested in something. Let’s collect data that will tell us about that thing.</li>
            </ol>
            
            <p>How I collect these data</p>
            
            <ol>
              <li>Downloading the data</li>
              <li>Looking inside the various datasets</li>
              <li>Representing all the spreadsheets as one spreadsheet</li>
            </ol>
            
            <p>Cool things that happen</p>
            
            <ol>
              <li>Better ways of searching</li>
              <li>Dealing with bad (meta)data.</li>
              <li>Quantifying data quality</li>
              <li>Causal inferences: Do open data do anything?</li>
            </ol>
            
            <h2 id="introduction">Introduction</h2>
            <p>Let’s talk about some things I’ve been <a href="/open-data">learning about open data</a>.
            For the longest time, I had no idea what people meant when they were talking
            about “open data”, and I think that’s part of why I started looking at the
            stuff. So now I have a bit of an idea of what the open data thing is.</p>
            
            <h3 id="open-data">“Open data”</h3>
            <p><a href="/!/open-data-in-plain-english">This video</a> summarizes pretty well most of
            the good things people say about open data, at least for government data.
            But I think this sort of thinking makes open data seem way to complicated.</p>
            
            <h3 id="sharing">Sharing</h3>
            <p>Sharing is usually good, and I think that’s the heart of what everyone likes
            about “open data”, and we have a word for it because people aren’t used to
            this idea. But when the thing that we’re sharing is complicated and
            sometimes-private numbers, useful sharing gets hard. This happens when you’re
            trying to share with the whole world, but it also happens when you’re trying
            to share within a small company. </p>
            
            <p>We want to share, but we’re not very good at it, and this open data stuff is
            part of our attempt to get better at sharing.</p>
            
            <h3 id="recursive-data-driving">Recursive data-driving</h3>
            <p>One benefit of open data might be the ability for people to use lots of different
            datasets in order to make data-driven decisions. The people who are releasing open
            data surely get this, so they’re obviously using data to make decisions about their
            open data initiatives, right?</p>
            
            <p>Actually, they’re not, so I started doing that. Also, I’m doing it quite publicly,
            so you could say this is open data about open data.</p>
            
            <h2 id="my-process-and-findings">My process and findings</h2>
            <p>I was taught in school that you come up with your question and then collect data
            that perfectly answer that question. This way works, but you can often learn more
            faster and with less work if you’re a bit sloppier.</p>
            
            <p>I like to think of two approaches of deciding what to study.</p>
            
            <ol>
              <li>We have all of these data, so something interesting must be in it.</li>
              <li>We are interested in something. Let’s collect data that will tell us about that thing.</li>
            </ol>
            
            <p>I think the former is more obvious: Initially, I found it quite odd that
            nobody had looked at the data about the data. So I did.</p>
            
            <p>Let’s talk a bit about the latter. Let’s say we want to study someone’s sleep patterns.
            In order to do this, we wind to find out when the person is sleeping. We could do this
            by having the person record on paper the times at which she goes to sleep and wakes up,
            but that would be a lot of work. Other ideas</p>
            
            <p><a href="http://yihui.name/en/2009/10/50000-revisions-committed-to-r/">Version control commits</a></p>
            
            <p><img src="../data-about-open-data-talk-december-2-2013/r-commits.gif" alt="R commits" class="wide" /></p>
            
            <p><a href="https://hackpad.com/Measuring-Socioeconomic-Indicators-in-Arabic-Tweets-IZ5ByP2LvIt">Tweets</a></p>
            
            <p><img src="../data-about-open-data-talk-december-2-2013/tweet-times.jpg" alt="Bar plot of Tweet times" class="wide" /></p>
            
            <p>If the person is me, we can use shell history activity.</p>
            
            <pre><code>#!/bin/sh&#x000A;# This file is history.sh&#x000A;for epochtime in $(grep '^#[0-9]\{10\}$' ~/.history/sh-2013-1[12]*|cut -d\# -f2); do&#x000A;  date --date=@$epochtime +%H&#x000A;done | sort | uniq -c | awk '{print $2, "%"$1"s"}' &gt; /tmp/formatted&#x000A;&#x000A;while read line; do&#x000A;  # Remove the first space&#x000A;  nospace=$(echo $line | sed 's/ //')&#x000A;  printf "$line\n" | tr \  -|sed s/----------------------------------------/=/g|sed -e s/-//g -e 's/=/ =/'&#x000A;done &lt; /tmp/formatted&#x000A;</code></pre>
            
            <p>Here’s the resulting histogram.</p>
            
            <pre><code>$ ./history.sh&#x000A;00 =========&#x000A;01 =======&#x000A;02 =====&#x000A;03 ====&#x000A;04 =======&#x000A;05 ============&#x000A;06 ======&#x000A;07 ====&#x000A;08 =&#x000A;09 =&#x000A;10 =====&#x000A;11 ===&#x000A;12 ==&#x000A;13 ======&#x000A;14 =====&#x000A;15 ======&#x000A;16 ==============&#x000A;17 ==============================&#x000A;18 =======================================&#x000A;19 ============================&#x000A;20 ===============&#x000A;21 ==========&#x000A;22 ==================&#x000A;23 ==================&#x000A;</code></pre>
            
            <p>Note that these times are in UTC because that’s how I roll.</p>
            
            <p>In this approach of deciding what to study, the idea is that we can
            answer our curiosities by building on some existing data collection.
            <a href="/!/brainstorming">These</a> brief thoughts on brainstorming might be
            of interest.</p>
            
            <h2 id="collecting-the-data-about-data">Collecting the data about data</h2>
            <p>First I download a bunch of spreadsheets and spreadsheet metadata.
            Then I assemble all this stuff into a spreadsheet about spreadsheets.
            In this super-spreadsheet, each record corresponds to a full
            sub-spreadsheet; you could say that I am aggregating each spreadsheet
            to produce a few statistics that get put into this spreadsheet.</p>
            
            <h3 id="downloading">Downloading</h3>
            <p>Data catalogs make it kind of easy to get a bunch of spreadsheets all together.
            The basic approach is this.</p>
            
            <ol>
              <li>Get all of the dataset identifiers.</li>
              <li>Download the metadata document about each dataset.</li>
              <li>Download data files about each dataset.</li>
            </ol>
            
            <p>I’ve implemented this for the following data catalog softwares.</p>
            
            <ul>
              <li>Socrata</li>
              <li>CKAN</li>
              <li>Junar (kind of)</li>
              <li>OpenDataSoft</li>
            </ul>
            
            <p>This allows me to get all of the data from most of the open data catalogs I know about.</p>
            
            <p>Let’s walk through how that works for the different softwares.</p>
            
            <h4 id="socrata">Socrata</h4>
            <p>In Socrata, I hit the <code>/api/views</code> endpoint to get all of the datasets.
            (They’re spread across different pages, but they’re all returned.)</p>
            
            <blockquote>
              <p>http://data.austintexas.gov//api/views?page=3</p>
            </blockquote>
            
            <p>All of the metadata are returned in the search results, so this also accomplishes
            the second step of downloading the metadata documents.</p>
            
            <p>That said, you can also download the metadata documents separately;
            here’s one of them.</p>
            
            <blockquote>
              <p>https://data.austintexas.gov/api/views/5tye-7ray</p>
            </blockquote>
            
            <p>Most datasets in Socrata Open Data Portal correspond to spreadsheets, and
            you can download those by appending <code>/rows.csv?accessType=DOWNLOAD</code>.</p>
            
            <blockquote>
              <p>https://data.austintexas.gov/api/views/5tye-7ray/rows.csv?accessType=DOWNLOAD</p>
            </blockquote>
            
            <p>It took me a while to figure all of this out, so a lot of what I was doing over
            the summer was writing documentation.</p>
            
            <h4 id="ckan">CKAN</h4>
            <p>Someone wrote a good CKAN client, so I use that to download the CKAN stuff.
            This is how I get a list of all the dataset identifiers.</p>
            
            <pre><code>#!/usr/bin/env python&#x000A;import ckanapi&#x000A;portal = ckanapi.RemoteCKAN('http://data.gov.uk')&#x000A;datasets = portal.action.package_list()&#x000A;</code></pre>
            
            <p>That only provides the identifiers, so I continue with the following code
            to get the metadata documents.</p>
            
            <pre><code>for dataset in datasets:&#x000A;    dataset_information = portal.action.package_show(id = dataset)&#x000A;</code></pre>
            
            <p><a href="http://data.gov.uk/api/2/rest/package/index-of-multiple-deprivation">Here</a>’s
            an example of one such metadata file.</p>
            
            <p>Most datasets on CKAN catalogs link to other websites for the main “data”
            files, and the links are stored in the matadata files.</p>
            
            <h4 id="junar">Junar</h4>
            <p>In Junar, it’s hard to get a list of all of the datasets. You can do a
            search like so.</p>
            
            <blockquote>
              <p>http://paloalto.cloudapi.junar.com/datastreams/search?query=grapefruit&amp;auth-key=da782fcac90afb0a310f72a4f63baff6d26fc0b1</p>
            </blockquote>
            
            <p>Well at least that used to work. It seems that that API key doesn’t work anymore.</p>
            
            <p>I’m pretty sure that the rest of the process works just fine once you have
            a dataset identifier, but I don’t remember how that all works at the moment.</p>
            
            <h4 id="opendatasoft">OpenDataSoft</h4>
            <p>In OpenDataSoft, you can run an empty search to get the metadata about all
            of the datasets in a single file.</p>
            
            <blockquote>
              <p>http://parisdata.opendatasoft.com/api/datasets/1.0/search?rows=1000000</p>
            </blockquote>
            
            <p>Like with the other softwares, you can also get the metadata about a
            specific dataset; here’s a URL for that.</p>
            
            <blockquote>
              <p>http://parisdata.opendatasoft.com/api/datasets/1.0/arbresremarquablesparis2011</p>
            </blockquote>
            
            <p>Each dataset corresponds to a spreadsheet, and you can download that by
            adding <code>/download?format=csv</code> to the above URL.</p>
            
            <blockquote>
              <p>http://parisdata.opendatasoft.com/explore/dataset/arbresremarquablesparis2011/download?format=csv</p>
            </blockquote>
            
            <h3 id="looking-inside-datasets">Looking inside datasets</h3>
            <p>A statistic is a single number that describes a bunch of numbers.</p>
            
            <pre><code>4&#x000A;2&#x000A;8    --mean--&gt; 5.8&#x000A;12&#x000A;3&#x000A;</code></pre>
            
            <p>I create statistics about each dataset.</p>
            
            <p><img src="/!/dataset-as-datapoint/dataset-features.jpg" alt="" class="wide" /></p>
            
            <h3 id="putting-them-in-a-spreadsheet">Putting them in a spreadsheet</h3>
            <p>Combining the metadata and the new dataset statistics, I create
            a spreadsheet of datasets, in which each record corresponds to a dataset.</p>
            
            <p><a href="/!/dataset-as-datapoint"><img src="/!/dataset-as-datapoint/spreadsheet-spreadsheet.png" alt="A spreadsheet of spreadsheets" class="wide" /></a></p>
            
            <h2 id="cool-things">Cool things</h2>
            
            <ol>
              <li>Better ways of searching</li>
              <li>Dealing with bad (meta)data.</li>
              <li>Quantifying data quality</li>
              <li>Causal inferences: Do open data do anything?</li>
            </ol>
            
            <p>(They’re really all the same thing, actually.)</p>
            
            <h3 id="searching">Searching</h3>
            <p>Many people know about datasets that are relevant to their work,
            municipality, &amp;c., but nobody seems to know about the availability of
            data on broader topics, and nobody seems to have a good way of
            finding out what is available. And nobody has a great idea of who
            is using which data. Here are two aspects of the difficulty.</p>
            
            <ol>
              <li>Naive search method</li>
              <li>Siloed open data portals</li>
            </ol>
            
            <p>(Read more <a href="/!/openprism">here</a>.)</p>
            
            <h4 id="search-method">Search method</h4>
            <p>We search for prose by typing prose into a search bar; why don’t
            we search for spreadsheets by typing spreadsheets into a search bar?
            Aside frorm finding datasets that contain particular keywords, here
            are some other ways we could search.</p>
            
            <ul>
              <li>Datasets that were produced by the same program as this dataset</li>
              <li>Datasets that I can join to this dataset</li>
              <li>Datasets that pertain to this particular geographic region</li>
              <li>Datasets in long format (rather than wide format)</li>
            </ul>
            
            <p>Here are some things that might get you thinking about the possibilities.</p>
            
            <ul>
              <li><a href="Datasets with the same schema">http://appgen.me/audit/report</a></li>
              <li><a href="https://pypi.python.org/pypi/special_snowflake"><code>special_snowflake</code></a></li>
              <li>Group by an arbitrary key, aggregate, join. The most common column
              in New York City’s data catalog was zip code. The zip code dataset!</li>
            </ul>
            
            <h4 id="siloed-data-catalogs">Siloed data catalogs.</h4>
            <p>Different people put out their datasets on their own websites.
            These sites work as a way of getting the data on the internet, but
            they aren’t really designed for accessing data around a specific analytical
            inquiry. For example, if I want to know where in Austin to build a house,
            I don’t necessarily just data from the City of Austin; I might want data from
            other cities, from the county, or from the state.</p>
            
            <p><img src="../data-about-open-data-talk-december-2-2013/unsilo.jpg" alt="Diagram about siloed open data portals and some layer to un-silo them" class="wide" /></p>
            
            <p>I made a <a href="http://openprism.thomaslevine.com">rather simple site</a> to demonstrate this idea.</p>
            
            <h3 id="dealing-with-bad-metadata">Dealing with bad (meta)data</h3>
            <p>People complain about how data are bad and metadata are bad. Rather than
            fixing it on a case-by-case basis, I think we should just come up with ways
            of dealing with it.</p>
            
            <p><a href="/!/missouri-data-licensing/">Missouri</a> provides a good illustration of this.
            The titles of datasets are related to the contents of datasets.</p>
            
            <p>You can see my alternative search approaches as ways of guessing metadata.</p>
            
            <h3 id="quantifying-data-quality">Quantifying data quality</h3>
            
            <ul>
              <li>Open Knowledge Foundation <a href="http://census.okfn.org/">Open Data Census</a></li>
              <li>Tim Berners-Lee <a href="http://inkdroid.org/journal/2010/06/04/the-5-stars-of-open-linked-data/">Five Stars</a> of open linked data.
              <!-- http://opendata.stackexchange.com/a/529 --></li>
              <li>Open Government Working Group <a href="http://www.opengovdata.org/home/8principles">8 Principles of Open Government Data</a></li>
              <li>Sunlight Foundation <a href="http://sunlightfoundation.com/opendataguidelines/">Open Data Policy Guidelines</a></li>
              <li>Open Data Institute <a href="https://certificates.theodi.org/">Certificates</a></li>
            </ul>
            
            <h4 id="licensing">Licensing</h4>
            <p>I <a href="http://thomaslevine.com/!/open-data-licensing/">looked at</a>
            the licenses that different datasets have.</p>
            
            <p><img src="/!/open-data-licensing/p2.png" alt="Licenses across all portals" class="wide" /></p>
            
            <p>Most data catalogs either have a license on everything or a license on nothing.)</p>
            
            <p><img src="/!/open-data-licensing/p1.png" alt="Bar graph of proportion of datasets" class="wide" /></p>
            
            <p><a href="http://opendatacommons.org/faq/">Licensing is important because it reduces uncertainty.</a></p>
            
            <h4 id="updating">Updating</h4>
            <p>Open government data are supposed to be kept up-to-date.
            <a href="http://thomaslevine.com/!/data-updatedness/#even-simpler">Pretty much nobody</a> does this.</p>
            
            <h4 id="getting-the-data">Getting the data</h4>
            <p>Socrata has some date fields in the metadata, so I could look at the update behavior.</p>
            
            <p>First, hardly any datasets ever get updated.</p>
            
            <p><img src="/!/data-updatedness/figure/any_update.png" alt="Hardly any datasets get updated" class="wide" /></p>
            
            <p>Second, the ones that have been updated were mostly updated two years ago.
            There might have been some bulk Socrata migration at the beginning of September 2011.</p>
            
            <p><img src="/!/data-updatedness/figure/publish_v_update.png" alt="Bulk migration?" class="wide" /></p>
            
            <p>Here are the datasets that got published before 2013 and got updated during 2013.</p>
            
            <p><img src="/!/data-updatedness/figure/publish_v_update_2013.png" alt="Old data still kept up-to-date" class="wide" /></p>
            
            <p>It’s only 13 datasets.</p>
            
            <p><img src="/!/data-updatedness/figure/updates_2013_url.png" alt="Those 13 datasets, by portal" class="wide" /></p>
            
            <p>Data are probably being updated thorugh other means,</p>
            
            <ul>
              <li>Adding new data as a separate dataset (a 2011 dataset and a 2012 dataset)</li>
              <li>Deleting the old dataset and adding a new one</li>
            </ul>
            
            <p>but these aren’t as good because they don’t preserve URIs.</p>
            
            <h3 id="causal-inferences-how-things-work">Causal inferences: How things work</h3>
            <p>It would be great to tie the release of spreadsheets to outcomes that people really
            care about, like levels of corruption, life expectancies, and employment rates.
            But that’s hard, so I’m starting simpler; we can start by seeing what different
            software products do.</p>
            
            <p>What do I mean by different products? Any of the below things would count.</p>
            
            <ul>
              <li>Different features within a product</li>
              <li>Different vendors (Socrata, Open Knowledge Foundation, Junar, OpenDataSoft, &amp;c.)</li>
              <li>Different plugins and <a href="http://ckan.org/category/extensions/">extensions</a></li>
              <li>Different <a href="/!/socrata-products">products and services</a> sold by one company</li>
            </ul>
            
            <p>This can be framed as practical questions like “Which data catalog software should I use?”</p>
            
            <h4 id="charting-tools">Charting tools</h4>
            <p>These open data catalog softwares all have charting tools built in.
            Basically, people don’t use these charting tools all that much.</p>
            
            <p>The Socrata metadata indicate the users that updated the data.
            Most of the users in the dataset (7790 to be exact) had made exactly one view.</p>
            
            <p><img src="/!/socrata-users/figure/n.views.png" alt="" class="wide" /></p>
            
            <p>Actually, there are probably even more with no views, but I don’t have the
            data on them. Also, about four-fifths of Socrata’s data is private, (Several
            people who work for Socrata have told me this.) so I’m probably missing even
            more. Oh well.</p>
            
            <p>Similarly, the users who have owned and authored the most tables tend to work
            for either Socrata or clients of Socrata.</p>
            
            <p>Neither of these discoveries should be a surprise; you can call it the
            <a href="http://en.wikipedia.org/wiki/Pareto_principle">Pareto principle</a> if you want.</p>
            
            <p>Socrata is trying to “consumerize” the data experience, so 
            I tried to find users who were not employed by Socrata or its
            clients. I eventually <a href="/!/socrata-users/#also-no-tables">found some</a>.</p>
            
            <p>As I said above, my main conclusion is that people don’t use these charting
            tools all that much. More specifically,</p>
            
            <ol>
              <li>The people who create the most charts are people who maintain data portals</li>
              <li>Aside from those who maintain data portals, the people who create the most
             charts are usually making different charts of the same data.</li>
              <li>I found a small number of people who seem to be using the charts for broader
             things. I haven’t really talked to any of them, but the little I do know of
             their stories is interesting.</li>
              <li>All of the data catalog softwares have charting tools like this.
             Socrata’s is the most built-out, so I doubt that we’ll see more usage
             in other sites’ charting tools.</li>
            </ol>
            
            <p>If you are making a data catalog for your organization, I recommend that you
            not worry about including data visualization things in the catalog.</p>
            
            <h4 id="links">Links</h4>
            <p>Links can go <a href="/404.html">dead</a>. I looked at dataset liveliness for Socrata
            and CKAN.</p>
            
            <p><img src="http://thomaslevine.com/!/zombie-links/figure/p_prop_links.png" alt="" class="wide" /></p>
            
            <p>In Socrata, datasets tend to be stored in the Socrata application and strongly
            associated with the entity on the data portal. All of these internally stored
            datasets stay alive (presumably). There are a few externally stored datasets,
            and some of those are dead.</p>
            
            <p>In CKAN, datasets tend to reference external files, and a lot of them are dead.
            That said, it seems to be better at keeping external links alive than Socrata is.</p>
            
            <p>This difference totally makes sense if you look at the processes for
            <a href="/!/data-catalog-dead-links/#software-suggests-behavior">uploading data</a>.</p>
            
            <p>We can also see what sorts of problems are arising.</p>
            
            <p><img src="/!/zombie-links/figure/p_no_redirects.png" alt="" class="wide" /></p>
            
            <p><img src="/!/zombie-links/figure/storage.png" alt="" class="wide" /></p>
            
            <p>Read more <a href="http://thomaslevine.com/!/zombie-links/#new-results">here</a></p>
            
            <h2 id="final-thoughts">Final thoughts</h2>
            
            <ol>
              <li>Ways of inquiring
                <ul>
                  <li>Question to data</li>
                  <li>Data to question</li>
                </ul>
              </li>
              <li>Open data is about sharing.</li>
              <li>You can collect data about data.</li>
              <li>You don’t really need to distinguish between metadata and data.</li>
            </ol>
          </article>
        </div>
        <div id='pagination'>
          <div class='base-little-card'>
            <a href="https://github.com/tlevine/www.thomaslevine.com/tree/master/content/!/cool-things-happen-when-you-have-lots-of-spreadsheets/index.md">View source</a>
            
            <a href="https://twitter.com/thomaslevine">Discuss</a>
          </div>
        </div>
      </div>
    </div>
    <script src='/js/application-cb286d6f677.js'></script>
    <!-- Piwik -->
    <script type="text/javascript">
    var pkBaseURL = (("https:" == document.location.protocol) ? "https://piwik.thomaslevine.com/" : "http://piwik.thomaslevine.com/");
    document.write(unescape("%3Cscript src='" + pkBaseURL + "piwik.js' type='text/javascript'%3E%3C/script%3E"));
    </script><script type="text/javascript">
    try {
    var piwikTracker = Piwik.getTracker(pkBaseURL + "piwik.php", 2);
    piwikTracker.trackPageView();
    piwikTracker.enableLinkTracking();
    } catch( err ) {}
    </script><noscript><p><img src="http://piwik.thomaslevine.com/piwik.php?idsite=2" style="border:0" alt="Piwik tracking image" /></p></noscript>
    <!-- End Piwik Tracking Code -->
  </body>
</html>
